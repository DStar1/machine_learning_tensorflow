{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>32.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>49.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>512.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId  Survived  Pclass   Age  SibSp  Parch  Fare\n",
       "count        891.0     891.0   891.0 714.0  891.0  891.0 891.0\n",
       "mean         446.0       0.4     2.3  29.7    0.5    0.4  32.2\n",
       "std          257.4       0.5     0.8  14.5    1.1    0.8  49.7\n",
       "min            1.0       0.0     1.0   0.4    0.0    0.0   0.0\n",
       "25%          223.5       0.0     2.0  20.1    0.0    0.0   7.9\n",
       "50%          446.0       0.0     3.0  28.0    0.0    0.0  14.5\n",
       "75%          668.5       1.0     3.0  38.0    1.0    0.0  31.0\n",
       "max          891.0       1.0     3.0  80.0    8.0    6.0 512.3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "\n",
    "titanic_dataframe = pd.read_csv(\"titanic/train.csv\", sep=\",\")\n",
    "\n",
    "titanic_dataframe = titanic_dataframe.reindex(\n",
    "    np.random.permutation(titanic_dataframe.index))\n",
    "\n",
    "titanic_dataframe.describe()\n",
    "\n",
    "# titanic_dataframe = titanic_dataframe[titanic_dataframe[\"TotRmsAbvGrd\"] <= 13]\n",
    "# titanic_dataframe = titanic_dataframe[titanic_dataframe[\"OverallQual\"] <= 9.5]\n",
    "# titanic_dataframe = titanic_dataframe[titanic_dataframe[\"GrLivArea\"] <= 3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#     #code categorical data\n",
    "#     label = LabelEncoder()    \n",
    "#     dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n",
    "#     dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n",
    "#     dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n",
    "#     dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n",
    "#     dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n",
    "\n",
    "\n",
    "#     #define y variable aka target/outcome\n",
    "#     Target = ['Survived']\n",
    "\n",
    "#     #define x variables for original features aka feature selection\n",
    "#     data1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts\n",
    "#     data1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\n",
    "#     data1_xy =  Target + data1_x\n",
    "#     print('Original X Y: ', data1_xy, '\\n')\n",
    "\n",
    "\n",
    "#     #define x variables for original w/bin features to remove continuous variables\n",
    "#     data1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\n",
    "#     data1_xy_bin = Target + data1_x_bin\n",
    "#     print('Bin X Y: ', data1_xy_bin, '\\n')\n",
    "\n",
    "\n",
    "#     #define x and y variables for dummy features original\n",
    "#     data1_dummy = pd.get_dummies(data1[data1_x])\n",
    "#     data1_x_dummy = data1_dummy.columns.tolist()\n",
    "#     data1_xy_dummy = Target + data1_x_dummy\n",
    "#     print('Dummy X Y: ', data1_xy_dummy, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "#     data1_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_na(correlation_dataframe):  \n",
    "    #complete missing age with median\n",
    "    correlation_dataframe['Age'].fillna(correlation_dataframe['Age'].median(), inplace = True)\n",
    "\n",
    "    #complete embarked with mode\n",
    "    correlation_dataframe['Embarked'].fillna(correlation_dataframe['Embarked'].mode()[0], inplace = True)\n",
    "\n",
    "    #complete missing fare with median\n",
    "    correlation_dataframe['Fare'].fillna(correlation_dataframe['Fare'].median(), inplace = True)\n",
    "\n",
    "#     #delete the cabin feature/column and others previously stated to exclude in train correlation_dataframe\n",
    "#     drop_column = ['Cabin', 'Ticket']#'PassengerId',\n",
    "#     correlation_dataframe.drop(drop_column, axis=1, inplace = True)\n",
    "\n",
    "    print(correlation_dataframe.isnull().sum())\n",
    "    print(\"-\"*10)\n",
    "    print(correlation_dataframe.isnull().sum())\n",
    "    return correlation_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_df(correlation_dataframe, fill):\n",
    "    if fill:\n",
    "        correlation_dataframe = fill_na(correlation_dataframe)\n",
    "\n",
    "    correlation_dataframe['FamilySize'] = correlation_dataframe ['SibSp'] + correlation_dataframe['Parch'] + 1\n",
    "\n",
    "    correlation_dataframe['IsAlone'] = 1 #initialize to yes/1 is alone\n",
    "    correlation_dataframe['IsAlone'].loc[correlation_dataframe['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n",
    "\n",
    "    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n",
    "    correlation_dataframe['Title'] = correlation_dataframe['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "\n",
    "#     correlation_dataframe['FareBin'] = pd.qcut(correlation_dataframe['Fare'], 4)\n",
    "\n",
    "#     correlation_dataframe['AgeBin'] = pd.cut(correlation_dataframe['Age'].astype(int), 5)\n",
    "        \n",
    "    #cleanup rare title names\n",
    "    #print(data1['Title'].value_counts())\n",
    "    stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\n",
    "    title_names = (correlation_dataframe['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n",
    "\n",
    "    #apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https://community.modeanalytics.com/python/tutorial/pandas-groupby-and-python-lambda-functions/\n",
    "    correlation_dataframe['Title'] = correlation_dataframe['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
    "    print(correlation_dataframe['Title'].value_counts())\n",
    "        \n",
    "      #     put back in\n",
    "    new = pd.DataFrame(correlation_dataframe)#.copy#.corr()\n",
    "    new = pd.concat([correlation_dataframe, pd.get_dummies(correlation_dataframe['SibSp']).rename(columns={0:\"SS0\", 1:'SS1', 2:'SS2'}),\n",
    "                 pd.get_dummies(correlation_dataframe['Parch'])[0:2].rename(columns={0:\"PC0\", 1:'PC1', 2:'PC2'}),\n",
    "                 pd.get_dummies(correlation_dataframe['Sex']), pd.get_dummies(correlation_dataframe['Embarked']),\n",
    "                pd.get_dummies(correlation_dataframe['Pclass']).rename(columns={1:'Class1', 2:'Class2', 3:\"Class3\"}),\n",
    "                pd.get_dummies(correlation_dataframe['Title'])],\n",
    "                axis=1)\n",
    "    \n",
    "    print(\"-\"*10)\n",
    "\n",
    "\n",
    "    #preview data again\n",
    "    new.info()\n",
    "    new.info()\n",
    "    new.sample(10)\n",
    "        \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_dataframe(titanic_dataframe, fill):\n",
    "  correlation_dataframe = clean_df(titanic_dataframe, fill)#titanic_dataframe.copy()\n",
    "#   correlation_dataframe[\"target\"] = correlation_dataframe[\"Survived\"]\n",
    "#   if drop:#creat drop function\n",
    "#     plt.scatter(correlation_dataframe['Age'], correlation_dataframe[\"target\"])\n",
    "    \n",
    "#     correlation_dataframe = clean_df(correlation_dataframe)\n",
    "\n",
    "#     correlation_dataframe = correlation_dataframe.dropna(subset = ['Age'])# correlation_dataframe['Age'].dropna()\n",
    "#     correlation_dataframe = correlation_dataframe.drop(['Cabin'], axis=1)# correlation_dataframe['Fare'].dropna()\n",
    "#     correlation_dataframe = correlation_dataframe.dropna(subset = ['Embarked'])\n",
    "\n",
    "    \n",
    "  return correlation_dataframe\n",
    "\n",
    "def preprocess_features(correlation_dataframe, fill):#, features):\n",
    "  \"\"\"Prepares input features from California housing data set.\n",
    "\n",
    "  Args:\n",
    "    titanic_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the California housing data set.\n",
    "  Returns:\n",
    "    A DataFrame that contains the features to be used for the model, including\n",
    "    synthetic features.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  new = correlation_dataframe.copy()#clean_df(correlation_dataframe, fill)\n",
    "\n",
    "  print(new.head(7))\n",
    "  new[\"target\"] = new[\"Survived\"]\n",
    "#   new.set_index('PassengerId')\n",
    "  new1 = new.corr()\n",
    "  features_i = new1['Survived'].sort_values(ascending=False).head(16)\n",
    "#   features_i\n",
    "\n",
    "  # new.hist(figsize=(20,20))\n",
    "  # print(features)\n",
    "  features_index = features_i.index[2:]\n",
    "  print(\"\\nChosen Features NULLs?\\n\\n\", new[features_index].isnull().sum())\n",
    "\n",
    "  selected_features = new[features_index]\n",
    "  processed_features = selected_features.copy()\n",
    "\n",
    "#   processed_features = processed_features.drop(['target','Survived'], axis=1)# \n",
    "#Simple version\n",
    "#   processed_features = pd.DataFrame()\n",
    "#   processed_features['Fare'] = titanic_dataframe['Fare']\n",
    "  print(features_index)\n",
    "\n",
    "  return processed_features, features_index\n",
    "\n",
    "def preprocess_test_features(correlation_dataframe, features_index, PID):\n",
    "\n",
    "\n",
    "  new = correlation_dataframe.copy()#clean_df(correlation_dataframe, 1)#correlation_dataframe.copy()#clean_df(correlation_dataframe)\n",
    "\n",
    "  selected_features = new[features_index]\n",
    "#   selected_features = pd.concat([new[features_index], correlation_dataframe['PassengerId']], axis=1)\n",
    "#   new.set_index(PID)\n",
    "  processed_features = selected_features.copy()\n",
    "#   processed_features = processed_features.drop(['Survived'], axis=1)\n",
    "  return processed_features\n",
    "\n",
    "def preprocess_targets(correlation_dataframe):\n",
    "  \"\"\"Prepares target features (i.e., labels) from California housing data set.\n",
    "\n",
    "  Args:\n",
    "    titanic_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the California housing data set.\n",
    "  Returns:\n",
    "    A DataFrame that contains the target feature.\n",
    "  \"\"\"\n",
    "\n",
    "  output_targets = pd.DataFrame()\n",
    "#   output_targets['PassengerId'] = correlation_dataframe['PassengerId']\n",
    "  output_targets[\"Survived\"] = correlation_dataframe[\"Survived\"]\n",
    "#   output_targets.set_index('PassengerId')\n",
    "  return output_targets\n",
    "# titanic_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Choose the first 12000 (out of 17000) examples for training.\n",
    "# correlation_df = preprocess_dataframe(titanic_dataframe, 1)\n",
    "\n",
    "# examples, features_index = preprocess_features(correlation_df, 1)\n",
    "# training_examples = examples.head(691)#preprocess_features(correlation_df.head(691), 1)\n",
    "# training_targets = preprocess_targets(correlation_df.head(691))#, 1)\n",
    "# #\n",
    "# # correlation_dataframe = titanic_dataframe.copy()\n",
    "# # correlation_dataframe[\"target\"] = training_targets[\"SalePrice\"]\n",
    "# # new = correlation_dataframe.corr()\n",
    "# # print(new)\n",
    "# # features1 = new['target'].sort_values(ascending=False).head(10).index\n",
    "# # features1 = features1[2:-1]\n",
    "# # #\n",
    "# # training_examples = preprocess_features(titanic_dataframe.head(1000), features1)\n",
    "\n",
    "# # Choose the last 5000 (out of 17000) examples for validation.\n",
    "# validation_examples = examples.head(460)#preprocess_features(correlation_df.tail(460), 1)#, features1)\n",
    "# validation_targets = preprocess_targets(correlation_df.tail(460))#, 1)\n",
    "\n",
    "# # display.display(titanic_dataframe.describe())\n",
    "\n",
    "# # Double-check that we've done the right thing.\n",
    "# print(\"Training examples summary:\")\n",
    "# display.display(training_examples)#.describe())\n",
    "# print(\"Validation examples summary:\")\n",
    "# display.display(validation_examples.describe())\n",
    "\n",
    "# print(\"Training targets summary:\")\n",
    "# display.display(training_targets.describe())\n",
    "# print(\"Validation targets summary:\")\n",
    "# display.display(validation_targets.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #find good feratures based on sale price\n",
    "# correlation_dataframe = titanic_dataframe.copy()\n",
    "# correlation_dataframe[\"target\"] = titanic_dataframe[\"SalePrice\"]\n",
    "\n",
    "# # display.display(training_targets.describe())\n",
    "\n",
    "# correlation_dataframe.corr()\n",
    "# new = pd.DataFrame()\n",
    "# new['target'] = correlation_dataframe.corr()['target']\n",
    "# new.to_csv('corr.csv')\n",
    "# # correlation_dataframe[\"OpenPorchSF\"].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation_dataframe = titanic_dataframe.copy()\n",
    "# correlation_dataframe[\"target\"] = training_targets[\"SalePrice\"]\n",
    "\n",
    "# # display.display(training_targets.describe())\n",
    "# # pd.options.display.max_rows = 20\n",
    "\n",
    "# new = correlation_dataframe.corr()\n",
    "# print(new['target'].sort_values(ascending=False).head(10))#.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    \"\"\"Trains a linear regression model.\n",
    "  \n",
    "    Args:\n",
    "      features: pandas DataFrame of features\n",
    "      targets: pandas DataFrame of targets\n",
    "      batch_size: Size of batches to be passed to the model\n",
    "      shuffle: True or False. Whether to shuffle the data.\n",
    "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
    "    Returns:\n",
    "      Tuple of (features, labels) for next data batch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert pandas data into a dict of np arrays.\n",
    "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
    "    \n",
    "    # Construct a dataset, and configure batching/repeating.\n",
    "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "    # Shuffle the data, if specified.\n",
    "    if shuffle:\n",
    "      ds = ds.shuffle(10000)\n",
    "    \n",
    "    # Return the next batch of data.\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_feature_columns(input_features):\n",
    "  \"\"\"Construct the TensorFlow Feature Columns.\n",
    "\n",
    "  Args:\n",
    "    input_features: The names of the numerical input features to use.\n",
    "  Returns:\n",
    "    A set of feature columns\n",
    "  \"\"\" \n",
    "  return set([tf.feature_column.numeric_column(my_feature)\n",
    "              for my_feature in input_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_linear_classifier_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "  \"\"\"Trains a linear classification model.\n",
    "  \n",
    "  In addition to training, this function also prints training progress information,\n",
    "  as well as a plot of the training and validation loss over time.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: A `float`, the learning rate.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    training_examples: A `DataFrame` containing one or more columns from\n",
    "      `california_housing_dataframe` to use as input features for training.\n",
    "    training_targets: A `DataFrame` containing exactly one column from\n",
    "      `california_housing_dataframe` to use as target for training.\n",
    "    validation_examples: A `DataFrame` containing one or more columns from\n",
    "      `california_housing_dataframe` to use as input features for validation.\n",
    "    validation_targets: A `DataFrame` containing exactly one column from\n",
    "      `california_housing_dataframe` to use as target for validation.\n",
    "      \n",
    "  Returns:\n",
    "    A `LinearClassifier` object trained on the training data.\n",
    "  \"\"\"\n",
    "\n",
    "  periods = 10\n",
    "  steps_per_period = steps / periods\n",
    "  \n",
    "  # Create a linear classifier object.\n",
    "  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)  \n",
    "  linear_classifier = tf.estimator.DNNClassifier(\n",
    "      feature_columns=construct_feature_columns(training_examples),\n",
    "      hidden_units=hidden_units,\n",
    "      optimizer=my_optimizer\n",
    "  )\n",
    "  \n",
    "  # Create input functions.\n",
    "  training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                          training_targets[\"Survived\"], \n",
    "                                          batch_size=batch_size)\n",
    "  predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                                  training_targets[\"Survived\"], \n",
    "                                                  num_epochs=1, \n",
    "                                                  shuffle=False)\n",
    "  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                                    validation_targets[\"Survived\"], \n",
    "                                                    num_epochs=1, \n",
    "                                                    shuffle=False)\n",
    "  \n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print(\"Training model...\")\n",
    "  print(\"LogLoss (on training data):\")\n",
    "  training_log_losses = []\n",
    "  validation_log_losses = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    linear_classifier.train(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps_per_period\n",
    "    )\n",
    "    # Take a break and compute predictions.    \n",
    "    training_probabilities = linear_classifier.predict(input_fn=predict_training_input_fn)\n",
    "    training_probabilities = np.array([item['probabilities'] for item in training_probabilities])\n",
    "#     training_guess = np.array([item['class_ids'] for item in training_probabilities])\n",
    "    \n",
    "    validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "    validation_probabilities = np.array([item['probabilities'] for item in validation_probabilities])\n",
    "#     validation_guess = np.array([item['class_ids'] for item in validation_probabilities])\n",
    "    \n",
    "    training_log_loss = metrics.log_loss(training_targets, training_probabilities)\n",
    "    validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)\n",
    "    # Occasionally print the current loss.\n",
    "    print(\"  period %02d : %0.2f\" % (period, training_log_loss))\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    training_log_losses.append(training_log_loss)\n",
    "    validation_log_losses.append(validation_log_loss)\n",
    "  print(\"Model training finished.\")\n",
    "  \n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.ylabel(\"LogLoss\")\n",
    "  plt.xlabel(\"Periods\")\n",
    "  plt.title(\"LogLoss vs. Periods\")\n",
    "  plt.tight_layout()\n",
    "  plt.plot(training_log_losses, label=\"training\")\n",
    "  plt.plot(validation_log_losses, label=\"validation\")\n",
    "  plt.legend()\n",
    "\n",
    "  return linear_classifier, predict_validation_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_scale(series):\n",
    "  min_val = series.min()\n",
    "  max_val = series.max()\n",
    "  scale = (max_val - min_val) / 2.0\n",
    "  return series.apply(lambda x:((x - min_val) / scale) - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "              ... \n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "----------\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "              ... \n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "Mr        517\n",
      "Miss      182\n",
      "Mrs       125\n",
      "Master     40\n",
      "Misc       27\n",
      "Name: Title, dtype: int64\n",
      "----------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 0 to 890\n",
      "Data columns (total 42 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            891 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       891 non-null object\n",
      "FamilySize     891 non-null int64\n",
      "IsAlone        891 non-null int64\n",
      "Title          891 non-null object\n",
      "SS0            891 non-null float64\n",
      "SS1            891 non-null float64\n",
      "SS2            891 non-null float64\n",
      "3              891 non-null float64\n",
      "4              891 non-null float64\n",
      "5              891 non-null float64\n",
      "8              891 non-null float64\n",
      "PC0            2 non-null float64\n",
      "PC1            2 non-null float64\n",
      "PC2            2 non-null float64\n",
      "3              2 non-null float64\n",
      "4              2 non-null float64\n",
      "5              2 non-null float64\n",
      "6              2 non-null float64\n",
      "female         891 non-null float64\n",
      "male           891 non-null float64\n",
      "C              891 non-null float64\n",
      "Q              891 non-null float64\n",
      "S              891 non-null float64\n",
      "Class1         891 non-null float64\n",
      "Class2         891 non-null float64\n",
      "Class3         891 non-null float64\n",
      "Master         891 non-null float64\n",
      "Misc           891 non-null float64\n",
      "Miss           891 non-null float64\n",
      "Mr             891 non-null float64\n",
      "Mrs            891 non-null float64\n",
      "dtypes: float64(29), int64(7), object(6)\n",
      "memory usage: 299.3+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 0 to 890\n",
      "Data columns (total 42 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            891 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       891 non-null object\n",
      "FamilySize     891 non-null int64\n",
      "IsAlone        891 non-null int64\n",
      "Title          891 non-null object\n",
      "SS0            891 non-null float64\n",
      "SS1            891 non-null float64\n",
      "SS2            891 non-null float64\n",
      "3              891 non-null float64\n",
      "4              891 non-null float64\n",
      "5              891 non-null float64\n",
      "8              891 non-null float64\n",
      "PC0            2 non-null float64\n",
      "PC1            2 non-null float64\n",
      "PC2            2 non-null float64\n",
      "3              2 non-null float64\n",
      "4              2 non-null float64\n",
      "5              2 non-null float64\n",
      "6              2 non-null float64\n",
      "female         891 non-null float64\n",
      "male           891 non-null float64\n",
      "C              891 non-null float64\n",
      "Q              891 non-null float64\n",
      "S              891 non-null float64\n",
      "Class1         891 non-null float64\n",
      "Class2         891 non-null float64\n",
      "Class3         891 non-null float64\n",
      "Master         891 non-null float64\n",
      "Misc           891 non-null float64\n",
      "Miss           891 non-null float64\n",
      "Mr             891 non-null float64\n",
      "Mrs            891 non-null float64\n",
      "dtypes: float64(29), int64(7), object(6)\n",
      "memory usage: 299.3+ KB\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "5            6         0       3   \n",
      "6            7         0       1   \n",
      "\n",
      "                                                Name     Sex  Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male 22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female 38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female 26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female 35.0      1   \n",
      "4                           Allen, Mr. William Henry    male 35.0      0   \n",
      "5                                   Moran, Mr. James    male 28.0      0   \n",
      "6                            McCarthy, Mr. Timothy J    male 54.0      0   \n",
      "\n",
      "   Parch            Ticket  Fare ...    Q   S  Class1  Class2 Class3  Master  \\\n",
      "0      0         A/5 21171   7.2 ...  0.0 1.0     0.0     0.0    1.0     0.0   \n",
      "1      0          PC 17599  71.3 ...  0.0 0.0     1.0     0.0    0.0     0.0   \n",
      "2      0  STON/O2. 3101282   7.9 ...  0.0 1.0     0.0     0.0    1.0     0.0   \n",
      "3      0            113803  53.1 ...  0.0 1.0     1.0     0.0    0.0     0.0   \n",
      "4      0            373450   8.1 ...  0.0 1.0     0.0     0.0    1.0     0.0   \n",
      "5      0            330877   8.5 ...  1.0 0.0     0.0     0.0    1.0     0.0   \n",
      "6      0             17463  51.9 ...  0.0 1.0     1.0     0.0    0.0     0.0   \n",
      "\n",
      "   Misc  Miss  Mr  Mrs  \n",
      "0   0.0   0.0 1.0  0.0  \n",
      "1   0.0   0.0 0.0  1.0  \n",
      "2   0.0   1.0 0.0  0.0  \n",
      "3   0.0   0.0 0.0  1.0  \n",
      "4   0.0   0.0 1.0  0.0  \n",
      "5   0.0   0.0 1.0  0.0  \n",
      "6   0.0   0.0 1.0  0.0  \n",
      "\n",
      "[7 rows x 42 columns]\n",
      "\n",
      "Chosen Features NULLs?\n",
      "\n",
      " female        0\n",
      "Mrs           0\n",
      "Miss          0\n",
      "Class1        0\n",
      "Fare          0\n",
      "             ..\n",
      "Parch         0\n",
      "SS2           0\n",
      "Misc          0\n",
      "FamilySize    0\n",
      "Q             0\n",
      "dtype: int64\n",
      "Index(['female', 'Mrs', 'Miss', 'Class1', 'Fare', 'SS1', 'C', 'Class2',\n",
      "       'Master', 'Parch', 'SS2', 'Misc', 'FamilySize', 'Q'],\n",
      "      dtype='object')\n",
      "YESSS\n",
      "      female  Mrs  Miss  Class1  Fare  SS1    C  Class2  Master  Parch  SS2  \\\n",
      "0      -1.0 -1.0  -1.0    -1.0  -1.0  1.0 -1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "1       1.0  1.0  -1.0     1.0  -0.7  1.0  1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "2       1.0 -1.0   1.0    -1.0  -1.0 -1.0 -1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "3       1.0  1.0  -1.0     1.0  -0.8  1.0 -1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "4      -1.0 -1.0  -1.0    -1.0  -1.0 -1.0 -1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "..      ...  ...   ...     ...   ...  ...  ...     ...     ...    ...  ...   \n",
      "886    -1.0 -1.0  -1.0    -1.0  -0.9 -1.0 -1.0     1.0    -1.0   -1.0 -1.0   \n",
      "887     1.0 -1.0   1.0     1.0  -0.9 -1.0 -1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "888     1.0 -1.0   1.0    -1.0  -0.9  1.0 -1.0    -1.0    -1.0   -0.3 -1.0   \n",
      "889    -1.0 -1.0  -1.0     1.0  -0.9 -1.0  1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "890    -1.0 -1.0  -1.0    -1.0  -1.0 -1.0 -1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "\n",
      "     Misc  FamilySize    Q  \n",
      "0    -1.0        -0.8 -1.0  \n",
      "1    -1.0        -0.8 -1.0  \n",
      "2    -1.0        -1.0 -1.0  \n",
      "3    -1.0        -0.8 -1.0  \n",
      "4    -1.0        -1.0 -1.0  \n",
      "..    ...         ...  ...  \n",
      "886   1.0        -1.0 -1.0  \n",
      "887  -1.0        -1.0 -1.0  \n",
      "888  -1.0        -0.4 -1.0  \n",
      "889  -1.0        -1.0 -1.0  \n",
      "890  -1.0        -1.0  1.0  \n",
      "\n",
      "[891 rows x 14 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harrisonfsmith95/anaconda3/lib/python3.5/site-packages/pandas/core/indexing.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "def normalize_linear_scale(examples_dataframe):#, features1):\n",
    "  \"\"\"Returns a version of the input `DataFrame` that has all its features normalized linearly.\"\"\"\n",
    "  #\n",
    "  # Your code here: normalize the inputs.\n",
    "  #\n",
    "    \n",
    "#   examples_dataframe = examples_dataframe[examples_dataframe[\"TotRmsAbvGrd\"] <= 13]\n",
    "#   examples_dataframe = examples_dataframe[examples_dataframe[\"OverallQual\"] <= 9.5]\n",
    "#   examples_dataframe = examples_dataframe[examples_dataframe[\"GrLivArea\"] <= 3500]\n",
    "\n",
    "#   examples_dataframe.dropna()\n",
    "\n",
    "  new = pd.DataFrame()\n",
    "  for i in examples_dataframe.columns:\n",
    "#     print(examples_dataframe[i])\n",
    "    new = pd.concat([new, linear_scale(examples_dataframe[i])], axis=1)\n",
    "  \n",
    "  return new#examples_dataframe.apply(lambda x: linear_scale(x))\n",
    "\n",
    "# def find_na(df):\n",
    "#     l = []\n",
    "#     for label in df.columns:\n",
    "#         df.dropna()\n",
    "# #         if df[label].dropna().shape[0] != df.shape[0]:\n",
    "# #             l.append(label)\n",
    "#     return df\n",
    "\n",
    "correlation_df2 = preprocess_dataframe(titanic_dataframe, 1)#, features1)\n",
    "\n",
    "normalized_dataframe, features_index = preprocess_features(correlation_df2, 1)#correlation_df2)\n",
    "\n",
    "# normalized_dataframe.dropna()\n",
    "# print(normalized_dataframe.isnull().sum())\n",
    "normalized_training_examples = normalize_linear_scale(normalized_dataframe.head(1000))\n",
    "new_training_targets = preprocess_targets(correlation_df2.head(1000))\n",
    "\n",
    "normalized_validation_examples = normalize_linear_scale(normalized_dataframe.tail(460))\n",
    "new_validation_targets = preprocess_targets(correlation_df2.head(460))\n",
    "\n",
    "print(\"YESSS\\n\", normalized_training_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "LogLoss (on training data):\n",
      "  period 00 : 0.35\n",
      "  period 01 : 0.35\n",
      "  period 02 : 0.34\n",
      "  period 03 : 0.34\n",
      "  period 04 : 0.34\n",
      "  period 05 : 0.33\n",
      "  period 06 : 0.33\n",
      "  period 07 : 0.33\n",
      "  period 08 : 0.33\n",
      "  period 09 : 0.33\n",
      "Model training finished.\n",
      "AUC on the validation set: 0.51\n",
      "Accuracy on the validation set: 0.55\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEbCAYAAACV0PCVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUxwofTAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa40d729cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# index1 = list(normalized_training_examples.index.values)\n",
    "# # print(indexs)\n",
    "# index2 = list(normalized_validation_examples.index.values)\n",
    "\n",
    "# print(normalized_training_examples)\n",
    "# print(normalized_validation_examples)\n",
    "\n",
    "linear_classifier, predict_validation_input_fn = train_linear_classifier_model(\n",
    "    learning_rate=0.05,\n",
    "    steps=50000,\n",
    "    batch_size=100,\n",
    "    hidden_units=[15, 10],\n",
    "    training_examples=normalized_training_examples,\n",
    "    training_targets=new_training_targets,\n",
    "    validation_examples=normalized_validation_examples,\n",
    "    validation_targets=new_validation_targets)\n",
    "\n",
    "evaluation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\n",
    "\n",
    "print(\"AUC on the validation set: %0.2f\" % evaluation_metrics['auc'])\n",
    "print(\"Accuracy on the validation set: %0.2f\" % evaluation_metrics['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['female', 'Mrs', 'Miss', 'Class1', 'Fare', 'SS1', 'C', 'Class2',\n",
      "       'Master', 'Parch', 'SS2', 'Misc', 'FamilySize', 'Q'],\n",
      "      dtype='object')\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age              0\n",
      "              ... \n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "Survived         0\n",
      "dtype: int64\n",
      "----------\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age              0\n",
      "              ... \n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "Survived         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harrisonfsmith95/anaconda3/lib/python3.5/site-packages/pandas/core/indexing.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr        240\n",
      "Miss       78\n",
      "Mrs        72\n",
      "Master     21\n",
      "Misc        7\n",
      "Name: Title, dtype: int64\n",
      "----------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 43 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Pclass         418 non-null int64\n",
      "Name           418 non-null object\n",
      "Sex            418 non-null object\n",
      "Age            418 non-null float64\n",
      "SibSp          418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Ticket         418 non-null object\n",
      "Fare           418 non-null float64\n",
      "Cabin          91 non-null object\n",
      "Embarked       418 non-null object\n",
      "Survived       418 non-null int64\n",
      "FamilySize     418 non-null int64\n",
      "IsAlone        418 non-null int64\n",
      "Title          418 non-null object\n",
      "SS0            418 non-null float64\n",
      "SS1            418 non-null float64\n",
      "SS2            418 non-null float64\n",
      "3              418 non-null float64\n",
      "4              418 non-null float64\n",
      "5              418 non-null float64\n",
      "8              418 non-null float64\n",
      "PC0            2 non-null float64\n",
      "PC1            2 non-null float64\n",
      "PC2            2 non-null float64\n",
      "3              2 non-null float64\n",
      "4              2 non-null float64\n",
      "5              2 non-null float64\n",
      "6              2 non-null float64\n",
      "9              2 non-null float64\n",
      "female         418 non-null float64\n",
      "male           418 non-null float64\n",
      "C              418 non-null float64\n",
      "Q              418 non-null float64\n",
      "S              418 non-null float64\n",
      "Class1         418 non-null float64\n",
      "Class2         418 non-null float64\n",
      "Class3         418 non-null float64\n",
      "Master         418 non-null float64\n",
      "Misc           418 non-null float64\n",
      "Miss           418 non-null float64\n",
      "Mr             418 non-null float64\n",
      "Mrs            418 non-null float64\n",
      "dtypes: float64(30), int64(7), object(6)\n",
      "memory usage: 140.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 43 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Pclass         418 non-null int64\n",
      "Name           418 non-null object\n",
      "Sex            418 non-null object\n",
      "Age            418 non-null float64\n",
      "SibSp          418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Ticket         418 non-null object\n",
      "Fare           418 non-null float64\n",
      "Cabin          91 non-null object\n",
      "Embarked       418 non-null object\n",
      "Survived       418 non-null int64\n",
      "FamilySize     418 non-null int64\n",
      "IsAlone        418 non-null int64\n",
      "Title          418 non-null object\n",
      "SS0            418 non-null float64\n",
      "SS1            418 non-null float64\n",
      "SS2            418 non-null float64\n",
      "3              418 non-null float64\n",
      "4              418 non-null float64\n",
      "5              418 non-null float64\n",
      "8              418 non-null float64\n",
      "PC0            2 non-null float64\n",
      "PC1            2 non-null float64\n",
      "PC2            2 non-null float64\n",
      "3              2 non-null float64\n",
      "4              2 non-null float64\n",
      "5              2 non-null float64\n",
      "6              2 non-null float64\n",
      "9              2 non-null float64\n",
      "female         418 non-null float64\n",
      "male           418 non-null float64\n",
      "C              418 non-null float64\n",
      "Q              418 non-null float64\n",
      "S              418 non-null float64\n",
      "Class1         418 non-null float64\n",
      "Class2         418 non-null float64\n",
      "Class3         418 non-null float64\n",
      "Master         418 non-null float64\n",
      "Misc           418 non-null float64\n",
      "Miss           418 non-null float64\n",
      "Mr             418 non-null float64\n",
      "Mrs            418 non-null float64\n",
      "dtypes: float64(30), int64(7), object(6)\n",
      "memory usage: 140.5+ KB\n",
      "     PassengerId  Pclass                                          Name  \\\n",
      "0            892       3                              Kelly, Mr. James   \n",
      "1            893       3              Wilkes, Mrs. James (Ellen Needs)   \n",
      "2            894       2                     Myles, Mr. Thomas Francis   \n",
      "3            895       3                              Wirz, Mr. Albert   \n",
      "4            896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
      "..           ...     ...                                           ...   \n",
      "413         1305       3                            Spector, Mr. Woolf   \n",
      "414         1306       1                  Oliva y Ocana, Dona. Fermina   \n",
      "415         1307       3                  Saether, Mr. Simon Sivertsen   \n",
      "416         1308       3                           Ware, Mr. Frederick   \n",
      "417         1309       3                      Peter, Master. Michael J   \n",
      "\n",
      "        Sex  Age  SibSp  Parch              Ticket  Fare Cabin ...    Q   S  \\\n",
      "0      male 34.5      0      0              330911   7.8   NaN ...  1.0 0.0   \n",
      "1    female 47.0      1      0              363272   7.0   NaN ...  0.0 1.0   \n",
      "2      male 62.0      0      0              240276   9.7   NaN ...  1.0 0.0   \n",
      "3      male 27.0      0      0              315154   8.7   NaN ...  0.0 1.0   \n",
      "4    female 22.0      1      1             3101298  12.3   NaN ...  0.0 1.0   \n",
      "..      ...  ...    ...    ...                 ...   ...   ... ...   ..  ..   \n",
      "413    male 27.0      0      0           A.5. 3236   8.1   NaN ...  0.0 1.0   \n",
      "414  female 39.0      0      0            PC 17758 108.9  C105 ...  0.0 0.0   \n",
      "415    male 38.5      0      0  SOTON/O.Q. 3101262   7.2   NaN ...  0.0 1.0   \n",
      "416    male 27.0      0      0              359309   8.1   NaN ...  0.0 1.0   \n",
      "417    male 27.0      1      1                2668  22.4   NaN ...  0.0 0.0   \n",
      "\n",
      "     Class1  Class2 Class3  Master  Misc  Miss  Mr  Mrs  \n",
      "0       0.0     0.0    1.0     0.0   0.0   0.0 1.0  0.0  \n",
      "1       0.0     0.0    1.0     0.0   0.0   0.0 0.0  1.0  \n",
      "2       0.0     1.0    0.0     0.0   0.0   0.0 1.0  0.0  \n",
      "3       0.0     0.0    1.0     0.0   0.0   0.0 1.0  0.0  \n",
      "4       0.0     0.0    1.0     0.0   0.0   0.0 0.0  1.0  \n",
      "..      ...     ...    ...     ...   ...   ...  ..  ...  \n",
      "413     0.0     0.0    1.0     0.0   0.0   0.0 1.0  0.0  \n",
      "414     1.0     0.0    0.0     0.0   1.0   0.0 0.0  0.0  \n",
      "415     0.0     0.0    1.0     0.0   0.0   0.0 1.0  0.0  \n",
      "416     0.0     0.0    1.0     0.0   0.0   0.0 1.0  0.0  \n",
      "417     0.0     0.0    1.0     1.0   0.0   0.0 0.0  0.0  \n",
      "\n",
      "[418 rows x 43 columns]\n",
      "examples\n",
      "      female  Mrs  Miss  Class1  Fare  SS1    C  Class2  Master  Parch  SS2  \\\n",
      "0      -1.0 -1.0  -1.0    -1.0  -1.0 -1.0 -1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "1       1.0  1.0  -1.0    -1.0  -1.0  1.0 -1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "2      -1.0 -1.0  -1.0    -1.0  -1.0 -1.0 -1.0     1.0    -1.0   -1.0 -1.0   \n",
      "3      -1.0 -1.0  -1.0    -1.0  -1.0 -1.0 -1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "4       1.0  1.0  -1.0    -1.0  -1.0  1.0 -1.0    -1.0    -1.0   -0.8 -1.0   \n",
      "..      ...  ...   ...     ...   ...  ...  ...     ...     ...    ...  ...   \n",
      "413    -1.0 -1.0  -1.0    -1.0  -1.0 -1.0 -1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "414     1.0 -1.0  -1.0     1.0  -0.6 -1.0  1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "415    -1.0 -1.0  -1.0    -1.0  -1.0 -1.0 -1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "416    -1.0 -1.0  -1.0    -1.0  -1.0 -1.0 -1.0    -1.0    -1.0   -1.0 -1.0   \n",
      "417    -1.0 -1.0  -1.0    -1.0  -0.9  1.0  1.0    -1.0     1.0   -0.8 -1.0   \n",
      "\n",
      "     Misc  FamilySize    Q  \n",
      "0    -1.0        -1.0  1.0  \n",
      "1    -1.0        -0.8 -1.0  \n",
      "2    -1.0        -1.0  1.0  \n",
      "3    -1.0        -1.0 -1.0  \n",
      "4    -1.0        -0.6 -1.0  \n",
      "..    ...         ...  ...  \n",
      "413  -1.0        -1.0 -1.0  \n",
      "414   1.0        -1.0 -1.0  \n",
      "415  -1.0        -1.0 -1.0  \n",
      "416  -1.0        -1.0 -1.0  \n",
      "417  -1.0        -0.6 -1.0  \n",
      "\n",
      "[418 rows x 14 columns]\n",
      "targets      Survived\n",
      "0           0\n",
      "1           1\n",
      "2           0\n",
      "3           0\n",
      "4           1\n",
      "..        ...\n",
      "413         0\n",
      "414         1\n",
      "415         0\n",
      "416         0\n",
      "417         0\n",
      "\n",
      "[418 rows x 1 columns]\n",
      "female        0\n",
      "Mrs           0\n",
      "Miss          0\n",
      "Class1        0\n",
      "Fare          0\n",
      "             ..\n",
      "Parch         0\n",
      "SS2           0\n",
      "Misc          0\n",
      "FamilySize    0\n",
      "Q             0\n",
      "dtype: int64\n",
      "Accuracy on test data: 0.85\n",
      "     Survived\n",
      "0           0\n",
      "1           1\n",
      "2           0\n",
      "3           0\n",
      "4           1\n",
      "..        ...\n",
      "413         0\n",
      "414         1\n",
      "415         0\n",
      "416         0\n",
      "417         1\n",
      "\n",
      "[418 rows x 1 columns]\n",
      "             Survived\n",
      "PassengerId          \n",
      "892                 0\n",
      "893                 1\n",
      "894                 0\n",
      "895                 0\n",
      "896                 1\n",
      "...               ...\n",
      "1305                0\n",
      "1306                1\n",
      "1307                0\n",
      "1308                0\n",
      "1309                1\n",
      "\n",
      "[418 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "titanic_test_data = pd.read_csv(\"titanic/test.csv\", sep=\",\")\n",
    "titanic_validation_data = pd.read_csv(\"titanic/gender_submission.csv\", sep=\",\")\n",
    "print(features_index)\n",
    "# inds = pd.isnull(titanic_test_data).any().notnull()#.nonzero()[0]\n",
    "# inds = np.where(titanic_test_data.notnull())\n",
    "\n",
    "# print(titanic_test_data.notnull().sum())\n",
    "# print(inds)\n",
    "# titanic_validation_data = titanic_validation_data.iloc[titanic_test_data.isnull().index()]\n",
    "# titanic_test_data = titanic_test_data.dropna()#iloc[inds[0]]\n",
    "\n",
    "# normalize_linear_scale(preprocess_features(titanic_dataframe))\n",
    "\n",
    "\n",
    "# # titanic_validation_data = titanic_validation_data.iloc[inds[0]]\n",
    "# # for i in titanic_test_data:\n",
    "    \n",
    "    \n",
    "# # print(titanic_test_data.isnull().sum())\n",
    "\n",
    "new1_titanic_test_data = pd.concat([titanic_test_data, titanic_validation_data['Survived']], axis=1)\n",
    "new_titanic_test_data = preprocess_dataframe(new1_titanic_test_data, 1)\n",
    "# yo = new2_titanic_test_data[features_index]\n",
    "# yo.drop('PassengerId', axis=1)\n",
    "# # new_titanic_test_data = pd.concat([yo, new2_titanic_test_data['PassengerId']], axis=1)\n",
    "print(new_titanic_test_data)#.index.values\n",
    "\n",
    "\n",
    "# print(features_index[2:-1])\n",
    "#######\n",
    "# features_index_n = features_index[2:]\n",
    "# print(new_titanic_test_data[features_index])\n",
    "# features_index = pd.concat([features_index, ['PassengerId']], axis=1)\n",
    "new_examples = normalize_linear_scale(preprocess_test_features(new_titanic_test_data, features_index, 'PassengerId'))#.dropna()\n",
    "new_targets = preprocess_targets(new_titanic_test_data)#.dropna())\n",
    "\n",
    "# new_examples.set_index('PassengerId')\n",
    "# new_targets.set_index('PassengerId')\n",
    "print(\"examples\\n\",new_examples)\n",
    "print(\"targets\",new_targets)\n",
    "\n",
    "# new_examples.fillna()\n",
    "# print(\"YESSSSSSSS\",new_examples.isnull().sum())\n",
    "# # ### Double-check\n",
    "# print(\"New examples summary:\")\n",
    "# display.display(new_examples.describe())\n",
    "# print(\"New Targets summary:\")\n",
    "# display.display(new_targets.describe())\n",
    "\n",
    "# new_examples.drop(new_examples.index[[2577-1461,2121-1461]], inplace=True)\n",
    "# new_targets.drop(new_targets.index[[2577-1461,2121-1461]], inplace=True)\n",
    "\n",
    "# new_examples = new_examples.fillna(0)\n",
    "new_examples = new_examples.fillna(new_examples.mean())\n",
    "print(new_examples.isnull().sum())\n",
    "# print(new_examples)\n",
    "# # new1.sort_values(ascending=True)\n",
    "\n",
    "# new_examples['Fare'].fillna(new_examples['Fare'].mean())\n",
    "# print(new_examples.isnull().sum())\n",
    "\n",
    "#############\n",
    "# predict_new_input_fn = lambda: my_input_fn(new_examples, new_targets[\"Survived\"], num_epochs=1, shuffle=False)\n",
    "\n",
    "# # new_predictions = linear_classifier.predict(input_fn=predict_new_input_fn)\n",
    "# # new_predictions = np.array([item['probabilities'][0] for item in new_predictions])\n",
    "\n",
    "\n",
    "# evaluation_metrics_2 = linear_classifier.evaluate(input_fn=predict_new_input_fn)\n",
    "\n",
    "# print(\"AUC on the validation set: %0.2f\" % evaluation_metrics_2['auc'])\n",
    "# print(\"Accuracy on the validation set: %0.2f\" % evaluation_metrics_2['accuracy'])\n",
    "\n",
    "# print(evaluation_metrics_2)\n",
    "\n",
    "##################\n",
    "\n",
    "predict_test_input_fn = lambda: my_input_fn(new_examples, new_targets[\"Survived\"], num_epochs=1, shuffle=False)\n",
    "\n",
    "test_predictions = linear_classifier.predict(input_fn=predict_test_input_fn)\n",
    "test_predictions = np.array([item['class_ids'][0] for item in test_predictions])\n",
    "  \n",
    "accuracy = metrics.accuracy_score(new_targets, test_predictions)\n",
    "print(\"Accuracy on test data: %0.2f\" % accuracy)\n",
    "# print(test_predictions)\n",
    "\n",
    "# #Obtain mean of columns as you need, nanmean is just convenient.\n",
    "# col_mean = np.nanmean(new_predictions, axis=0)\n",
    "# print(\"avg\",col_mean)\n",
    "\n",
    "\n",
    "# #Find indicies that you need to replace\n",
    "# inds = np.where(np.isnan(new_predictions))\n",
    "# print(inds)\n",
    "\n",
    "# #Place column means in the indices. Align the arrays using take\n",
    "# new_predictions[inds] = [col_mean]#np.take(col_mean, inds[1])\n",
    "# # # print(\"YES\")\n",
    "# print(new_predictions)\n",
    "# ##############################\n",
    "\n",
    "\n",
    "# # new_preditions=new_predictions[0:2577-1461] = 100\n",
    "\n",
    "# new_predictions.fillna(new_predictions.mean())\n",
    "\n",
    "# new_predictions = new_predictions[0:2121-1462]\n",
    "# new_targets = new_targets[0:2121-1462]# = 100\n",
    "# # new = pd.DataFrame()\n",
    "# # new['SalePrice'] = 0\n",
    "new = pd.DataFrame(test_predictions, columns=[\"Survived\"])#[1], index=new_predictions[0])\n",
    "print(new)\n",
    "# new.index.name = 'PassengerId'\n",
    "new_targets.index.name = 'PassengerId'\n",
    "new.rename({0:\"Survived\"})#, axis='columns')#[0].rename = 'SalePrice'\n",
    "# pred = new\n",
    "new.index += 892\n",
    "new.index.name = 'PassengerId'\n",
    "# new = pd.concat([new_targets, new], axis=1)\n",
    "# new[\"PassengerId\"] *= 1000\n",
    "new.to_csv('predictions_3.csv')\n",
    "# print(new.isnull().sum())\n",
    "print(new)\n",
    "\n",
    "\n",
    "\n",
    "# new_root_mean_squared_error = math.sqrt(metrics.mean_squared_error(pred, new_targets))\n",
    "# # female  Class1  Fare    C  SS1  PC1  PC2\n",
    "# print(\"Final RMSE (on test data): %0.2f\" % new_root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_new_input_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-3b23c4700648>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluation_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict_new_input_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AUC on the validation set: %0.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mevaluation_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'auc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy on the validation set: %0.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mevaluation_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_new_input_fn' is not defined"
     ]
    }
   ],
   "source": [
    "evaluation_metrics = linear_classifier.evaluate(input_fn=predict_new_input_fn)\n",
    "\n",
    "print(\"AUC on the validation set: %0.2f\" % evaluation_metrics['auc'])\n",
    "print(\"Accuracy on the validation set: %0.2f\" % evaluation_metrics['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.scatter(training_examples[\"OverallQual\"], training_targets[\"SalePrice\"])\n",
    "# plt.scatter(normalized_training_examples[\"OverallQual\"], training_targets[\"SalePrice\"])\n",
    "plt.scatter(normalized_training_examples[\"GrLivArea\"], training_targets[\"SalePrice\"])\n",
    "# plt.scatter(training_examples[\"TotalBsmtSF\"], training_targets[\"SalePrice\"])\n",
    "# plt.scatter(normalized_training_examples[\"GarageArea\"], training_targets[\"SalePrice\"])\n",
    "# plt.scatter(training_examples[\"GarageYrBlt\"], training_targets[\"SalePrice\"])\n",
    "# training_examples[\"OverallQual\"].hist()\n",
    "# training_examples[\"GrLivArea\"].hist()\n",
    "# training_examples[\"TotalBsmtSF\"].hist()\n",
    "# training_examples[\"GarageArea\"].hist()\n",
    "# training_examples[\"GarageYrBlt\"]#.hist()\n",
    "\n",
    "_ = normalized_training_examples.hist(bins=20, figsize=(18, 12), xlabelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(new_targets)\n",
    "# print(new)\n",
    "# plt.scatter(new_targets.index, new_targets[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.scatter(new.index, new[\"SalePrice\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
