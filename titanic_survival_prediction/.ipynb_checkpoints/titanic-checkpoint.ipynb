{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>32.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>49.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>512.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId  Survived  Pclass   Age  SibSp  Parch  Fare\n",
       "count        891.0     891.0   891.0 714.0  891.0  891.0 891.0\n",
       "mean         446.0       0.4     2.3  29.7    0.5    0.4  32.2\n",
       "std          257.4       0.5     0.8  14.5    1.1    0.8  49.7\n",
       "min            1.0       0.0     1.0   0.4    0.0    0.0   0.0\n",
       "25%          223.5       0.0     2.0  20.1    0.0    0.0   7.9\n",
       "50%          446.0       0.0     3.0  28.0    0.0    0.0  14.5\n",
       "75%          668.5       1.0     3.0  38.0    1.0    0.0  31.0\n",
       "max          891.0       1.0     3.0  80.0    8.0    6.0 512.3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "\n",
    "titanic_dataframe = pd.read_csv(\"titanic/train.csv\", sep=\",\")\n",
    "\n",
    "titanic_dataframe = titanic_dataframe.reindex(\n",
    "    np.random.permutation(titanic_dataframe.index))\n",
    "\n",
    "titanic_dataframe.describe()\n",
    "\n",
    "# titanic_dataframe = titanic_dataframe[titanic_dataframe[\"TotRmsAbvGrd\"] <= 13]\n",
    "# titanic_dataframe = titanic_dataframe[titanic_dataframe[\"OverallQual\"] <= 9.5]\n",
    "# titanic_dataframe = titanic_dataframe[titanic_dataframe[\"GrLivArea\"] <= 3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# titanic_dataframe[\"Cabin\"].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_features(titanic_dataframe, drop):#, features):\n",
    "  \"\"\"Prepares input features from California housing data set.\n",
    "\n",
    "  Args:\n",
    "    titanic_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the California housing data set.\n",
    "  Returns:\n",
    "    A DataFrame that contains the features to be used for the model, including\n",
    "    synthetic features.\n",
    "  \"\"\"\n",
    "  correlation_dataframe = titanic_dataframe.copy()\n",
    "  correlation_dataframe[\"target\"] = correlation_dataframe[\"Survived\"]\n",
    "  if drop:#creat drop function\n",
    "    plt.scatter(correlation_dataframe['Age'], correlation_dataframe[\"target\"])\n",
    "\n",
    "    correlation_dataframe = correlation_dataframe.dropna(subset = ['Age'])# correlation_dataframe['Age'].dropna()\n",
    "    correlation_dataframe = correlation_dataframe.drop(['Cabin'], axis=1)# correlation_dataframe['Fare'].dropna()\n",
    "    correlation_dataframe = correlation_dataframe.dropna(subset = ['Embarked'])\n",
    "\n",
    "  new = pd.DataFrame(correlation_dataframe)#.copy#.corr()\n",
    "  new = pd.concat([correlation_dataframe, pd.get_dummies(correlation_dataframe['SibSp']).rename(columns={0:\"SS0\", 1:'SS1', 2:'SS2'}),\n",
    "                 pd.get_dummies(correlation_dataframe['Parch']).rename(columns={0:\"PC0\", 1:'PC1', 2:'PC2'}),\n",
    "                 pd.get_dummies(correlation_dataframe['Sex']), pd.get_dummies(correlation_dataframe['Embarked']),\n",
    "                pd.get_dummies(correlation_dataframe['Pclass']).rename(columns={1:'Class1', 2:'Class2', 3:\"Class3\"})],\n",
    "                axis=1)\n",
    "\n",
    "  print(new.head(7))\n",
    "\n",
    "\n",
    "  new1 = new.corr()\n",
    "  features_i = new1['target'].sort_values(ascending=False).head(10)\n",
    "#   features_i\n",
    "\n",
    "  # new.hist(figsize=(20,20))\n",
    "  # print(features)\n",
    "  features_index = features_i.index\n",
    "  print(\"\\nChosen Features NULLs?\\n\\n\", new[features_index].isnull().sum())\n",
    "\n",
    "  selected_features = new[features_index]\n",
    "  processed_features = selected_features.copy()\n",
    "\n",
    "  return processed_features\n",
    "\n",
    "def preprocess_targets(titanic_dataframe):\n",
    "  \"\"\"Prepares target features (i.e., labels) from California housing data set.\n",
    "\n",
    "  Args:\n",
    "    titanic_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the California housing data set.\n",
    "  Returns:\n",
    "    A DataFrame that contains the target feature.\n",
    "  \"\"\"\n",
    "  output_targets = pd.DataFrame()\n",
    "  output_targets[\"Survived\"] = titanic_dataframe[\"Survived\"]\n",
    "  return output_targets\n",
    "# titanic_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "150          151         0       2   \n",
      "148          149         0       2   \n",
      "412          413         1       1   \n",
      "709          710         1       3   \n",
      "707          708         1       1   \n",
      "744          745         1       3   \n",
      "279          280         1       3   \n",
      "\n",
      "                                                  Name     Sex  Age  SibSp  \\\n",
      "150                         Bateman, Rev. Robert James    male 51.0      0   \n",
      "148           Navratil, Mr. Michel (\"Louis M Hoffman\")    male 36.5      0   \n",
      "412                             Minahan, Miss. Daisy E  female 33.0      1   \n",
      "709  Moubarek, Master. Halim Gonios (\"William George\")    male  nan      1   \n",
      "707                  Calderhead, Mr. Edward Pennington    male 42.0      0   \n",
      "744                                 Stranden, Mr. Juho    male 31.0      0   \n",
      "279                   Abbott, Mrs. Stanton (Rosa Hunt)  female 35.0      1   \n",
      "\n",
      "     Parch             Ticket  Fare   ...     4   5  female  male   C   Q   S  \\\n",
      "150      0        S.O.P. 1166  12.5   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0   \n",
      "148      2             230080  26.0   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0   \n",
      "412      0              19928  90.0   ...   0.0 0.0     1.0   0.0 0.0 1.0 0.0   \n",
      "709      1               2661  15.2   ...   0.0 0.0     0.0   1.0 1.0 0.0 0.0   \n",
      "707      0           PC 17476  26.3   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0   \n",
      "744      0  STON/O 2. 3101288   7.9   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0   \n",
      "279      1          C.A. 2673  20.2   ...   0.0 0.0     1.0   0.0 0.0 0.0 1.0   \n",
      "\n",
      "     Class1  Class2  Class3  \n",
      "150     0.0     1.0     0.0  \n",
      "148     0.0     1.0     0.0  \n",
      "412     1.0     0.0     0.0  \n",
      "709     0.0     0.0     1.0  \n",
      "707     1.0     0.0     0.0  \n",
      "744     0.0     0.0     1.0  \n",
      "279     0.0     0.0     1.0  \n",
      "\n",
      "[7 rows x 34 columns]\n",
      "\n",
      "Chosen Features NULLs?\n",
      "\n",
      " Survived    0\n",
      "target      0\n",
      "female      0\n",
      "Class1      0\n",
      "Fare        0\n",
      "SS1         0\n",
      "PC1         0\n",
      "C           0\n",
      "Class2      0\n",
      "Parch       0\n",
      "dtype: int64\n",
      "     PassengerId  Survived  Pclass  \\\n",
      "828          829         1       3   \n",
      "32            33         1       3   \n",
      "571          572         1       1   \n",
      "494          495         0       3   \n",
      "346          347         1       2   \n",
      "498          499         0       1   \n",
      "677          678         1       3   \n",
      "\n",
      "                                                Name     Sex  Age  SibSp  \\\n",
      "828                     McCormack, Mr. Thomas Joseph    male  nan      0   \n",
      "32                          Glynn, Miss. Mary Agatha  female  nan      0   \n",
      "571    Appleton, Mrs. Edward Dale (Charlotte Lamson)  female 53.0      2   \n",
      "494                       Stanley, Mr. Edward Roland    male 21.0      0   \n",
      "346                        Smith, Miss. Marion Elsie  female 40.0      0   \n",
      "498  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female 25.0      1   \n",
      "677                          Turja, Miss. Anna Sofia  female 18.0      0   \n",
      "\n",
      "     Parch     Ticket  Fare   ...     5   6  female  male   C   Q   S  Class1  \\\n",
      "828      0     367228   7.8   ...   0.0 0.0     0.0   1.0 0.0 1.0 0.0     0.0   \n",
      "32       0     335677   7.8   ...   0.0 0.0     1.0   0.0 0.0 1.0 0.0     0.0   \n",
      "571      0      11769  51.5   ...   0.0 0.0     1.0   0.0 0.0 0.0 1.0     1.0   \n",
      "494      0  A/4 45380   8.1   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0     0.0   \n",
      "346      0      31418  13.0   ...   0.0 0.0     1.0   0.0 0.0 0.0 1.0     0.0   \n",
      "498      2     113781 151.6   ...   0.0 0.0     1.0   0.0 0.0 0.0 1.0     1.0   \n",
      "677      0       4138   9.8   ...   0.0 0.0     1.0   0.0 0.0 0.0 1.0     0.0   \n",
      "\n",
      "     Class2  Class3  \n",
      "828     0.0     1.0  \n",
      "32      0.0     1.0  \n",
      "571     0.0     0.0  \n",
      "494     0.0     1.0  \n",
      "346     1.0     0.0  \n",
      "498     0.0     0.0  \n",
      "677     0.0     1.0  \n",
      "\n",
      "[7 rows x 35 columns]\n",
      "\n",
      "Chosen Features NULLs?\n",
      "\n",
      " Survived    0\n",
      "target      0\n",
      "female      0\n",
      "Class1      0\n",
      "Fare        0\n",
      "C           0\n",
      "SS1         0\n",
      "PC1         0\n",
      "PC2         0\n",
      "Parch       0\n",
      "dtype: int64\n",
      "Training examples summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>target</th>\n",
       "      <th>female</th>\n",
       "      <th>Class1</th>\n",
       "      <th>Fare</th>\n",
       "      <th>SS1</th>\n",
       "      <th>PC1</th>\n",
       "      <th>C</th>\n",
       "      <th>Class2</th>\n",
       "      <th>Parch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>691.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>691.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>51.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>512.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Survived  target  female  Class1  Fare   SS1   PC1     C  Class2  Parch\n",
       "count     691.0   691.0   691.0   691.0 691.0 691.0 691.0 691.0   691.0  691.0\n",
       "mean        0.4     0.4     0.4     0.3  32.7   0.2   0.1   0.2     0.2    0.4\n",
       "std         0.5     0.5     0.5     0.4  51.7   0.4   0.3   0.4     0.4    0.8\n",
       "min         0.0     0.0     0.0     0.0   0.0   0.0   0.0   0.0     0.0    0.0\n",
       "25%         0.0     0.0     0.0     0.0   7.9   0.0   0.0   0.0     0.0    0.0\n",
       "50%         0.0     0.0     0.0     0.0  14.5   0.0   0.0   0.0     0.0    0.0\n",
       "75%         1.0     1.0     1.0     1.0  31.0   0.0   0.0   0.0     0.0    0.0\n",
       "max         1.0     1.0     1.0     1.0 512.3   1.0   1.0   1.0     1.0    5.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation examples summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>target</th>\n",
       "      <th>female</th>\n",
       "      <th>Class1</th>\n",
       "      <th>Fare</th>\n",
       "      <th>C</th>\n",
       "      <th>SS1</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>Parch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>29.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>37.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Survived  target  female  Class1  Fare     C   SS1   PC1   PC2  Parch\n",
       "count     460.0   460.0   460.0   460.0 460.0 460.0 460.0 460.0 460.0  460.0\n",
       "mean        0.4     0.4     0.3     0.3  29.9   0.2   0.2   0.1   0.1    0.4\n",
       "std         0.5     0.5     0.5     0.4  37.1   0.4   0.4   0.3   0.3    0.8\n",
       "min         0.0     0.0     0.0     0.0   0.0   0.0   0.0   0.0   0.0    0.0\n",
       "25%         0.0     0.0     0.0     0.0   7.9   0.0   0.0   0.0   0.0    0.0\n",
       "50%         0.0     0.0     0.0     0.0  14.5   0.0   0.0   0.0   0.0    0.0\n",
       "75%         1.0     1.0     1.0     1.0  32.4   0.0   0.0   0.0   0.0    0.0\n",
       "max         1.0     1.0     1.0     1.0 263.0   1.0   1.0   1.0   1.0    6.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training targets summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>691.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Survived\n",
       "count     691.0\n",
       "mean        0.4\n",
       "std         0.5\n",
       "min         0.0\n",
       "25%         0.0\n",
       "50%         0.0\n",
       "75%         1.0\n",
       "max         1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation targets summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Survived\n",
       "count     460.0\n",
       "mean        0.4\n",
       "std         0.5\n",
       "min         0.0\n",
       "25%         0.0\n",
       "50%         0.0\n",
       "75%         1.0\n",
       "max         1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose the first 12000 (out of 17000) examples for training.\n",
    "training_examples = preprocess_features(titanic_dataframe.head(691), 0)\n",
    "training_targets = preprocess_targets(titanic_dataframe.head(691))\n",
    "#\n",
    "# correlation_dataframe = titanic_dataframe.copy()\n",
    "# correlation_dataframe[\"target\"] = training_targets[\"SalePrice\"]\n",
    "# new = correlation_dataframe.corr()\n",
    "# print(new)\n",
    "# features1 = new['target'].sort_values(ascending=False).head(10).index\n",
    "# features1 = features1[2:-1]\n",
    "# #\n",
    "# training_examples = preprocess_features(titanic_dataframe.head(1000), features1)\n",
    "\n",
    "# Choose the last 5000 (out of 17000) examples for validation.\n",
    "validation_examples = preprocess_features(titanic_dataframe.tail(460), 0)#, features1)\n",
    "validation_targets = preprocess_targets(titanic_dataframe.tail(460))\n",
    "\n",
    "# display.display(titanic_dataframe.describe())\n",
    "\n",
    "# Double-check that we've done the right thing.\n",
    "print(\"Training examples summary:\")\n",
    "display.display(training_examples.describe())\n",
    "print(\"Validation examples summary:\")\n",
    "display.display(validation_examples.describe())\n",
    "\n",
    "print(\"Training targets summary:\")\n",
    "display.display(training_targets.describe())\n",
    "print(\"Validation targets summary:\")\n",
    "display.display(validation_targets.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #find good feratures based on sale price\n",
    "# correlation_dataframe = titanic_dataframe.copy()\n",
    "# correlation_dataframe[\"target\"] = titanic_dataframe[\"SalePrice\"]\n",
    "\n",
    "# # display.display(training_targets.describe())\n",
    "\n",
    "# correlation_dataframe.corr()\n",
    "# new = pd.DataFrame()\n",
    "# new['target'] = correlation_dataframe.corr()['target']\n",
    "# new.to_csv('corr.csv')\n",
    "# # correlation_dataframe[\"OpenPorchSF\"].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation_dataframe = titanic_dataframe.copy()\n",
    "# correlation_dataframe[\"target\"] = training_targets[\"SalePrice\"]\n",
    "\n",
    "# # display.display(training_targets.describe())\n",
    "# # pd.options.display.max_rows = 20\n",
    "\n",
    "# new = correlation_dataframe.corr()\n",
    "# print(new['target'].sort_values(ascending=False).head(10))#.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    \"\"\"Trains a linear regression model.\n",
    "  \n",
    "    Args:\n",
    "      features: pandas DataFrame of features\n",
    "      targets: pandas DataFrame of targets\n",
    "      batch_size: Size of batches to be passed to the model\n",
    "      shuffle: True or False. Whether to shuffle the data.\n",
    "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
    "    Returns:\n",
    "      Tuple of (features, labels) for next data batch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert pandas data into a dict of np arrays.\n",
    "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
    "    \n",
    "    # Construct a dataset, and configure batching/repeating.\n",
    "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "    # Shuffle the data, if specified.\n",
    "    if shuffle:\n",
    "      ds = ds.shuffle(10000)\n",
    "    \n",
    "    # Return the next batch of data.\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_feature_columns(input_features):\n",
    "  \"\"\"Construct the TensorFlow Feature Columns.\n",
    "\n",
    "  Args:\n",
    "    input_features: The names of the numerical input features to use.\n",
    "  Returns:\n",
    "    A set of feature columns\n",
    "  \"\"\" \n",
    "  return set([tf.feature_column.numeric_column(my_feature)\n",
    "              for my_feature in input_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def create_training_input_fn(features, labels, batch_size, num_epochs=None, shuffle=True):\n",
    "#   \"\"\"A custom input_fn for sending MNIST data to the estimator for training.\n",
    "\n",
    "#   Args:\n",
    "#     features: The training features.\n",
    "#     labels: The training labels.\n",
    "#     batch_size: Batch size to use during training.\n",
    "\n",
    "#   Returns:\n",
    "#     A function that returns batches of training features and labels during\n",
    "#     training.\n",
    "#   \"\"\"\n",
    "#   def _input_fn(num_epochs=None, shuffle=True):\n",
    "#     # Input pipelines are reset with each call to .train(). To ensure model\n",
    "#     # gets a good sampling of data, even when number of steps is small, we \n",
    "#     # shuffle all the data before creating the Dataset object\n",
    "#     idx = np.random.permutation(features.index)\n",
    "#     raw_features = {\"pixels\":features.reindex(idx)}\n",
    "#     raw_targets = np.array(labels[idx])\n",
    "   \n",
    "#     ds = Dataset.from_tensor_slices((raw_features,raw_targets)) # warning: 2GB limit\n",
    "#     ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "#     if shuffle:\n",
    "#       ds = ds.shuffle(10000)\n",
    "    \n",
    "#     # Return the next batch of data.\n",
    "#     feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "#     return feature_batch, label_batch\n",
    "\n",
    "#   return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def create_predict_input_fn(features, labels, batch_size):\n",
    "#   \"\"\"A custom input_fn for sending mnist data to the estimator for predictions.\n",
    "\n",
    "#   Args:\n",
    "#     features: The features to base predictions on.\n",
    "#     labels: The labels of the prediction examples.\n",
    "\n",
    "#   Returns:\n",
    "#     A function that returns features and labels for predictions.\n",
    "#   \"\"\"\n",
    "#   def _input_fn():\n",
    "#     raw_features = {\"pixels\": features.values}\n",
    "#     raw_targets = np.array(labels)\n",
    "    \n",
    "#     ds = Dataset.from_tensor_slices((raw_features, raw_targets)) # warning: 2GB limit\n",
    "#     ds = ds.batch(batch_size)\n",
    "    \n",
    "        \n",
    "#     # Return the next batch of data.\n",
    "#     feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "#     return feature_batch, label_batch\n",
    "\n",
    "#   return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_scale(series):\n",
    "  min_val = series.min()\n",
    "  max_val = series.max()\n",
    "  scale = (max_val - min_val) / 2.0\n",
    "  return series.apply(lambda x:((x - min_val) / scale) - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_linear_classification_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "  \"\"\"Trains a linear classification model for the MNIST digits dataset.\n",
    "  \n",
    "  In addition to training, this function also prints training progress information,\n",
    "  a plot of the training and validation loss over time, and a confusion\n",
    "  matrix.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: An `int`, the learning rate to use.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    training_examples: A `DataFrame` containing the training features.\n",
    "    training_targets: A `DataFrame` containing the training labels.\n",
    "    validation_examples: A `DataFrame` containing the validation features.\n",
    "    validation_targets: A `DataFrame` containing the validation labels.\n",
    "      \n",
    "  Returns:\n",
    "    The trained `LinearClassifier` object.\n",
    "  \"\"\"\n",
    "\n",
    "  periods = 10\n",
    "\n",
    "  steps_per_period = steps / periods  \n",
    "  # Create the input functions.\n",
    "#   predict_training_input_fn = create_predict_input_fn(\n",
    "#     training_examples, training_targets, batch_size)\n",
    "#   predict_validation_input_fn = create_predict_input_fn(\n",
    "#     validation_examples, validation_targets, batch_size)\n",
    "#   training_input_fn = create_training_input_fn(\n",
    "#     training_examples, training_targets, batch_size)\n",
    "  \n",
    "  # Create a LinearClassifier object.\n",
    "  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "  classifier = tf.estimator.LinearClassifier(\n",
    "      feature_columns=construct_feature_columns(training_examples),\n",
    "      n_classes=10,\n",
    "      optimizer=my_optimizer,\n",
    "      config=tf.estimator.RunConfig(keep_checkpoint_max=1)\n",
    "  )\n",
    "    \n",
    "  # Create input functions.\n",
    "  training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                          training_targets[\"Survived\"], \n",
    "                                          batch_size=batch_size)\n",
    "  predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                                  training_targets[\"Survived\"], \n",
    "                                                  num_epochs=1, \n",
    "                                                  shuffle=False)\n",
    "  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                                    validation_targets[\"Survived\"], \n",
    "                                                    num_epochs=1, \n",
    "                                                    shuffle=False)\n",
    "\n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print(\"Training model...\")\n",
    "  print(\"LogLoss error (on validation data):\")\n",
    "  training_errors = []\n",
    "  validation_errors = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    classifier.train(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps_per_period\n",
    "    )\n",
    "  \n",
    "    # Take a break and compute probabilities.\n",
    "    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
    "    training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
    "    training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
    "    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
    "        \n",
    "    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
    "    validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
    "    validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
    "    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n",
    "    \n",
    "    # Compute training and validation errors.\n",
    "    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
    "    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
    "    # Occasionally print the current loss.\n",
    "    print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    training_errors.append(training_log_loss)\n",
    "    validation_errors.append(validation_log_loss)\n",
    "  print(\"Model training finished.\")\n",
    "  # Remove event files to save disk space.\n",
    "  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
    "  \n",
    "  # Calculate final predictions (not probabilities, as above).\n",
    "  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
    "  final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
    "  \n",
    "  \n",
    "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
    "  print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
    "\n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.ylabel(\"LogLoss\")\n",
    "  plt.xlabel(\"Periods\")\n",
    "  plt.title(\"LogLoss vs. Periods\")\n",
    "  plt.plot(training_errors, label=\"training\")\n",
    "  plt.plot(validation_errors, label=\"validation\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  \n",
    "  # Output a plot of the confusion matrix.\n",
    "  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
    "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "  # in each class).\n",
    "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
    "  ax.set_aspect(1)\n",
    "  plt.title(\"Confusion matrix\")\n",
    "  plt.ylabel(\"True label\")\n",
    "  plt.xlabel(\"Predicted label\")\n",
    "  plt.show()\n",
    "\n",
    "  return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(learning_rate, steps, batch_size, input_feature):\n",
    "  \"\"\"Trains a linear regression model.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: A `float`, the learning rate.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    input_feature: A `string` specifying a column from `titanic_dataframe`\n",
    "      to use as input feature.\n",
    "      \n",
    "  Returns:\n",
    "    A Pandas `DataFrame` containing targets and the corresponding predictions done\n",
    "    after training the model.\n",
    "  \"\"\"\n",
    "  \n",
    "  periods = 10\n",
    "  steps_per_period = steps // periods\n",
    "\n",
    "  my_feature = input_feature\n",
    "  my_feature_data = titanic_dataframe[[my_feature]].astype('float32')\n",
    "  my_label = \"Survived\"\n",
    "  targets = titanic_dataframe[my_label].astype('float32')\n",
    "\n",
    "  # Create input functions.\n",
    "  training_input_fn = lambda: my_input_fn(my_feature_data, targets, batch_size=batch_size)\n",
    "  predict_training_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=False)\n",
    "  \n",
    "  # Create feature columns.\n",
    "  feature_columns = [tf.feature_column.numeric_column(my_feature)]\n",
    "    \n",
    "  # Create a linear regressor object.\n",
    "  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "  linear_regressor = tf.estimator.LinearClassifier(\n",
    "      feature_columns=feature_columns,\n",
    "      optimizer=my_optimizer\n",
    "  )\n",
    "\n",
    "  # Set up to plot the state of our model's line each period.\n",
    "  plt.figure(figsize=(15, 6))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.title(\"Learned Line by Period\")\n",
    "  plt.ylabel(my_label)\n",
    "  plt.xlabel(my_feature)\n",
    "  sample = titanic_dataframe.sample(n=300)\n",
    "  plt.scatter(sample[my_feature], sample[my_label])\n",
    "  colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]\n",
    "\n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print(\"Training model...\")\n",
    "  print(\"RMSE (on training data):\")\n",
    "  root_mean_squared_errors = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    linear_regressor.train(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps_per_period,\n",
    "    )\n",
    "    # Take a break and compute predictions.\n",
    "    predictions = linear_regressor.predict(input_fn=predict_training_input_fn)\n",
    "    predictions = np.array([item['predictions'][0] for item in predictions])\n",
    "    \n",
    "    # Compute loss.\n",
    "    root_mean_squared_error = math.sqrt(\n",
    "      metrics.mean_squared_error(predictions, targets))\n",
    "    # Occasionally print the current loss.\n",
    "    print(\"  period %02d : %0.2f\" % (period, root_mean_squared_error))\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    root_mean_squared_errors.append(root_mean_squared_error)\n",
    "    # Finally, track the weights and biases over time.\n",
    "    # Apply some math to ensure that the data and line are plotted neatly.\n",
    "    y_extents = np.array([0, sample[my_label].max()])\n",
    "    \n",
    "    weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]\n",
    "    bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n",
    "    \n",
    "    x_extents = (y_extents - bias) // weight\n",
    "    x_extents = np.maximum(np.minimum(x_extents,\n",
    "                                      sample[my_feature].max()),\n",
    "                           sample[my_feature].min())\n",
    "    y_extents = weight * x_extents + bias\n",
    "    plt.plot(x_extents, y_extents, color=colors[period]) \n",
    "  print(\"Model training finished.\")\n",
    "\n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.ylabel('RMSE')\n",
    "  plt.xlabel('Periods')\n",
    "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "  plt.tight_layout()\n",
    "  plt.plot(root_mean_squared_errors)\n",
    "\n",
    "  # Create a table with calibration data.\n",
    "  calibration_data = pd.DataFrame()\n",
    "  calibration_data[\"predictions\"] = pd.Series(predictions)\n",
    "  calibration_data[\"targets\"] = pd.Series(targets)\n",
    "  display.display(calibration_data.describe())\n",
    "\n",
    "  print(\"Final RMSE (on training data): %0.2f\" % root_mean_squared_error)\n",
    "  \n",
    "  return calibration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nn_regression_model(\n",
    "    my_optimizer,\n",
    "    steps,\n",
    "    batch_size,\n",
    "#     hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "  \"\"\"Trains a neural network regression model.\n",
    "  \n",
    "  In addition to training, this function also prints training progress information,\n",
    "  as well as a plot of the training and validation loss over time.\n",
    "  \n",
    "  Args:\n",
    "    my_optimizer: An instance of `tf.train.Optimizer`, the optimizer to use.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n",
    "    training_examples: A `DataFrame` containing one or more columns from\n",
    "      `titanic_dataframe` to use as input features for training.\n",
    "    training_targets: A `DataFrame` containing exactly one column from\n",
    "      `titanic_dataframe` to use as target for training.\n",
    "    validation_examples: A `DataFrame` containing one or more columns from\n",
    "      `titanic_dataframe` to use as input features for validation.\n",
    "    validation_targets: A `DataFrame` containing exactly one column from\n",
    "      `titanic_dataframe` to use as target for validation.\n",
    "      \n",
    "  Returns:\n",
    "    A tuple `(estimator, training_losses, validation_losses)`:\n",
    "      estimator: the trained `DNNRegressor` object.\n",
    "      training_losses: a `list` containing the training loss values taken during training.\n",
    "      validation_losses: a `list` containing the validation loss values taken during training.\n",
    "  \"\"\"\n",
    "\n",
    "  periods = 10\n",
    "  steps_per_period = steps / periods\n",
    "  \n",
    "  # Create a DNNRegressor object.\n",
    "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "  dnn_regressor = tf.estimator.DNNRegressor(\n",
    "      feature_columns=construct_feature_columns(training_examples),\n",
    "      hidden_units=hidden_units,\n",
    "      optimizer=my_optimizer\n",
    "  )\n",
    "  \n",
    "  # Create input functions.\n",
    "  training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                          training_targets[\"SalePrice\"], \n",
    "                                          batch_size=batch_size)\n",
    "  predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                                  training_targets[\"SalePrice\"], \n",
    "                                                  num_epochs=1, \n",
    "                                                  shuffle=False)\n",
    "  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                                    validation_targets[\"SalePrice\"], \n",
    "                                                    num_epochs=1, \n",
    "                                                    shuffle=False)\n",
    "\n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print(\"Training model...\")\n",
    "  print(\"RMSE (on training data):\")\n",
    "  training_rmse = []\n",
    "  validation_rmse = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    dnn_regressor.train(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps_per_period\n",
    "    )\n",
    "    # Take a break and compute predictions.\n",
    "    training_predictions = dnn_regressor.predict(input_fn=predict_training_input_fn)\n",
    "    training_predictions = np.array([item['predictions'][0] for item in training_predictions])\n",
    "    \n",
    "    validation_predictions = dnn_regressor.predict(input_fn=predict_validation_input_fn)\n",
    "    validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n",
    "    \n",
    "    # Compute training and validation loss.\n",
    "    training_root_mean_squared_error = math.sqrt(\n",
    "        metrics.mean_squared_error(training_predictions, training_targets))\n",
    "    validation_root_mean_squared_error = math.sqrt(\n",
    "        metrics.mean_squared_error(validation_predictions, validation_targets))\n",
    "    # Occasionally print the current loss.\n",
    "    print(\"  period %02d : %0.2f\" % (period, training_root_mean_squared_error))\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    training_rmse.append(training_root_mean_squared_error)\n",
    "    validation_rmse.append(validation_root_mean_squared_error)\n",
    "  print(\"Model training finished.\")\n",
    "\n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.ylabel(\"RMSE\")\n",
    "  plt.xlabel(\"Periods\")\n",
    "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "  plt.tight_layout()\n",
    "  plt.plot(training_rmse, label=\"training\")\n",
    "  plt.plot(validation_rmse, label=\"validation\")\n",
    "  plt.legend()\n",
    "\n",
    "  print(\"Final RMSE (on training data):   %0.2f\" % training_root_mean_squared_error)\n",
    "  print(\"Final RMSE (on validation data): %0.2f\" % validation_root_mean_squared_error)\n",
    "\n",
    "  return dnn_regressor, training_rmse, validation_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass                                      Name  \\\n",
      "150          151         0       2                Bateman, Rev. Robert James   \n",
      "148          149         0       2  Navratil, Mr. Michel (\"Louis M Hoffman\")   \n",
      "412          413         1       1                    Minahan, Miss. Daisy E   \n",
      "707          708         1       1         Calderhead, Mr. Edward Pennington   \n",
      "744          745         1       3                        Stranden, Mr. Juho   \n",
      "279          280         1       3          Abbott, Mrs. Stanton (Rosa Hunt)   \n",
      "312          313         0       2     Lahtinen, Mrs. William (Anna Sylfven)   \n",
      "\n",
      "        Sex  Age  SibSp  Parch             Ticket  Fare   ...     5   6  \\\n",
      "150    male 51.0      0      0        S.O.P. 1166  12.5   ...   0.0 0.0   \n",
      "148    male 36.5      0      2             230080  26.0   ...   0.0 0.0   \n",
      "412  female 33.0      1      0              19928  90.0   ...   0.0 0.0   \n",
      "707    male 42.0      0      0           PC 17476  26.3   ...   0.0 0.0   \n",
      "744    male 31.0      0      0  STON/O 2. 3101288   7.9   ...   0.0 0.0   \n",
      "279  female 35.0      1      1          C.A. 2673  20.2   ...   0.0 0.0   \n",
      "312  female 26.0      1      1             250651  26.0   ...   0.0 0.0   \n",
      "\n",
      "     female  male   C   Q   S  Class1  Class2  Class3  \n",
      "150     0.0   1.0 0.0 0.0 1.0     0.0     1.0     0.0  \n",
      "148     0.0   1.0 0.0 0.0 1.0     0.0     1.0     0.0  \n",
      "412     1.0   0.0 0.0 1.0 0.0     1.0     0.0     0.0  \n",
      "707     0.0   1.0 0.0 0.0 1.0     1.0     0.0     0.0  \n",
      "744     0.0   1.0 0.0 0.0 1.0     0.0     0.0     1.0  \n",
      "279     1.0   0.0 0.0 0.0 1.0     0.0     0.0     1.0  \n",
      "312     1.0   0.0 0.0 0.0 1.0     0.0     1.0     0.0  \n",
      "\n",
      "[7 rows x 33 columns]\n",
      "\n",
      "Chosen Features NULLs?\n",
      "\n",
      " Survived    0\n",
      "target      0\n",
      "female      0\n",
      "Class1      0\n",
      "Fare        0\n",
      "C           0\n",
      "SS1         0\n",
      "PC1         0\n",
      "PC2         0\n",
      "Parch       0\n",
      "dtype: int64\n",
      "YESSS\n",
      "      Survived  target  female  Class1  Fare    C  SS1  PC1  PC2  Parch\n",
      "150      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "148      -1.0    -1.0    -1.0    -1.0  -0.9 -1.0 -1.0 -1.0  1.0   -0.3\n",
      "412       1.0     1.0     1.0     1.0  -0.6 -1.0  1.0 -1.0 -1.0   -1.0\n",
      "707       1.0     1.0    -1.0     1.0  -0.9 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "744       1.0     1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "279       1.0     1.0     1.0    -1.0  -0.9 -1.0  1.0  1.0 -1.0   -0.7\n",
      "312      -1.0    -1.0     1.0    -1.0  -0.9 -1.0  1.0  1.0 -1.0   -0.7\n",
      "748      -1.0    -1.0    -1.0     1.0  -0.8 -1.0  1.0 -1.0 -1.0   -1.0\n",
      "0        -1.0    -1.0    -1.0    -1.0  -1.0 -1.0  1.0 -1.0 -1.0   -1.0\n",
      "114      -1.0    -1.0     1.0    -1.0  -0.9  1.0 -1.0 -1.0 -1.0   -1.0\n",
      "200      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "131      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "500      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "81        1.0     1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "854      -1.0    -1.0     1.0    -1.0  -0.9 -1.0  1.0 -1.0 -1.0   -1.0\n",
      "722      -1.0    -1.0    -1.0    -1.0  -0.9 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "172       1.0     1.0     1.0    -1.0  -1.0 -1.0  1.0  1.0 -1.0   -0.7\n",
      "178      -1.0    -1.0    -1.0    -1.0  -0.9 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "761      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "414       1.0     1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "499      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "874       1.0     1.0     1.0    -1.0  -0.9  1.0  1.0 -1.0 -1.0   -1.0\n",
      "741      -1.0    -1.0    -1.0     1.0  -0.7 -1.0  1.0 -1.0 -1.0   -1.0\n",
      "86       -1.0    -1.0    -1.0    -1.0  -0.9 -1.0  1.0 -1.0 -1.0    0.0\n",
      "770      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "307       1.0     1.0     1.0     1.0  -0.6  1.0  1.0 -1.0 -1.0   -1.0\n",
      "227      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "237       1.0     1.0     1.0    -1.0  -0.9 -1.0 -1.0 -1.0  1.0   -0.3\n",
      "782      -1.0    -1.0    -1.0     1.0  -0.9 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "737       1.0     1.0    -1.0     1.0   1.0  1.0 -1.0 -1.0 -1.0   -1.0\n",
      "..        ...     ...     ...     ...   ...  ...  ...  ...  ...    ...\n",
      "374      -1.0    -1.0     1.0    -1.0  -0.9 -1.0 -1.0  1.0 -1.0   -0.7\n",
      "706       1.0     1.0     1.0    -1.0  -0.9 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "565      -1.0    -1.0    -1.0    -1.0  -0.9 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "608       1.0     1.0     1.0    -1.0  -0.8  1.0  1.0 -1.0  1.0   -0.3\n",
      "450      -1.0    -1.0    -1.0    -1.0  -0.9 -1.0  1.0 -1.0  1.0   -0.3\n",
      "688      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "556       1.0     1.0     1.0     1.0  -0.8  1.0  1.0 -1.0 -1.0   -1.0\n",
      "252      -1.0    -1.0    -1.0     1.0  -0.9 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "850      -1.0    -1.0    -1.0    -1.0  -0.9 -1.0 -1.0 -1.0  1.0   -0.3\n",
      "136       1.0     1.0     1.0     1.0  -0.9 -1.0 -1.0 -1.0  1.0   -0.3\n",
      "465      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "619      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "218       1.0     1.0     1.0     1.0  -0.7  1.0 -1.0 -1.0 -1.0   -1.0\n",
      "662      -1.0    -1.0    -1.0     1.0  -0.9 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "576       1.0     1.0     1.0    -1.0  -0.9 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "243      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "363      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "870      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "88        1.0     1.0     1.0     1.0   0.0 -1.0 -1.0 -1.0  1.0   -0.3\n",
      "473       1.0     1.0     1.0    -1.0  -0.9  1.0 -1.0 -1.0 -1.0   -1.0\n",
      "234      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "668      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "781       1.0     1.0     1.0     1.0  -0.8 -1.0  1.0 -1.0 -1.0   -1.0\n",
      "1         1.0     1.0     1.0     1.0  -0.7  1.0  1.0 -1.0 -1.0   -1.0\n",
      "116      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "378      -1.0    -1.0    -1.0    -1.0  -1.0  1.0 -1.0 -1.0 -1.0   -1.0\n",
      "660       1.0     1.0    -1.0     1.0  -0.5 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "506       1.0     1.0     1.0    -1.0  -0.9 -1.0 -1.0 -1.0  1.0   -0.3\n",
      "807      -1.0    -1.0     1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "129      -1.0    -1.0    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0   -1.0\n",
      "\n",
      "[712 rows x 10 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAG41JREFUeJzt3H+QVeWd5/H3txuuXiEQGRlmwq+OIjMSIUKUsMHsXnVs\nGxaVZCl/xKRE4xitZaSy1PpjatUmNVMbSSW7sVRSTDCmRn7IYkBhRHuScGvTZLCxFIjTTdofaUd0\nB9o1i6tJNoZ89497+/a5p8+53Z17G5qnP6+qLs85z3Oe5znPOffD4aGv5u6IiEiY6k72AEREZOgo\n5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAlaTkDez9WZ2xMwOppR/wcwOFH9azWx2LfoVEZHKavUm\n/z3gigrlrwP/1t0/CfwN8Hc16ldERCoYVYtG3L3VzKZXKN8b2d0LTK5FvyIiUtnJWJO/Bdh1EvoV\nERlxavImP1BmdglwE3DxiexXRGSkOmEhb2ZzgHVAk7v/skI9/c90REQGyd0t6Xgtl2us+NO3wGwa\n8CTwJXd/rb+G3P2k/tx///0nfQzD5UdzobnQXAz/uaikJm/yZrYRyAF/ZGb/AtwPZAp57euAe4EJ\nwCNmZsCH7j6/Fn2LiEi6Wv12zRf6Kf9L4C9r0ZeIiAycvvGaIJfLnewhDBuai16ai16ai17DfS6s\nv/WcE83MfLiNSURkODMz/AT8w6uIiAwzCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQ\nFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp\n5EVEAqaQFxEJmEJeRCRgNQl5M1tvZkfM7GCFOg+a2Stmtt/MLqhFvyIiUll9c3Nz1Y2sXr36XeBR\n4HPNzc1r4+VmtghocvcFq1evfgl4uLm5+bspbTXXYkzV6O7uZs+ePaxcuZKVK1fyi1/8giVLlrBn\nzx7Wr19PJpPhJz/5Cffeey+///3vefbZZ1mxYgXvvfceCxcuZOfOnaxZs4a6ujqOHz/OM888w2mn\nnQZAe3s7mUyGMWPG0NHRUSp75513StsTJ07sM56e8371q1+lttHZ2Vka38GDB0tjmDlzJmvXrmXV\nqlX85je/4YUXXihtb968mVtvvZXDhw9zxRVXsGrVqtL+2LFjE693zpw5LF++nC9/+ct0dHTwwQcf\nlMq++tWvcuutt5LP57nxxhtT+3344YdL5y9dupQ1a9aU5vC5557jtttuo7u7my1btnDLLbfw2muv\nsWTJEu67775S2c6dO7n11lvp7Oxk0qRJbNiwgXvuuaffvqJjX7hwYWk+t2/fXnaNGzZsKO3v2bOn\n1N7YsWPL7lX0GhsaGkrt7d+/vzR/06ZNK2tvzpw5ifd3zJgxZe0dOXKkdB/37dtXdn7avc9ms2Xt\nRZ/beFm0jehzFz1n2rRpZc9j2jnxskrPdCji9+5kWb16Nc3NzasTC929Jj/AdOBgStl3gGsj+x3A\npJS6fjJt3LjZR4/+iMNoh6zDucX/WmQ/XjYqth0tyzjMdMh6ff0ZPn78PM9mJ/jlly8qls90OC2y\nnfUVK+4oG082O8HHj5/nmcx4Hz16bEob0XGUj88sun9awviSrjHrUJ9yTt0A5yLeXmYA/cb7Smsv\nXu/0P6Cvwv748fMSriO+H5+LwpyPHj2mT73x4+e5Wfl44vWmTm3oc3+z2Ql+xhkf7TO+wnb5c3XG\nGeMSnp+e+50ttXf++XP7XFfa89Pz3PUeL5zT2Lio9DyuWLEy8ZyksuizH60Xivi927hx80kbSzE3\nk7M5rWCwP/2E/A7gM5H9HwLzUuoO6WRUcvToUT/99I869HwgDzh48b9Zh8cc2lPK2h12J5Sd6XA0\nth2td7R4vLy99vZ2P3r0qGezE1La25Yyjh2x49v66StpTPH2oudE61Wai57t3VXPRd+5jdcb6Lwn\njSnrcG/KdcTrVZqLaL3HY8eT6z3yyCOx+/u3Ke2lje/Bfu7pYynntaaO6fHH42MvHG9tbfX29uT7\n3d7enlJWPu/t7e0n7bNda0mfzWx2gh89evSkjKdSyI8aor89VCW6XJPL5cjlciek366uLurrJwH/\nFxgL9PyVeg4wGXi9uD81VjYFaANmFbejZQ1AF3BRZHtMpN4+4ON92mtra2PWrFlkMg38+tdJ7R1L\n6GsKsD02vmi9pL6SxhS95u2xc6L1vl9hLm4snj8mUvaHzUXfuY3XSxp7Ul8XJYxpMvBEyrXH61Wa\ni2i9LbF5Sa63adOm2P19PqW9tPEdABYkzFnP9b6ecl4LsCShbApbtmxJPKelpYWzzz6bpPvd1tZW\n3I+X9YzjolK98847jxB0dXX1+WyOHj2drq6uE7I0lc/nyefzA6uclv6D/WFwyzWHGIbLNXqT15u8\n3uT1Jj8Qp9KbfC1DvgH4WUrZYuAfitsLgL0V2hnSyehPYU1+rPeugc7w8rXcGQlloyLb9bGy3vXg\n+vozfNy4uZ7NTvDGxui6ZyaynbwmP27c3NKafHIb0XGUj88sup9JGF/SNfZcS9I5A52LeHuZAfQb\n7yutvXi90/6Avgr748bN7WfOkuaiMOeZTLZPvXHj5rpZ+Xji9eJr8j33dMyYcX3GV9guf65660Wf\nn56xZ0vtzZ59Qay90anPT89z13u8cE75mvwdiecklUWf/ZDX5Hvmc7iuyVuhvDpmthHIAX8EHAHu\nBzLFjtcV6zwENAEfADe5+4spbXktxlSN7u5uXnrpJR588EGef/55li1bxtq1a9mzZw8tLS00NjbS\n1dXFli1buOaaa3jrrbfYtGkT119/PXfeeSc7d+5k+/btLF26lHPOOYe2tjbmz5/PWWedRVdXFw0N\nDUycOJGOjo5SGVDajv+Vtru7u3QekNrGu+++WxrfL3/5y9IYlixZwtq1a0tjBErbr776Klu3bmXZ\nsmV885vfZNWqVaX9z3/+84nXe8MNN7B8+XJ27NjBlVdeyeWXX14qe/TRR2ltbeXiiy/mRz/6UWq/\nzz//fOn8xx57jDVr1pTK3n//fZ544gmuvfZauru72bZtG5/73OdYu3Yt9913X6nsgw8+YOvWrSxe\nvJibb76Z3bt3s3Pnzn77io79G9/4Rmk+W1payq5xw4YNpf333nuv1F4ulyu7V9FrXLZsWam9zs7O\n0vwtXLiwrL0bbrgh8f72/LZOT3tTp04t3cdjx46VnZ9272fOnFnWXvS5jZdF24g+d9FzFi5cWPY8\npp0TL6v0TIcifu9OFjPD3S2x7GQHatxwCHkRkVNJpZDXN15FRAKmkBcRCZhCXkQkYAp5EZGAKeRF\nRAKmkBcRCZhCXkQkYAp5EZGAKeRFRAKmkBcRCZhCXkQkYAp5EZGAKeRFRAKmkBcRCZhCXkQkYAp5\nEZGAKeRFRAKmkBcRCZhCXkQkYAp5EZGAKeRFRAKmkBcRCZhCXkQkYDUJeTNrMrNDZtZpZncllI8z\ns6fNbL+Z/czMlteiXxERqczcvboGzOqATuAy4G1gH3Cdux+K1LkHGOfu95jZWcDPgUnu/ruE9rza\nMYmIjCRmhrtbUlkt3uTnA6+4+xvu/iGwGbg6VseBjxS3PwL876SAFxGR2qpFyE8G3ozsHy4ei3oI\nmGVmbwMHgJU16FdERPox6gT1cwXwkrtfambnAP9oZnPc/f2kys3NzaXtXC5HLpc7IYMUETkV5PN5\n8vn8gOrWYk1+AdDs7k3F/bsBd/cHInV2Av/V3fcU938E3OXuLyS0pzV5EZFBGOo1+X3ADDObbmYZ\n4Drg6VidN4C/KA5mEjATeL0GfYuISAVVL9e4+3EzWwG0UPhDY727d5jZVwrFvg74G+AxMztYPO1O\nd3+32r5FRKSyqpdrak3LNSIigzPUyzUiIjJMKeRFRAKmkBcRCZhCXkQkYAp5EZGAKeRFRAKmkBcR\nCZhCXkQkYAp5EZGAKeRFRAKmkBcRCZhCXkQkYAp5EZGAKeRFRAKmkBcRCZhCXkQkYAp5EZGAKeRF\nRAKmkBcRCZhCXkQkYAp5EZGAKeRFRAKmkBcRCVhNQt7MmszskJl1mtldKXVyZvaSmb1sZrtr0a+I\niFRm7l5dA2Z1QCdwGfA2sA+4zt0PReqMB34KNLr7W2Z2lru/k9KeVzsmEZGRxMxwd0sqq8Wb/Hzg\nFXd/w90/BDYDV8fqfAF40t3fAkgLeBERqa1ahPxk4M3I/uHisaiZwAQz221m+8zsSzXoV0RE+jHq\nBPYzD7gUGAP8k5n9k7u/mlS5ubm5tJ3L5cjlcidgiCIip4Z8Pk8+nx9Q3VqsyS8Amt29qbh/N+Du\n/kCkzl3A6e6+urj/XWCXuz+Z0J7W5EVEBmGo1+T3ATPMbLqZZYDrgKdjdZ4CLjazejM7A/g00FGD\nvkVEpIKql2vc/biZrQBaKPyhsd7dO8zsK4ViX+fuh8zsOeAgcBxY5+7t1fYtIiKVVb1cU2tarhER\nGZyhXq4REZFhSiEvIhIwhbyISMAU8iIiAVPIi4gETCEvIhIwhbyISMAU8iIiAVPIi4gETCEvIhIw\nhbyISMAU8iIiAVPIi4gETCEvIhIwhbyISMAU8iIiAVPIi4gETCEvIhIwhbyISMAU8iIiAVPIi4gE\nTCEvIhIwhbyISMBqEvJm1mRmh8ys08zuqlDvIjP70Mw+X4t+RUSksqpD3szqgIeAK4BPANeb2Z+n\n1Ps68Fy1fYqIyMDU4k1+PvCKu7/h7h8Cm4GrE+r9FbAVOFqDPkVEZABqEfKTgTcj+4eLx0rM7GPA\nUndfC1gN+hQRkQEYdYL6+e9AdK2+YtA3NzeXtnO5HLlcbkgGJSJyKsrn8+Tz+QHVNXevqjMzWwA0\nu3tTcf9uwN39gUid13s2gbOAD4Bb3f3phPa82jGJiIwkZoa7J7481yLk64GfA5cB/wtoA653946U\n+t8Ddrj7D1LKFfIiIoNQKeSrXq5x9+NmtgJoobDGv97dO8zsK4ViXxc/pdo+RURkYKp+k681vcmL\niAxOpTd5feNVRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkR\nkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJe\nRCRgCnkRkYAp5EVEAlaTkDezJjM7ZGadZnZXQvkXzOxA8afVzGbXol8REanM3L26BszqgE7gMuBt\nYB9wnbsfitRZAHS4+zEzawKa3X1BSnte7ZhEREYSM8PdLamsFm/y84FX3P0Nd/8Q2AxcHa3g7nvd\n/Vhxdy8wuQb9iohIP2oR8pOBNyP7h6kc4rcAu2rQr4iI9GPUiezMzC4BbgIurlSvubm5tJ3L5cjl\nckM6LhGRU0k+nyefzw+obi3W5BdQWGNvKu7fDbi7PxCrNwd4Emhy99cqtKc1eRGRQRjqNfl9wAwz\nm25mGeA64OnYAKZRCPgvVQp4ERGpraqXa9z9uJmtAFoo/KGx3t07zOwrhWJfB9wLTAAeMTMDPnT3\n+dX2LSIilVW9XFNrWq4RERmcoV6uERGRYUohLyISMIW8iEjAFPIiIgFTyIuIBEwhLyISMIW8iEjA\nFPIiIgFTyIuIBEwhLyISMIW8iEjAFPIiIgFTyIuIBEwhLyISMIW8iEjAFPIiIgFTyIuIBEwhLyIS\nMIW8iEjAFPIiIgFTyIuIBEwhLyISMIW8iEjAahLyZtZkZofMrNPM7kqp86CZvWJm+83sglr0KyIi\nlY2qtgEzqwMeAi4D3gb2mdlT7n4oUmcRcI67n2tmnwa+Ayyotu+h1tHRQVtbG/Pnz+e8885jzZo1\nbNq0ieuvv56bbrqJrq4uGhoa6OzspKWlhcbGRhYuXFjWRnd3d6neO++8U9ZetP18Pl9q+/bbby8r\ne/fdd1Pbj9qzZw8tLS1cdNFFZDIZAObOncvEiRNZvnw5O3bs4Morr2TWrFmlvn7wgx/wwgsvcOGF\nF7J3714++9nPsnfvXhYsWMDkyZPZtWsXl1xyCS+//DKvvfYas2fP5uDBg2X1zj//fLZt20ZTUxMv\nv/wyBw4cYMGCBXzrW9/itttu48CBA1x44YU0NDSwa9cuFi1axLx580pjuPPOO/nUpz7FSy+9xNSp\nU5k1axY//elPWbRoEd3d3bS2tjJ37lxuv/12du/eze7du1m2bBkAW7duZfHixdx88808+uijPPPM\nM33KRo8eTT6f54tf/CJ33nkny5cv56mnnuLiiy9m8eLFPPvss1xzzTW89957Zfdg1apVbN26lWXL\nljFp0iQ2bdrEpZdeyvHjxzl8+DDLly9nyZIlZc/FrFmz2L59O0uXLuXNN98sa2/NmjU89thjzJ49\nm6uuuorOzk4aGxt57rnneOKJJ7j22mv52te+VrqPjY2NTJgwofQcvPjii2zZsoVrrrmGG264gQ0b\nNpT2GxoaUp+RaL3Gxka6uroYO3Ys77//Pr/97W959dVXmT9/PkCpr+h2/FmNlsWfzfhnJu2zFL3G\nmTNnlj4jEydOHNRnc8aMGWQymbLriPYbF/08DrSvU567V/VDIax3RfbvBu6K1fkOcG1kvwOYlNKe\nDwcrVqx0yDrMdMh6Xd1pxf1zHQrb48fPc7Ns5HjWGxsXldrYuHGzZ7MTfPz4eV5fP6asvfPPvyCy\nX1/WxujR2UjZqNT2oy6/fFFZPRjtMMMzmfF92oBMcXt07Lj1uca+58TrZYvt99devKy+QnvmcGaf\neekd04yEMUXr9r1X6X3VDWDs0Wvsae+cyPG0axzlyePrGfuMhPHF641KfA7M4n31zkX0GZky5eN9\n5jCbPdsh6/X1f1Y89qeRccyMbWd99uy5ZftpY5oypaGs3ooVdyR+lnrr9VxL4bOUzU7wjRs3D+Kz\n+TEvfDZnRq6jt9+46OdxoH2dKoq5mZzRaQUD/QH+A7Ausv9F4MFYnR3AZyL7PwTmpbQ3xNPRv/b2\n9uJDc8DBi//NOux2OOqFADoQ2+6t19ra6kePHvVsdkLFeoX2dqT09bcOrYllra2tZeNtbU2uB+0O\nDyaUnZnaNmxLGe+ZxeO7U85Lu45txXEklbWmjC/rQOx4Ur89Yxrc/SnUi+4/UuF+x8cbbW9b5Ly0\na2yvMPb2lPFti+0/Pui5aG1t9ccfj593wOGjxZ/osQkO41PmLO1+D2RMWd+xI+25iM5t79iz2Ql+\n9OjRAXw2dxfHHb+Owjja29vLziv/PPqA+jqVVAr5qpdrhkJzc3NpO5fLkcvlTmj/bW1twFRgTvHI\nHGAyMAboAj5ePLYvst1br6WlhUwmQybTwK9/nV6v0N7jwJSEsueB/5dY1tLSUvZX8paWloR6U4A2\n4EBCWQPQknKNx2LXGD2nqzjmpL62V2gvbT5bgLdSrv/V2PGkfnvGdBGDuT+FetH9TSnji9abUhxv\ntL1jkTF9P6GNnnswK2XsbSnjOxZrY8ug56KlpYX9+/cn1JtG4bmKj7PnWHzO0u73QMY0he3b056L\n6Nz2jn306Ol0dXWlLqX0fjbHFM+Ltju9NI62trayZZuurq7I57FQv7++hrN8Pk8+nx9Y5bT0H+gP\nheWaZyP7A1muOcQwXq7Rm7ze5PUmrzf5UwlDvFxTT+G1azqQAfYD58XqLAb+wXv/UNhbob0hn5CB\nWLHiDo+uG9bXZ4r7M7yw/pn1cePmRtbkC+vESWvy48bN9fr6M8ramz37gsh+XVkbmUw2Ulaf2n5U\nY+OisnqF9eFzimvydbGynvXgUbHj5vFr7HtOvF7PGm1/7cXL6iu0Z14IovjxpDX5aHtJY49fR7zN\nugGMPXqNPe2dk9BvvI1RFdrLePL46hPa6PscmMX76p2L6DMydWqDx+fw9NMLx+rro2vZPeM4N7Yd\nf1bTx9TbV6Fe75p8+WcpPiazwmdp4Gvyd0TGnfW6unPL9vtbkx9MX6eKSiFvhfLqmFkT8G0Kv5K5\n3t2/bmZfKXa8rljnIaAJ+AC4yd1fTGnLazGmWtBv1+i3a/TbNfrtmlOBmeHullg2XAK1x3AKeRGR\nU0GlkNc3XkVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJ\nmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVE\nAqaQFxEJWFUhb2ZnmlmLmf3czJ4zs/EJdaaY2Y/N7J/N7Gdmdkc1fYqIyMBV+yZ/N/BDd/8z4MfA\nPQl1fgf8J3f/BPBvgP9oZn9eZb9DKp/Pn+whDBuai16ai16ai17DfS6qDfmrge8Xt78PLI1XcPd/\ndff9xe33gQ5gcpX9DqnhftNOJM1FL81FL81Fr+E+F9WG/B+7+xEohDnwx5Uqm1kDcAHwfJX9iojI\nAIzqr4KZ/SMwKXoIcOC/JFT3Cu2MBbYCK4tv9CIiMsTMPTWX+z/ZrAPIufsRM/sTYLe7n5dQbxSw\nE9jl7t/up80/fEAiIiOUu1vS8X7f5PvxNLAceAC4EXgqpd6jQHt/AQ/pAxURkcGr9k1+ArAFmAq8\nAVzj7v/HzP4U+Dt3X2JmC4H/CfyMwnKOA3/t7s9WPXoREamoqpAXEZHhTd94jTCzJjM7ZGadZnbX\nyR7PiZT2pbWBfOEtVGZWZ2YvmtnTxf0RORdmNt7M/oeZdRSfj0+P4Ln4qpm9bGYHzWyDmWWG+1wo\n5IvMrA54CLgC+ARw/XD/0laNpX1pbSBfeAvVSqA9sj9S5+LbwDPFX6r4JHCIETgXZvYx4K+Aee4+\nh8K/aV7PMJ8LhXyv+cAr7v6Gu38IbKbwZa8RIeVLa1MYwBfeQmRmU4DFwHcjh0fcXJjZOOCz7v49\nAHf/nbsfYwTORVE9MKb4G4NZ4C2G+Vwo5HtNBt6M7B9mmH8zd6hEvrS2F5g0mC+8BeS/Af+Z8u9+\njMS5+Djwjpl9r7h0tc7MzmAEzoW7vw18E/gXCuF+zN1/yDCfC4W8lEn40lr8X+aD/5d6M/v3wJHi\n32wq/Upv8HNBYUliHvCwu88DPqCwPDESn4uPUnhrnw58jMIb/Q0M87lQyPd6C5gW2Z9SPDZiFP8K\nuhX4e3fv+c7DETObVCz/E+DoyRrfCbQQuMrMXgc2AZea2d8D/zoC5+Iw8Ka7v1Dcf5JC6I/E5+Iv\ngNfd/V13Pw5sAz7DMJ8LhXyvfcAMM5tuZhngOgpf9hpJkr601vOFN6j8hbdguPtfu/s0dz+bwnPw\nY3f/ErCDkTcXR4A3zWxm8dBlwD8zAp8LCss0C8zsdDMzCnPRzjCfC/2efISZNVH4TYI6YL27f/0k\nD+mESfvSGtBGwhfeTtY4TzQz+3fAKne/Ku3Lfyd1gCeAmX2Swj9AjwZeB26i8A+QI3Eu7qfwB/+H\nwEvALcBHGMZzoZAXEQmYlmtERAKmkBcRCZhCXkQkYAp5EZGAKeRFRAKmkBcRCZhCXkQkYAp5EZGA\n/X9hD7Ifh0LSbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69559aff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_linear_scale(examples_dataframe):#, features1):\n",
    "  \"\"\"Returns a version of the input `DataFrame` that has all its features normalized linearly.\"\"\"\n",
    "  #\n",
    "  # Your code here: normalize the inputs.\n",
    "  #\n",
    "\n",
    "#   clipped_feature = examples_dataframe[\"TotRmsAbvGrd\"].apply(lambda x: min(x, 13))\n",
    "#   examples_dataframe[\"TotRmsAbvGrd\"] = clipped_feature\n",
    "#   # titanic_dataframe = titanic_dataframe[titanic_dataframe[\"TotRmsAbvGrd\"] <= 13]\n",
    "\n",
    "#   clipped_feature = examples_dataframe[\"OverallQual\"].apply(lambda x: min(x, 9.5))\n",
    "#   examples_dataframe[\"OverallQual\"] = clipped_feature\n",
    "#   # titanic_dataframe = titanic_dataframe[titanic_dataframe[\"OverallQual\"] <= 9.5]\n",
    "\n",
    "#   clipped_feature = examples_dataframe[\"GrLivArea\"].apply(lambda x: min(x, 3500)) #was max\n",
    "#   examples_dataframe[\"GrLivArea\"] = clipped_feature    \n",
    "    \n",
    "#   examples_dataframe = examples_dataframe[examples_dataframe[\"TotRmsAbvGrd\"] <= 13]\n",
    "#   examples_dataframe = examples_dataframe[examples_dataframe[\"OverallQual\"] <= 9.5]\n",
    "#   examples_dataframe = examples_dataframe[examples_dataframe[\"GrLivArea\"] <= 3500]\n",
    "\n",
    "#   examples_dataframe.dropna()\n",
    "\n",
    "  new = pd.DataFrame()\n",
    "  for i in examples_dataframe.columns:\n",
    "#     print(examples_dataframe[i])\n",
    "    new = pd.concat([new, linear_scale(examples_dataframe[i])], axis=1)\n",
    "\n",
    "#   new = linear_scale(examples_dataframe[features1])\n",
    "#   new[\"OverallQual\"] = linear_scale(examples_dataframe[\"OverallQual\"])\n",
    "#   new[\"GrLivArea\"] = linear_scale(examples_dataframe[\"GrLivArea\"])\n",
    "#   new[\"TotRmsAbvGrd\"] = linear_scale(examples_dataframe[\"TotRmsAbvGrd\"])\n",
    "#   new[\"TotalBsmtSF\"] = linear_scale(examples_dataframe[\"TotalBsmtSF\"])\n",
    "# #   new[\"1stFlrSF\"] = linear_scale(examples_dataframe[\"1stFlrSF\"])\n",
    "# #   new[\"FullBath\"] = linear_scale(examples_dataframe[\"FullBath\"])\n",
    "#   new[\"Fireplaces\"] = linear_scale(examples_dataframe[\"Fireplaces\"])\n",
    "# #   new[\"GarageYrBlt\"] = linear_scale(examples_dataframe[\"GarageYrBlt\"])\n",
    "# #   new[\"GarageCars\"] = linear_scale(examples_dataframe[\"GarageCars\"])\n",
    "#   new[\"GarageArea\"] = linear_scale(examples_dataframe[\"GarageArea\"])\n",
    "# #   new[\"SalePrice\"] = examples_dataframe[\"SalePrice\"]\n",
    "  \n",
    "  return new\n",
    "\n",
    "##############\n",
    "# clipped_feature = titanic_dataframe[\"TotRmsAbvGrd\"].apply(lambda x: min(x, 13))\n",
    "# titanic_dataframe[\"TotRmsAbvGrd\"] = clipped_feature\n",
    "# # titanic_dataframe = titanic_dataframe[titanic_dataframe[\"TotRmsAbvGrd\"] <= 13]\n",
    "\n",
    "# clipped_feature = titanic_dataframe[\"OverallQual\"].apply(lambda x: min(x, 9.5))\n",
    "# titanic_dataframe[\"OverallQual\"] = clipped_feature\n",
    "# # titanic_dataframe = titanic_dataframe[titanic_dataframe[\"OverallQual\"] <= 9.5]\n",
    "\n",
    "# clipped_feature = titanic_dataframe[\"GrLivArea\"].apply(lambda x: min(x, 3500)) #was max\n",
    "# titanic_dataframe[\"GrLivArea\"] = clipped_feature\n",
    "# titanic_dataframe = titanic_dataframe[titanic_dataframe[\"GrLivArea\"] <= 3500]\n",
    "###########\n",
    "\n",
    "# def find_na(df):\n",
    "#     l = []\n",
    "#     for label in df.columns:\n",
    "#         df.dropna()\n",
    "# #         if df[label].dropna().shape[0] != df.shape[0]:\n",
    "# #             l.append(label)\n",
    "#     return df\n",
    "\n",
    "normalized_dataframe = normalize_linear_scale(preprocess_features(titanic_dataframe, 1))#, features1)\n",
    "# normalized_dataframe.dropna()\n",
    "# print(normalized_dataframe.isnull().sum())\n",
    "normalized_training_examples = normalized_dataframe.head(1000)\n",
    "normalized_validation_examples = normalized_dataframe.tail(460)\n",
    "\n",
    "print(\"YESSS\\n\", normalized_dataframe)\n",
    "################################\n",
    "\n",
    "\n",
    "# # clipped_feature = titanic_dataframe[\"TotRmsAbvGrd\"].apply(lambda x: min(x, 13))\n",
    "# # titanic_dataframe[\"TotRmsAbvGrd\"] = clipped_feature\n",
    "\n",
    "# # clipped_feature = titanic_dataframe[\"OverallQual\"].apply(lambda x: min(x, 9.5))\n",
    "# # titanic_dataframe[\"OverallQual\"] = clipped_feature\n",
    "\n",
    "# # clipped_feature = titanic_dataframe[\"GrLivArea\"].apply(lambda x: max(x, 3500))\n",
    "# # titanic_dataframe[\"GrLivArea\"] = clipped_feature\n",
    "\n",
    "# minimal_training_examples = training_examples[minimal_features]\n",
    "# # clipped_feature = minimal_training_examples[\"TotRmsAbvGrd\"].apply(lambda x: min(x, 13))\n",
    "# # minimal_training_examples[\"TotRmsAbvGrd\"] = clipped_feature\n",
    "\n",
    "# clipped_feature = training_examples[\"OverallQual\"].apply(lambda x: min(x, 9.5))\n",
    "# training_examples[\"OverallQual\"] = clipped_feature\n",
    "\n",
    "# clipped_feature = training_examples[\"GarageArea\"].apply(lambda x: min(x, 1000))\n",
    "# training_examples[\"GarageArea\"] = clipped_feature\n",
    "\n",
    "# # clipped_feature = minimal_training_examples[\"GrLivArea\"].apply(lambda x: max(x, 3500))\n",
    "# # minimal_training_examples[\"GrLivArea\"] = clipped_feature\n",
    "\n",
    "\n",
    "# minimal_validation_examples = validation_examples[minimal_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_model() got an unexpected keyword argument 'training_examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-52763c82ab6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m              \u001b[0mtraining_targets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m              \u001b[0mvalidation_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m              validation_targets=validation_targets)\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: train_model() got an unexpected keyword argument 'training_examples'"
     ]
    }
   ],
   "source": [
    "# nn_regressor = train_nn_regression_model(\n",
    "#     my_optimizer=tf.train.AdamOptimizer(learning_rate=0.005),\n",
    "#     steps=2000,\n",
    "#     batch_size=40,\n",
    "#     hidden_units=[10, 4],\n",
    "#     training_examples=minimal_training_examples,#normalized_training_examples,#minimal_training_examples,\n",
    "#     training_targets=training_targets,\n",
    "#     validation_examples=minimal_validation_examples,#normalized_validation_examples,#\n",
    "#     validation_targets=validation_targets\n",
    "# )\n",
    "\n",
    "# With normalized values it is under 33, but is overfitting\n",
    "# dnn_regressor, rsm, validation = train_nn_regression_model(\n",
    "#     my_optimizer=tf.train.AdagradOptimizer(learning_rate=0.005),\n",
    "#     steps=1000,\n",
    "#     batch_size=10,\n",
    "# #     hidden_units=[20],\n",
    "#     training_examples=normalized_training_examples,#minimal_training_examples,#minimal_training_examples,\n",
    "#     training_targets=training_targets,\n",
    "#     validation_examples=normalized_validation_examples,#minimal_validation_examples,\n",
    "#     validation_targets=validation_targets#validation_targets\n",
    "# )\n",
    "# classifier = train_linear_classification_model(\n",
    "#              learning_rate=0.002,\n",
    "#              steps=1000,\n",
    "#              batch_size=10,\n",
    "#              training_examples=training_examples,\n",
    "#              training_targets=training_targets,\n",
    "#              validation_examples=validation_examples,\n",
    "#              validation_targets=validation_targets)\n",
    "\n",
    "classifier = train_model(\n",
    "             learning_rate=0.002,\n",
    "             steps=1000,\n",
    "             batch_size=10,\n",
    "             training_examples=training_examples,\n",
    "             training_targets=training_targets,\n",
    "             validation_examples=validation_examples,\n",
    "             validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.scatter(training_examples[\"OverallQual\"], training_targets[\"SalePrice\"])\n",
    "# plt.scatter(normalized_training_examples[\"OverallQual\"], training_targets[\"SalePrice\"])\n",
    "plt.scatter(normalized_training_examples[\"GrLivArea\"], training_targets[\"SalePrice\"])\n",
    "# plt.scatter(training_examples[\"TotalBsmtSF\"], training_targets[\"SalePrice\"])\n",
    "# plt.scatter(normalized_training_examples[\"GarageArea\"], training_targets[\"SalePrice\"])\n",
    "# plt.scatter(training_examples[\"GarageYrBlt\"], training_targets[\"SalePrice\"])\n",
    "# training_examples[\"OverallQual\"].hist()\n",
    "# training_examples[\"GrLivArea\"].hist()\n",
    "# training_examples[\"TotalBsmtSF\"].hist()\n",
    "# training_examples[\"GarageArea\"].hist()\n",
    "# training_examples[\"GarageYrBlt\"]#.hist()\n",
    "\n",
    "_ = normalized_training_examples.hist(bins=20, figsize=(18, 12), xlabelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "california_housing_test_data = pd.read_csv(\"data/test.csv\", sep=\",\")\n",
    "california_housing_saleprice_data = pd.read_csv(\"data/sample_submission.csv\", sep=\",\")\n",
    "\n",
    "# inds = pd.isnull(california_housing_test_data).any().notnull()#.nonzero()[0]\n",
    "# inds = np.where(california_housing_test_data.notnull())\n",
    "\n",
    "# print(california_housing_test_data.notnull().sum())\n",
    "# print(inds)\n",
    "# california_housing_saleprice_data = california_housing_saleprice_data.iloc[california_housing_test_data.isnull().index()]\n",
    "# california_housing_test_data = california_housing_test_data.dropna()#iloc[inds[0]]\n",
    "\n",
    "# normalize_linear_scale(preprocess_features(titanic_dataframe))\n",
    "\n",
    "print(california_housing_test_data)#.index.values\n",
    "# california_housing_saleprice_data = california_housing_saleprice_data.iloc[inds[0]]\n",
    "# for i in california_housing_test_data:\n",
    "    \n",
    "    \n",
    "# print(california_housing_test_data.isnull().sum())\n",
    "\n",
    "# california_housing_test_data = california_housing_test_data[california_housing_test_data[\"TotRmsAbvGrd\"] <= 13]\n",
    "# california_housing_test_data = california_housing_test_data[california_housing_test_data[\"OverallQual\"] <= 9.5]\n",
    "# california_housing_test_data = california_housing_test_data[california_housing_test_data[\"GrLivArea\"] <= 3500]\n",
    "\n",
    "new_examples = normalize_linear_scale(preprocess_features(california_housing_test_data))#.dropna()\n",
    "new_targets = preprocess_targets(california_housing_saleprice_data)#.dropna())\n",
    "# new_examples.fillna()\n",
    "# print(\"YESSSSSSSS\",new_examples.isnull().sum())\n",
    "# # ### Double-check\n",
    "# print(\"New examples summary:\")\n",
    "# display.display(new_examples.describe())\n",
    "# print(\"New Targets summary:\")\n",
    "display.display(new_targets.describe())\n",
    "\n",
    "# new_examples.drop(new_examples.index[[2577-1461,2121-1461]], inplace=True)\n",
    "# new_targets.drop(new_targets.index[[2577-1461,2121-1461]], inplace=True)\n",
    "\n",
    "\n",
    "# # new_examples.isnull().sum()\n",
    "print(new_examples)\n",
    "# # # new1.sort_values(ascending=True)\n",
    "\n",
    "# new_examples.fillna(new_examples.mean())\n",
    "\n",
    "predict_new_input_fn = lambda: my_input_fn(new_examples, new_targets[\"SalePrice\"], num_epochs=1, shuffle=False)\n",
    "\n",
    "new_predictions = dnn_regressor.predict(input_fn=predict_new_input_fn)\n",
    "new_predictions = np.array([item['predictions'][0] for item in new_predictions])\n",
    "\n",
    "\n",
    "#Obtain mean of columns as you need, nanmean is just convenient.\n",
    "col_mean = np.nanmean(new_predictions, axis=0)\n",
    "print(\"avg\",col_mean)\n",
    "\n",
    "\n",
    "#Find indicies that you need to replace\n",
    "inds = np.where(np.isnan(new_predictions))\n",
    "print(inds)\n",
    "\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "new_predictions[inds] = [col_mean,col_mean]#np.take(col_mean, inds[1])\n",
    "print(\"YES\")\n",
    "print(new_predictions)\n",
    "\n",
    "\n",
    "# new_preditions=new_predictions[0:2577-1461] = 100\n",
    "\n",
    "# new_predictions.fillna(new_predictions.mean())\n",
    "\n",
    "# new_predictions = new_predictions[0:2121-1462]\n",
    "# new_targets = new_targets[0:2121-1462]# = 100\n",
    "# # new = pd.DataFrame()\n",
    "# # new['SalePrice'] = 0\n",
    "new = pd.DataFrame(new_predictions, columns=[\"SalePrice\"])#[1], index=new_predictions[0])\n",
    "print(new)\n",
    "new.index.name = 'Id'\n",
    "new_targets.index.name = 'Id'\n",
    "new.rename({0:\"SalePrice\"})#, axis='columns')#[0].rename = 'SalePrice'\n",
    "# pred = new\n",
    "# new.index += 1461\n",
    "# new[\"SalePrice\"] *= 1000\n",
    "# new.to_csv('predictions_60_rmse.csv')\n",
    "# print(new.isnull().sum())\n",
    "# print(pred)\n",
    "\n",
    "\n",
    "\n",
    "new_root_mean_squared_error = math.sqrt(metrics.mean_squared_error(pred, new_targets))\n",
    "\n",
    "print(\"Final RMSE (on test data): %0.2f\" % new_root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(new_targets)\n",
    "print(new)\n",
    "plt.scatter(new_targets.index, new_targets[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(new.index, new[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # titanic_dataframe[\"rooms_per_person\"] = titanic_dataframe[\"total_rooms\"] / titanic_dataframe[\"population\"]\n",
    "\n",
    "# calibration_data = train_model(\n",
    "#     learning_rate=.5,\n",
    "#     steps=500,\n",
    "#     batch_size=5,\n",
    "#     input_feature=\"OverallQual\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def train_model(learning_rate, steps, batch_size, input_feature):\n",
    "#   \"\"\"Trains a linear regression model.\n",
    "  \n",
    "#   Args:\n",
    "#     learning_rate: A `float`, the learning rate.\n",
    "#     steps: A non-zero `int`, the total number of training steps. A training step\n",
    "#       consists of a forward and backward pass using a single batch.\n",
    "#     batch_size: A non-zero `int`, the batch size.\n",
    "#     input_feature: A `string` specifying a column from `titanic_dataframe`\n",
    "#       to use as input feature.\n",
    "      \n",
    "#   Returns:\n",
    "#     A Pandas `DataFrame` containing targets and the corresponding predictions done\n",
    "#     after training the model.\n",
    "#   \"\"\"\n",
    "  \n",
    "#   periods = 10\n",
    "#   steps_per_period = steps // periods\n",
    "\n",
    "#   my_feature = input_feature\n",
    "#   my_feature_data = titanic_dataframe[[my_feature]].astype('float32')\n",
    "#   my_label = \"SalePrice\"\n",
    "#   targets = titanic_dataframe[my_label].astype('float32')\n",
    "\n",
    "#   # Create input functions.\n",
    "#   training_input_fn = lambda: my_input_fn(my_feature_data, targets, batch_size=batch_size)\n",
    "#   predict_training_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=False)\n",
    "  \n",
    "#   # Create feature columns.\n",
    "#   feature_columns = [tf.feature_column.numeric_column(my_feature)]\n",
    "    \n",
    "#   # Create a linear regressor object.\n",
    "#   my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#   my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "#   linear_regressor = tf.estimator.LinearRegressor(\n",
    "#       feature_columns=feature_columns,\n",
    "#       optimizer=my_optimizer\n",
    "#   )\n",
    "\n",
    "#   # Set up to plot the state of our model's line each period.\n",
    "#   plt.figure(figsize=(15, 6))\n",
    "#   plt.subplot(1, 2, 1)\n",
    "#   plt.title(\"Learned Line by Period\")\n",
    "#   plt.ylabel(my_label)\n",
    "#   plt.xlabel(my_feature)\n",
    "#   sample = titanic_dataframe.sample(n=300)\n",
    "#   plt.scatter(sample[my_feature], sample[my_label])\n",
    "#   colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]\n",
    "\n",
    "#   # Train the model, but do so inside a loop so that we can periodically assess\n",
    "#   # loss metrics.\n",
    "#   print(\"Training model...\")\n",
    "#   print(\"RMSE (on training data):\")\n",
    "#   root_mean_squared_errors = []\n",
    "#   for period in range (0, periods):\n",
    "#     # Train the model, starting from the prior state.\n",
    "#     linear_regressor.train(\n",
    "#         input_fn=training_input_fn,\n",
    "#         steps=steps_per_period,\n",
    "#     )\n",
    "#     # Take a break and compute predictions.\n",
    "#     predictions = linear_regressor.predict(input_fn=predict_training_input_fn)\n",
    "#     predictions = np.array([item['predictions'][0] for item in predictions])\n",
    "    \n",
    "#     # Compute loss.\n",
    "#     root_mean_squared_error = math.sqrt(\n",
    "#       metrics.mean_squared_error(predictions, targets))\n",
    "#     # Occasionally print the current loss.\n",
    "#     print(\"  period %02d : %0.2f\" % (period, root_mean_squared_error))\n",
    "#     # Add the loss metrics from this period to our list.\n",
    "#     root_mean_squared_errors.append(root_mean_squared_error)\n",
    "#     # Finally, track the weights and biases over time.\n",
    "#     # Apply some math to ensure that the data and line are plotted neatly.\n",
    "#     y_extents = np.array([0, sample[my_label].max()])\n",
    "    \n",
    "#     weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]\n",
    "#     bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n",
    "    \n",
    "#     x_extents = (y_extents - bias) // weight\n",
    "#     x_extents = np.maximum(np.minimum(x_extents,\n",
    "#                                       sample[my_feature].max()),\n",
    "#                            sample[my_feature].min())\n",
    "#     y_extents = weight * x_extents + bias\n",
    "#     plt.plot(x_extents, y_extents, color=colors[period]) \n",
    "#   print(\"Model training finished.\")\n",
    "\n",
    "#   # Output a graph of loss metrics over periods.\n",
    "#   plt.subplot(1, 2, 2)\n",
    "#   plt.ylabel('RMSE')\n",
    "#   plt.xlabel('Periods')\n",
    "#   plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "#   plt.tight_layout()\n",
    "#   plt.plot(root_mean_squared_errors)\n",
    "\n",
    "#   # Create a table with calibration data.\n",
    "#   calibration_data = pd.DataFrame()\n",
    "#   calibration_data[\"predictions\"] = pd.Series(predictions)\n",
    "#   calibration_data[\"targets\"] = pd.Series(targets)\n",
    "#   display.display(calibration_data.describe())\n",
    "\n",
    "#   print(\"Final RMSE (on training data): %0.2f\" % root_mean_squared_error)\n",
    "  \n",
    "#   return calibration_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
