{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>32.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>49.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>512.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId  Survived  Pclass   Age  SibSp  Parch  Fare\n",
       "count        891.0     891.0   891.0 714.0  891.0  891.0 891.0\n",
       "mean         446.0       0.4     2.3  29.7    0.5    0.4  32.2\n",
       "std          257.4       0.5     0.8  14.5    1.1    0.8  49.7\n",
       "min            1.0       0.0     1.0   0.4    0.0    0.0   0.0\n",
       "25%          223.5       0.0     2.0  20.1    0.0    0.0   7.9\n",
       "50%          446.0       0.0     3.0  28.0    0.0    0.0  14.5\n",
       "75%          668.5       1.0     3.0  38.0    1.0    0.0  31.0\n",
       "max          891.0       1.0     3.0  80.0    8.0    6.0 512.3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "\n",
    "titanic_dataframe = pd.read_csv(\"titanic/train.csv\", sep=\",\")\n",
    "\n",
    "titanic_dataframe = titanic_dataframe.reindex(\n",
    "    np.random.permutation(titanic_dataframe.index))\n",
    "\n",
    "titanic_dataframe.describe()\n",
    "\n",
    "# titanic_dataframe = titanic_dataframe[titanic_dataframe[\"TotRmsAbvGrd\"] <= 13]\n",
    "# titanic_dataframe = titanic_dataframe[titanic_dataframe[\"OverallQual\"] <= 9.5]\n",
    "# titanic_dataframe = titanic_dataframe[titanic_dataframe[\"GrLivArea\"] <= 3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# titanic_dataframe[\"Cabin\"].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# global selected_features_index = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_dataframe(titanic_dataframe, drop):\n",
    "  correlation_dataframe = titanic_dataframe.copy()\n",
    "  correlation_dataframe[\"target\"] = correlation_dataframe[\"Survived\"]\n",
    "  if drop:#creat drop function\n",
    "    plt.scatter(correlation_dataframe['Age'], correlation_dataframe[\"target\"])\n",
    "\n",
    "    correlation_dataframe = correlation_dataframe.dropna(subset = ['Age'])# correlation_dataframe['Age'].dropna()\n",
    "    correlation_dataframe = correlation_dataframe.drop(['Cabin', 'Name'], axis=1)# correlation_dataframe['Fare'].dropna()\n",
    "    correlation_dataframe = correlation_dataframe.dropna(subset = ['Embarked'])\n",
    "  return correlation_dataframe\n",
    "\n",
    "def preprocess_features(correlation_dataframe):#, features):\n",
    "  \"\"\"Prepares input features from California housing data set.\n",
    "\n",
    "  Args:\n",
    "    titanic_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the California housing data set.\n",
    "  Returns:\n",
    "    A DataFrame that contains the features to be used for the model, including\n",
    "    synthetic features.\n",
    "  \"\"\"\n",
    "\n",
    "#     put back in\n",
    "  new = pd.DataFrame(correlation_dataframe)#.copy#.corr()\n",
    "  new = pd.concat([correlation_dataframe, pd.get_dummies(correlation_dataframe['SibSp']).rename(columns={0:\"SS0\", 1:'SS1', 2:'SS2'}),\n",
    "                 pd.get_dummies(correlation_dataframe['Parch']).rename(columns={0:\"PC0\", 1:'PC1', 2:'PC2'}),\n",
    "                 pd.get_dummies(correlation_dataframe['Sex']), pd.get_dummies(correlation_dataframe['Embarked']),\n",
    "                pd.get_dummies(correlation_dataframe['Pclass']).rename(columns={1:'Class1', 2:'Class2', 3:\"Class3\"})],\n",
    "                axis=1)\n",
    "\n",
    "  print(new.head(7))\n",
    "\n",
    "  new.set_index('PassengerId')\n",
    "  new1 = new.corr()\n",
    "  features_i = new1['target'].sort_values(ascending=False).head(9)\n",
    "#   features_i\n",
    "\n",
    "  # new.hist(figsize=(20,20))\n",
    "  # print(features)\n",
    "  features_index = features_i.index\n",
    "  print(\"\\nChosen Features NULLs?\\n\\n\", new[features_index].isnull().sum())\n",
    "\n",
    "  selected_features = new[features_index]\n",
    "  processed_features = selected_features.copy()\n",
    "\n",
    "  processed_features = processed_features.drop(['target', 'Survived'], axis=1)\n",
    "#Simple version\n",
    "#   processed_features = pd.DataFrame()\n",
    "#   processed_features['Fare'] = titanic_dataframe['Fare']\n",
    "\n",
    "  return processed_features, features_index\n",
    "\n",
    "def preprocess_test_features(correlation_dataframe, features_index, PID):\n",
    "\n",
    "  new = pd.DataFrame(correlation_dataframe)#.copy#.corr()\n",
    "  new = pd.concat([correlation_dataframe, pd.get_dummies(correlation_dataframe['SibSp']).rename(columns={0:\"SS0\", 1:'SS1', 2:'SS2'}),\n",
    "                 pd.get_dummies(correlation_dataframe['Parch']).rename(columns={0:\"PC0\", 1:'PC1', 2:'PC2'}),\n",
    "                 pd.get_dummies(correlation_dataframe['Sex']), pd.get_dummies(correlation_dataframe['Embarked']),\n",
    "                pd.get_dummies(correlation_dataframe['Pclass']).rename(columns={1:'Class1', 2:'Class2', 3:\"Class3\"})],\n",
    "                axis=1)\n",
    "\n",
    "  selected_features = new[features_index]\n",
    "#   selected_features = pd.concat([new[features_index], correlation_dataframe['PassengerId']], axis=1)\n",
    "#   new.set_index(PID)\n",
    "  processed_features = selected_features.copy()\n",
    "#   processed_features = processed_features.drop(['Survived'], axis=1)\n",
    "  return processed_features\n",
    "\n",
    "def preprocess_targets(correlation_dataframe):\n",
    "  \"\"\"Prepares target features (i.e., labels) from California housing data set.\n",
    "\n",
    "  Args:\n",
    "    titanic_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the California housing data set.\n",
    "  Returns:\n",
    "    A DataFrame that contains the target feature.\n",
    "  \"\"\"\n",
    "\n",
    "  output_targets = pd.DataFrame()\n",
    "#   output_targets['PassengerId'] = correlation_dataframe['PassengerId']\n",
    "  output_targets[\"Survived\"] = correlation_dataframe[\"Survived\"]\n",
    "#   output_targets.set_index('PassengerId')\n",
    "  return output_targets\n",
    "# titanic_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass     Sex  Age  SibSp  Parch    Ticket  Fare  \\\n",
      "688          689         0       3    male 18.0      0      0    350036   7.8   \n",
      "1              2         1       1  female 38.0      1      0  PC 17599  71.3   \n",
      "805          806         0       3    male 31.0      0      0    347063   7.8   \n",
      "153          154         0       3    male 40.5      0      2  A/5. 851  14.5   \n",
      "178          179         0       2    male 30.0      0      0    250653  13.0   \n",
      "623          624         0       3    male 21.0      0      0    350029   7.9   \n",
      "125          126         1       3    male 12.0      1      0      2651  11.2   \n",
      "\n",
      "    Embarked   ...     5   6  female  male   C   Q   S  Class1  Class2  Class3  \n",
      "688        S   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0     0.0     0.0     1.0  \n",
      "1          C   ...   0.0 0.0     1.0   0.0 1.0 0.0 0.0     1.0     0.0     0.0  \n",
      "805        S   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0     0.0     0.0     1.0  \n",
      "153        S   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0     0.0     0.0     1.0  \n",
      "178        S   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0     0.0     1.0     0.0  \n",
      "623        S   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0     0.0     0.0     1.0  \n",
      "125        C   ...   0.0 0.0     0.0   1.0 1.0 0.0 0.0     0.0     0.0     1.0  \n",
      "\n",
      "[7 rows x 32 columns]\n",
      "\n",
      "Chosen Features NULLs?\n",
      "\n",
      " Survived    0\n",
      "target      0\n",
      "female      0\n",
      "Class1      0\n",
      "Fare        0\n",
      "C           0\n",
      "SS1         0\n",
      "PC1         0\n",
      "PC2         0\n",
      "dtype: int64\n",
      "Training examples summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>Class1</th>\n",
       "      <th>Fare</th>\n",
       "      <th>C</th>\n",
       "      <th>SS1</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>108.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>227.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>247.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>106.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>247.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>691 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     female  Class1  Fare   C  SS1  PC1  PC2\n",
       "688     0.0     0.0   7.8 0.0  0.0  0.0  0.0\n",
       "1       1.0     1.0  71.3 1.0  1.0  0.0  0.0\n",
       "805     0.0     0.0   7.8 0.0  0.0  0.0  0.0\n",
       "153     0.0     0.0  14.5 0.0  0.0  0.0  1.0\n",
       "178     0.0     0.0  13.0 0.0  0.0  0.0  0.0\n",
       "623     0.0     0.0   7.9 0.0  0.0  0.0  0.0\n",
       "125     0.0     0.0  11.2 1.0  1.0  0.0  0.0\n",
       "681     0.0     1.0  76.7 1.0  0.0  0.0  0.0\n",
       "871     1.0     1.0  52.6 0.0  1.0  1.0  0.0\n",
       "187     0.0     1.0  26.6 0.0  0.0  0.0  0.0\n",
       "102     0.0     1.0  77.3 0.0  0.0  1.0  0.0\n",
       "505     0.0     1.0 108.9 1.0  1.0  0.0  0.0\n",
       "302     0.0     0.0   0.0 0.0  0.0  0.0  0.0\n",
       "556     1.0     1.0  39.6 1.0  1.0  0.0  0.0\n",
       "480     0.0     0.0  46.9 0.0  0.0  0.0  1.0\n",
       "848     0.0     0.0  33.0 0.0  0.0  1.0  0.0\n",
       "287     0.0     0.0   7.9 0.0  0.0  0.0  0.0\n",
       "132     1.0     0.0  14.5 0.0  1.0  0.0  0.0\n",
       "767     1.0     0.0   7.8 0.0  0.0  0.0  0.0\n",
       "728     0.0     0.0  26.0 0.0  1.0  0.0  0.0\n",
       "197     0.0     0.0   8.4 0.0  0.0  1.0  0.0\n",
       "649     1.0     0.0   7.5 0.0  0.0  0.0  0.0\n",
       "167     1.0     0.0  27.9 0.0  1.0  0.0  0.0\n",
       "207     0.0     0.0  18.8 1.0  0.0  0.0  0.0\n",
       "580     1.0     0.0  30.0 0.0  1.0  1.0  0.0\n",
       "603     0.0     0.0   8.1 0.0  0.0  0.0  0.0\n",
       "717     1.0     0.0  10.5 0.0  0.0  0.0  0.0\n",
       "752     0.0     0.0   9.5 0.0  0.0  0.0  0.0\n",
       "380     1.0     1.0 227.5 1.0  0.0  0.0  0.0\n",
       "372     0.0     0.0   8.1 0.0  0.0  0.0  0.0\n",
       "..      ...     ...   ...  ..  ...  ...  ...\n",
       "759     1.0     1.0  86.5 0.0  0.0  0.0  0.0\n",
       "323     1.0     0.0  29.0 0.0  1.0  1.0  0.0\n",
       "408     0.0     0.0   7.8 0.0  0.0  0.0  0.0\n",
       "657     1.0     0.0  15.5 0.0  1.0  1.0  0.0\n",
       "404     1.0     0.0   8.7 0.0  0.0  0.0  0.0\n",
       "313     0.0     0.0   7.9 0.0  0.0  0.0  0.0\n",
       "608     1.0     0.0  41.6 1.0  1.0  0.0  1.0\n",
       "487     0.0     1.0  29.7 1.0  0.0  0.0  0.0\n",
       "299     1.0     1.0 247.5 1.0  0.0  1.0  0.0\n",
       "583     0.0     1.0  40.1 1.0  0.0  0.0  0.0\n",
       "795     0.0     0.0  13.0 0.0  0.0  0.0  0.0\n",
       "91      0.0     0.0   7.9 0.0  0.0  0.0  0.0\n",
       "441     0.0     0.0   9.5 0.0  0.0  0.0  0.0\n",
       "103     0.0     0.0   8.7 0.0  0.0  0.0  0.0\n",
       "836     0.0     0.0   8.7 0.0  0.0  0.0  0.0\n",
       "314     0.0     0.0  26.2 0.0  1.0  1.0  0.0\n",
       "520     1.0     1.0  93.5 0.0  0.0  0.0  0.0\n",
       "844     0.0     0.0   8.7 0.0  0.0  0.0  0.0\n",
       "662     0.0     1.0  25.6 0.0  0.0  0.0  0.0\n",
       "671     0.0     1.0  52.0 0.0  1.0  0.0  0.0\n",
       "175     0.0     0.0   7.9 0.0  1.0  1.0  0.0\n",
       "437     1.0     0.0  18.8 0.0  0.0  0.0  0.0\n",
       "544     0.0     1.0 106.4 1.0  1.0  0.0  0.0\n",
       "558     1.0     1.0  79.7 0.0  1.0  1.0  0.0\n",
       "248     0.0     1.0  52.6 0.0  1.0  1.0  0.0\n",
       "118     0.0     1.0 247.5 1.0  0.0  1.0  0.0\n",
       "764     0.0     0.0   7.8 0.0  0.0  0.0  0.0\n",
       "204     0.0     0.0   8.1 0.0  0.0  0.0  0.0\n",
       "171     0.0     0.0  29.1 0.0  0.0  1.0  0.0\n",
       "389     1.0     0.0  12.0 1.0  0.0  0.0  0.0\n",
       "\n",
       "[691 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation examples summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>Class1</th>\n",
       "      <th>Fare</th>\n",
       "      <th>C</th>\n",
       "      <th>SS1</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>52.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>512.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       female  Class1  Fare     C   SS1   PC1   PC2\n",
       "count   460.0   460.0 460.0 460.0 460.0 460.0 460.0\n",
       "mean      0.4     0.3  35.3   0.2   0.3   0.2   0.1\n",
       "std       0.5     0.4  52.8   0.4   0.4   0.4   0.3\n",
       "min       0.0     0.0   0.0   0.0   0.0   0.0   0.0\n",
       "25%       0.0     0.0   8.1   0.0   0.0   0.0   0.0\n",
       "50%       0.0     0.0  15.9   0.0   0.0   0.0   0.0\n",
       "75%       1.0     1.0  36.8   0.0   1.0   0.0   0.0\n",
       "max       1.0     1.0 512.3   1.0   1.0   1.0   1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training targets summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>691.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Survived\n",
       "count     691.0\n",
       "mean        0.4\n",
       "std         0.5\n",
       "min         0.0\n",
       "25%         0.0\n",
       "50%         0.0\n",
       "75%         1.0\n",
       "max         1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation targets summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Survived\n",
       "count     460.0\n",
       "mean        0.4\n",
       "std         0.5\n",
       "min         0.0\n",
       "25%         0.0\n",
       "50%         0.0\n",
       "75%         1.0\n",
       "max         1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6ba9a2f128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose the first 12000 (out of 17000) examples for training.\n",
    "correlation_df = preprocess_dataframe(titanic_dataframe, 1)\n",
    "\n",
    "examples, features_index = preprocess_features(correlation_df)\n",
    "training_examples = examples.head(691)#preprocess_features(correlation_df.head(691), 1)\n",
    "training_targets = preprocess_targets(correlation_df.head(691))#, 1)\n",
    "#\n",
    "# correlation_dataframe = titanic_dataframe.copy()\n",
    "# correlation_dataframe[\"target\"] = training_targets[\"SalePrice\"]\n",
    "# new = correlation_dataframe.corr()\n",
    "# print(new)\n",
    "# features1 = new['target'].sort_values(ascending=False).head(10).index\n",
    "# features1 = features1[2:-1]\n",
    "# #\n",
    "# training_examples = preprocess_features(titanic_dataframe.head(1000), features1)\n",
    "\n",
    "# Choose the last 5000 (out of 17000) examples for validation.\n",
    "validation_examples = examples.head(460)#preprocess_features(correlation_df.tail(460), 1)#, features1)\n",
    "validation_targets = preprocess_targets(correlation_df.tail(460))#, 1)\n",
    "\n",
    "# display.display(titanic_dataframe.describe())\n",
    "\n",
    "# Double-check that we've done the right thing.\n",
    "print(\"Training examples summary:\")\n",
    "display.display(training_examples)#.describe())\n",
    "print(\"Validation examples summary:\")\n",
    "display.display(validation_examples.describe())\n",
    "\n",
    "print(\"Training targets summary:\")\n",
    "display.display(training_targets.describe())\n",
    "print(\"Validation targets summary:\")\n",
    "display.display(validation_targets.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #find good feratures based on sale price\n",
    "# correlation_dataframe = titanic_dataframe.copy()\n",
    "# correlation_dataframe[\"target\"] = titanic_dataframe[\"SalePrice\"]\n",
    "\n",
    "# # display.display(training_targets.describe())\n",
    "\n",
    "# correlation_dataframe.corr()\n",
    "# new = pd.DataFrame()\n",
    "# new['target'] = correlation_dataframe.corr()['target']\n",
    "# new.to_csv('corr.csv')\n",
    "# # correlation_dataframe[\"OpenPorchSF\"].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation_dataframe = titanic_dataframe.copy()\n",
    "# correlation_dataframe[\"target\"] = training_targets[\"SalePrice\"]\n",
    "\n",
    "# # display.display(training_targets.describe())\n",
    "# # pd.options.display.max_rows = 20\n",
    "\n",
    "# new = correlation_dataframe.corr()\n",
    "# print(new['target'].sort_values(ascending=False).head(10))#.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    \"\"\"Trains a linear regression model.\n",
    "  \n",
    "    Args:\n",
    "      features: pandas DataFrame of features\n",
    "      targets: pandas DataFrame of targets\n",
    "      batch_size: Size of batches to be passed to the model\n",
    "      shuffle: True or False. Whether to shuffle the data.\n",
    "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
    "    Returns:\n",
    "      Tuple of (features, labels) for next data batch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert pandas data into a dict of np arrays.\n",
    "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
    "    \n",
    "    # Construct a dataset, and configure batching/repeating.\n",
    "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "    # Shuffle the data, if specified.\n",
    "    if shuffle:\n",
    "      ds = ds.shuffle(10000)\n",
    "    \n",
    "    # Return the next batch of data.\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_feature_columns(input_features):\n",
    "  \"\"\"Construct the TensorFlow Feature Columns.\n",
    "\n",
    "  Args:\n",
    "    input_features: The names of the numerical input features to use.\n",
    "  Returns:\n",
    "    A set of feature columns\n",
    "  \"\"\" \n",
    "  return set([tf.feature_column.numeric_column(my_feature)\n",
    "              for my_feature in input_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_linear_classifier_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "  \"\"\"Trains a linear classification model.\n",
    "  \n",
    "  In addition to training, this function also prints training progress information,\n",
    "  as well as a plot of the training and validation loss over time.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: A `float`, the learning rate.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    training_examples: A `DataFrame` containing one or more columns from\n",
    "      `california_housing_dataframe` to use as input features for training.\n",
    "    training_targets: A `DataFrame` containing exactly one column from\n",
    "      `california_housing_dataframe` to use as target for training.\n",
    "    validation_examples: A `DataFrame` containing one or more columns from\n",
    "      `california_housing_dataframe` to use as input features for validation.\n",
    "    validation_targets: A `DataFrame` containing exactly one column from\n",
    "      `california_housing_dataframe` to use as target for validation.\n",
    "      \n",
    "  Returns:\n",
    "    A `LinearClassifier` object trained on the training data.\n",
    "  \"\"\"\n",
    "\n",
    "  periods = 10\n",
    "  steps_per_period = steps / periods\n",
    "  \n",
    "  # Create a linear classifier object.\n",
    "  my_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)  \n",
    "  linear_classifier = tf.estimator.DNNClassifier(\n",
    "      feature_columns=construct_feature_columns(training_examples),\n",
    "      hidden_units=hidden_units,\n",
    "      optimizer=my_optimizer\n",
    "  )\n",
    "  \n",
    "  # Create input functions.\n",
    "  training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                          training_targets[\"Survived\"], \n",
    "                                          batch_size=batch_size)\n",
    "  predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                                  training_targets[\"Survived\"], \n",
    "                                                  num_epochs=1, \n",
    "                                                  shuffle=False)\n",
    "  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                                    validation_targets[\"Survived\"], \n",
    "                                                    num_epochs=1, \n",
    "                                                    shuffle=False)\n",
    "  \n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print(\"Training model...\")\n",
    "  print(\"LogLoss (on training data):\")\n",
    "  training_log_losses = []\n",
    "  validation_log_losses = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    linear_classifier.train(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps_per_period\n",
    "    )\n",
    "    # Take a break and compute predictions.    \n",
    "    training_probabilities = linear_classifier.predict(input_fn=predict_training_input_fn)\n",
    "    training_probabilities = np.array([item['probabilities'] for item in training_probabilities])\n",
    "    \n",
    "    validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "    validation_probabilities = np.array([item['probabilities'] for item in validation_probabilities])\n",
    "    \n",
    "    training_log_loss = metrics.log_loss(training_targets, training_probabilities)\n",
    "    validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)\n",
    "    # Occasionally print the current loss.\n",
    "    print(\"  period %02d : %0.2f\" % (period, training_log_loss))\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    training_log_losses.append(training_log_loss)\n",
    "    validation_log_losses.append(validation_log_loss)\n",
    "  print(\"Model training finished.\")\n",
    "  \n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.ylabel(\"LogLoss\")\n",
    "  plt.xlabel(\"Periods\")\n",
    "  plt.title(\"LogLoss vs. Periods\")\n",
    "  plt.tight_layout()\n",
    "  plt.plot(training_log_losses, label=\"training\")\n",
    "  plt.plot(validation_log_losses, label=\"validation\")\n",
    "  plt.legend()\n",
    "\n",
    "  return linear_classifier, predict_validation_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_scale(series):\n",
    "  min_val = series.min()\n",
    "  max_val = series.max()\n",
    "  scale = (max_val - min_val) / 2.0\n",
    "  return series.apply(lambda x:((x - min_val) / scale) - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass     Sex  Age  SibSp  Parch    Ticket  Fare  \\\n",
      "688          689         0       3    male 18.0      0      0    350036   7.8   \n",
      "1              2         1       1  female 38.0      1      0  PC 17599  71.3   \n",
      "805          806         0       3    male 31.0      0      0    347063   7.8   \n",
      "153          154         0       3    male 40.5      0      2  A/5. 851  14.5   \n",
      "178          179         0       2    male 30.0      0      0    250653  13.0   \n",
      "623          624         0       3    male 21.0      0      0    350029   7.9   \n",
      "125          126         1       3    male 12.0      1      0      2651  11.2   \n",
      "\n",
      "    Embarked   ...     5   6  female  male   C   Q   S  Class1  Class2  Class3  \n",
      "688        S   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0     0.0     0.0     1.0  \n",
      "1          C   ...   0.0 0.0     1.0   0.0 1.0 0.0 0.0     1.0     0.0     0.0  \n",
      "805        S   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0     0.0     0.0     1.0  \n",
      "153        S   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0     0.0     0.0     1.0  \n",
      "178        S   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0     0.0     1.0     0.0  \n",
      "623        S   ...   0.0 0.0     0.0   1.0 0.0 0.0 1.0     0.0     0.0     1.0  \n",
      "125        C   ...   0.0 0.0     0.0   1.0 1.0 0.0 0.0     0.0     0.0     1.0  \n",
      "\n",
      "[7 rows x 32 columns]\n",
      "\n",
      "Chosen Features NULLs?\n",
      "\n",
      " Survived    0\n",
      "target      0\n",
      "female      0\n",
      "Class1      0\n",
      "Fare        0\n",
      "C           0\n",
      "SS1         0\n",
      "PC1         0\n",
      "PC2         0\n",
      "dtype: int64\n",
      "688   0.0\n",
      "1     1.0\n",
      "805   0.0\n",
      "153   0.0\n",
      "178   0.0\n",
      "623   0.0\n",
      "125   0.0\n",
      "681   0.0\n",
      "871   1.0\n",
      "187   0.0\n",
      "102   0.0\n",
      "505   0.0\n",
      "302   0.0\n",
      "556   1.0\n",
      "480   0.0\n",
      "848   0.0\n",
      "287   0.0\n",
      "132   1.0\n",
      "767   1.0\n",
      "728   0.0\n",
      "197   0.0\n",
      "649   1.0\n",
      "167   1.0\n",
      "207   0.0\n",
      "580   1.0\n",
      "603   0.0\n",
      "717   1.0\n",
      "752   0.0\n",
      "380   1.0\n",
      "372   0.0\n",
      "       ..\n",
      "437   1.0\n",
      "544   0.0\n",
      "558   1.0\n",
      "248   0.0\n",
      "118   0.0\n",
      "764   0.0\n",
      "204   0.0\n",
      "171   0.0\n",
      "389   1.0\n",
      "122   0.0\n",
      "104   0.0\n",
      "150   0.0\n",
      "569   0.0\n",
      "309   1.0\n",
      "373   0.0\n",
      "515   0.0\n",
      "559   1.0\n",
      "157   0.0\n",
      "405   0.0\n",
      "68    1.0\n",
      "701   0.0\n",
      "403   0.0\n",
      "532   0.0\n",
      "754   1.0\n",
      "52    1.0\n",
      "708   1.0\n",
      "24    1.0\n",
      "801   1.0\n",
      "847   0.0\n",
      "646   0.0\n",
      "Name: female, dtype: float64\n",
      "688   0.0\n",
      "1     1.0\n",
      "805   0.0\n",
      "153   0.0\n",
      "178   0.0\n",
      "623   0.0\n",
      "125   0.0\n",
      "681   1.0\n",
      "871   1.0\n",
      "187   1.0\n",
      "102   1.0\n",
      "505   1.0\n",
      "302   0.0\n",
      "556   1.0\n",
      "480   0.0\n",
      "848   0.0\n",
      "287   0.0\n",
      "132   0.0\n",
      "767   0.0\n",
      "728   0.0\n",
      "197   0.0\n",
      "649   0.0\n",
      "167   0.0\n",
      "207   0.0\n",
      "580   0.0\n",
      "603   0.0\n",
      "717   0.0\n",
      "752   0.0\n",
      "380   1.0\n",
      "372   0.0\n",
      "       ..\n",
      "437   0.0\n",
      "544   1.0\n",
      "558   1.0\n",
      "248   1.0\n",
      "118   1.0\n",
      "764   0.0\n",
      "204   0.0\n",
      "171   0.0\n",
      "389   0.0\n",
      "122   0.0\n",
      "104   0.0\n",
      "150   0.0\n",
      "569   0.0\n",
      "309   1.0\n",
      "373   1.0\n",
      "515   1.0\n",
      "559   0.0\n",
      "157   0.0\n",
      "405   0.0\n",
      "68    0.0\n",
      "701   1.0\n",
      "403   0.0\n",
      "532   0.0\n",
      "754   0.0\n",
      "52    1.0\n",
      "708   1.0\n",
      "24    0.0\n",
      "801   0.0\n",
      "847   0.0\n",
      "646   0.0\n",
      "Name: Class1, dtype: float64\n",
      "688     7.8\n",
      "1      71.3\n",
      "805     7.8\n",
      "153    14.5\n",
      "178    13.0\n",
      "623     7.9\n",
      "125    11.2\n",
      "681    76.7\n",
      "871    52.6\n",
      "187    26.6\n",
      "102    77.3\n",
      "505   108.9\n",
      "302     0.0\n",
      "556    39.6\n",
      "480    46.9\n",
      "848    33.0\n",
      "287     7.9\n",
      "132    14.5\n",
      "767     7.8\n",
      "728    26.0\n",
      "197     8.4\n",
      "649     7.5\n",
      "167    27.9\n",
      "207    18.8\n",
      "580    30.0\n",
      "603     8.1\n",
      "717    10.5\n",
      "752     9.5\n",
      "380   227.5\n",
      "372     8.1\n",
      "       ... \n",
      "437    18.8\n",
      "544   106.4\n",
      "558    79.7\n",
      "248    52.6\n",
      "118   247.5\n",
      "764     7.8\n",
      "204     8.1\n",
      "171    29.1\n",
      "389    12.0\n",
      "122    30.1\n",
      "104     7.9\n",
      "150    12.5\n",
      "569     7.9\n",
      "309    56.9\n",
      "373   135.6\n",
      "515    34.0\n",
      "559    17.4\n",
      "157     8.1\n",
      "405    21.0\n",
      "68      7.9\n",
      "701    26.3\n",
      "403    15.8\n",
      "532     7.2\n",
      "754    65.0\n",
      "52     76.7\n",
      "708   151.6\n",
      "24     21.1\n",
      "801    26.2\n",
      "847     7.9\n",
      "646     7.9\n",
      "Name: Fare, dtype: float64\n",
      "688   0.0\n",
      "1     1.0\n",
      "805   0.0\n",
      "153   0.0\n",
      "178   0.0\n",
      "623   0.0\n",
      "125   1.0\n",
      "681   1.0\n",
      "871   0.0\n",
      "187   0.0\n",
      "102   0.0\n",
      "505   1.0\n",
      "302   0.0\n",
      "556   1.0\n",
      "480   0.0\n",
      "848   0.0\n",
      "287   0.0\n",
      "132   0.0\n",
      "767   0.0\n",
      "728   0.0\n",
      "197   0.0\n",
      "649   0.0\n",
      "167   0.0\n",
      "207   1.0\n",
      "580   0.0\n",
      "603   0.0\n",
      "717   0.0\n",
      "752   0.0\n",
      "380   1.0\n",
      "372   0.0\n",
      "       ..\n",
      "437   0.0\n",
      "544   1.0\n",
      "558   0.0\n",
      "248   0.0\n",
      "118   1.0\n",
      "764   0.0\n",
      "204   0.0\n",
      "171   0.0\n",
      "389   1.0\n",
      "122   1.0\n",
      "104   0.0\n",
      "150   0.0\n",
      "569   0.0\n",
      "309   1.0\n",
      "373   1.0\n",
      "515   0.0\n",
      "559   0.0\n",
      "157   0.0\n",
      "405   0.0\n",
      "68    0.0\n",
      "701   0.0\n",
      "403   0.0\n",
      "532   1.0\n",
      "754   0.0\n",
      "52    1.0\n",
      "708   0.0\n",
      "24    0.0\n",
      "801   0.0\n",
      "847   1.0\n",
      "646   0.0\n",
      "Name: C, dtype: float64\n",
      "688   0.0\n",
      "1     1.0\n",
      "805   0.0\n",
      "153   0.0\n",
      "178   0.0\n",
      "623   0.0\n",
      "125   1.0\n",
      "681   0.0\n",
      "871   1.0\n",
      "187   0.0\n",
      "102   0.0\n",
      "505   1.0\n",
      "302   0.0\n",
      "556   1.0\n",
      "480   0.0\n",
      "848   0.0\n",
      "287   0.0\n",
      "132   1.0\n",
      "767   0.0\n",
      "728   1.0\n",
      "197   0.0\n",
      "649   0.0\n",
      "167   1.0\n",
      "207   0.0\n",
      "580   1.0\n",
      "603   0.0\n",
      "717   0.0\n",
      "752   0.0\n",
      "380   0.0\n",
      "372   0.0\n",
      "       ..\n",
      "437   0.0\n",
      "544   1.0\n",
      "558   1.0\n",
      "248   1.0\n",
      "118   0.0\n",
      "764   0.0\n",
      "204   0.0\n",
      "171   0.0\n",
      "389   0.0\n",
      "122   1.0\n",
      "104   0.0\n",
      "150   0.0\n",
      "569   0.0\n",
      "309   0.0\n",
      "373   0.0\n",
      "515   0.0\n",
      "559   1.0\n",
      "157   0.0\n",
      "405   1.0\n",
      "68    0.0\n",
      "701   0.0\n",
      "403   1.0\n",
      "532   1.0\n",
      "754   1.0\n",
      "52    1.0\n",
      "708   0.0\n",
      "24    0.0\n",
      "801   1.0\n",
      "847   0.0\n",
      "646   0.0\n",
      "Name: SS1, dtype: float64\n",
      "688   0.0\n",
      "1     0.0\n",
      "805   0.0\n",
      "153   0.0\n",
      "178   0.0\n",
      "623   0.0\n",
      "125   0.0\n",
      "681   0.0\n",
      "871   1.0\n",
      "187   0.0\n",
      "102   1.0\n",
      "505   0.0\n",
      "302   0.0\n",
      "556   0.0\n",
      "480   0.0\n",
      "848   1.0\n",
      "287   0.0\n",
      "132   0.0\n",
      "767   0.0\n",
      "728   0.0\n",
      "197   1.0\n",
      "649   0.0\n",
      "167   0.0\n",
      "207   0.0\n",
      "580   1.0\n",
      "603   0.0\n",
      "717   0.0\n",
      "752   0.0\n",
      "380   0.0\n",
      "372   0.0\n",
      "       ..\n",
      "437   0.0\n",
      "544   0.0\n",
      "558   1.0\n",
      "248   1.0\n",
      "118   1.0\n",
      "764   0.0\n",
      "204   0.0\n",
      "171   1.0\n",
      "389   0.0\n",
      "122   0.0\n",
      "104   0.0\n",
      "150   0.0\n",
      "569   0.0\n",
      "309   0.0\n",
      "373   0.0\n",
      "515   0.0\n",
      "559   0.0\n",
      "157   0.0\n",
      "405   0.0\n",
      "68    0.0\n",
      "701   0.0\n",
      "403   0.0\n",
      "532   1.0\n",
      "754   0.0\n",
      "52    0.0\n",
      "708   0.0\n",
      "24    1.0\n",
      "801   1.0\n",
      "847   0.0\n",
      "646   0.0\n",
      "Name: PC1, dtype: float64\n",
      "688   0.0\n",
      "1     0.0\n",
      "805   0.0\n",
      "153   1.0\n",
      "178   0.0\n",
      "623   0.0\n",
      "125   0.0\n",
      "681   0.0\n",
      "871   0.0\n",
      "187   0.0\n",
      "102   0.0\n",
      "505   0.0\n",
      "302   0.0\n",
      "556   0.0\n",
      "480   1.0\n",
      "848   0.0\n",
      "287   0.0\n",
      "132   0.0\n",
      "767   0.0\n",
      "728   0.0\n",
      "197   0.0\n",
      "649   0.0\n",
      "167   0.0\n",
      "207   0.0\n",
      "580   0.0\n",
      "603   0.0\n",
      "717   0.0\n",
      "752   0.0\n",
      "380   0.0\n",
      "372   0.0\n",
      "       ..\n",
      "437   0.0\n",
      "544   0.0\n",
      "558   0.0\n",
      "248   0.0\n",
      "118   0.0\n",
      "764   0.0\n",
      "204   0.0\n",
      "171   0.0\n",
      "389   0.0\n",
      "122   0.0\n",
      "104   0.0\n",
      "150   0.0\n",
      "569   0.0\n",
      "309   0.0\n",
      "373   0.0\n",
      "515   0.0\n",
      "559   0.0\n",
      "157   0.0\n",
      "405   0.0\n",
      "68    1.0\n",
      "701   0.0\n",
      "403   0.0\n",
      "532   0.0\n",
      "754   1.0\n",
      "52    0.0\n",
      "708   0.0\n",
      "24    0.0\n",
      "801   0.0\n",
      "847   0.0\n",
      "646   0.0\n",
      "Name: PC2, dtype: float64\n",
      "873   0.0\n",
      "630   0.0\n",
      "817   0.0\n",
      "60    0.0\n",
      "111   1.0\n",
      "789   0.0\n",
      "177   1.0\n",
      "572   0.0\n",
      "814   0.0\n",
      "244   0.0\n",
      "503   1.0\n",
      "449   0.0\n",
      "676   0.0\n",
      "92    0.0\n",
      "553   0.0\n",
      "887   1.0\n",
      "693   0.0\n",
      "561   0.0\n",
      "156   1.0\n",
      "120   0.0\n",
      "191   0.0\n",
      "670   1.0\n",
      "494   0.0\n",
      "624   0.0\n",
      "753   0.0\n",
      "416   1.0\n",
      "105   0.0\n",
      "163   0.0\n",
      "382   0.0\n",
      "206   0.0\n",
      "       ..\n",
      "437   1.0\n",
      "544   0.0\n",
      "558   1.0\n",
      "248   0.0\n",
      "118   0.0\n",
      "764   0.0\n",
      "204   0.0\n",
      "171   0.0\n",
      "389   1.0\n",
      "122   0.0\n",
      "104   0.0\n",
      "150   0.0\n",
      "569   0.0\n",
      "309   1.0\n",
      "373   0.0\n",
      "515   0.0\n",
      "559   1.0\n",
      "157   0.0\n",
      "405   0.0\n",
      "68    1.0\n",
      "701   0.0\n",
      "403   0.0\n",
      "532   0.0\n",
      "754   1.0\n",
      "52    1.0\n",
      "708   1.0\n",
      "24    1.0\n",
      "801   1.0\n",
      "847   0.0\n",
      "646   0.0\n",
      "Name: female, dtype: float64\n",
      "873   0.0\n",
      "630   1.0\n",
      "817   0.0\n",
      "60    0.0\n",
      "111   0.0\n",
      "789   1.0\n",
      "177   1.0\n",
      "572   1.0\n",
      "814   0.0\n",
      "244   0.0\n",
      "503   0.0\n",
      "449   1.0\n",
      "676   0.0\n",
      "92    1.0\n",
      "553   0.0\n",
      "887   1.0\n",
      "693   0.0\n",
      "561   0.0\n",
      "156   0.0\n",
      "120   0.0\n",
      "191   0.0\n",
      "670   0.0\n",
      "494   0.0\n",
      "624   0.0\n",
      "753   0.0\n",
      "416   0.0\n",
      "105   0.0\n",
      "163   0.0\n",
      "382   0.0\n",
      "206   0.0\n",
      "       ..\n",
      "437   0.0\n",
      "544   1.0\n",
      "558   1.0\n",
      "248   1.0\n",
      "118   1.0\n",
      "764   0.0\n",
      "204   0.0\n",
      "171   0.0\n",
      "389   0.0\n",
      "122   0.0\n",
      "104   0.0\n",
      "150   0.0\n",
      "569   0.0\n",
      "309   1.0\n",
      "373   1.0\n",
      "515   1.0\n",
      "559   0.0\n",
      "157   0.0\n",
      "405   0.0\n",
      "68    0.0\n",
      "701   1.0\n",
      "403   0.0\n",
      "532   0.0\n",
      "754   0.0\n",
      "52    1.0\n",
      "708   1.0\n",
      "24    0.0\n",
      "801   0.0\n",
      "847   0.0\n",
      "646   0.0\n",
      "Name: Class1, dtype: float64\n",
      "873     9.0\n",
      "630    30.0\n",
      "817    37.0\n",
      "60      7.2\n",
      "111    14.5\n",
      "789    79.2\n",
      "177    28.7\n",
      "572    26.4\n",
      "814     8.1\n",
      "244     7.2\n",
      "503     9.6\n",
      "449    30.5\n",
      "676     8.1\n",
      "92     61.2\n",
      "553     7.2\n",
      "887    30.0\n",
      "693     7.2\n",
      "561     7.9\n",
      "156     7.7\n",
      "120    73.5\n",
      "191    13.0\n",
      "670    39.0\n",
      "494     8.1\n",
      "624    16.1\n",
      "753     7.9\n",
      "416    32.5\n",
      "105     7.9\n",
      "163     8.7\n",
      "382     7.9\n",
      "206    15.8\n",
      "       ... \n",
      "437    18.8\n",
      "544   106.4\n",
      "558    79.7\n",
      "248    52.6\n",
      "118   247.5\n",
      "764     7.8\n",
      "204     8.1\n",
      "171    29.1\n",
      "389    12.0\n",
      "122    30.1\n",
      "104     7.9\n",
      "150    12.5\n",
      "569     7.9\n",
      "309    56.9\n",
      "373   135.6\n",
      "515    34.0\n",
      "559    17.4\n",
      "157     8.1\n",
      "405    21.0\n",
      "68      7.9\n",
      "701    26.3\n",
      "403    15.8\n",
      "532     7.2\n",
      "754    65.0\n",
      "52     76.7\n",
      "708   151.6\n",
      "24     21.1\n",
      "801    26.2\n",
      "847     7.9\n",
      "646     7.9\n",
      "Name: Fare, dtype: float64\n",
      "873   0.0\n",
      "630   0.0\n",
      "817   1.0\n",
      "60    1.0\n",
      "111   1.0\n",
      "789   1.0\n",
      "177   1.0\n",
      "572   0.0\n",
      "814   0.0\n",
      "244   1.0\n",
      "503   0.0\n",
      "449   0.0\n",
      "676   0.0\n",
      "92    0.0\n",
      "553   1.0\n",
      "887   0.0\n",
      "693   1.0\n",
      "561   0.0\n",
      "156   0.0\n",
      "120   0.0\n",
      "191   0.0\n",
      "670   0.0\n",
      "494   0.0\n",
      "624   0.0\n",
      "753   0.0\n",
      "416   0.0\n",
      "105   0.0\n",
      "163   0.0\n",
      "382   0.0\n",
      "206   0.0\n",
      "       ..\n",
      "437   0.0\n",
      "544   1.0\n",
      "558   0.0\n",
      "248   0.0\n",
      "118   1.0\n",
      "764   0.0\n",
      "204   0.0\n",
      "171   0.0\n",
      "389   1.0\n",
      "122   1.0\n",
      "104   0.0\n",
      "150   0.0\n",
      "569   0.0\n",
      "309   1.0\n",
      "373   1.0\n",
      "515   0.0\n",
      "559   0.0\n",
      "157   0.0\n",
      "405   0.0\n",
      "68    0.0\n",
      "701   0.0\n",
      "403   0.0\n",
      "532   1.0\n",
      "754   0.0\n",
      "52    1.0\n",
      "708   0.0\n",
      "24    0.0\n",
      "801   0.0\n",
      "847   1.0\n",
      "646   0.0\n",
      "Name: C, dtype: float64\n",
      "873   0.0\n",
      "630   0.0\n",
      "817   1.0\n",
      "60    0.0\n",
      "111   1.0\n",
      "789   0.0\n",
      "177   0.0\n",
      "572   0.0\n",
      "814   0.0\n",
      "244   0.0\n",
      "503   0.0\n",
      "449   0.0\n",
      "676   0.0\n",
      "92    1.0\n",
      "553   0.0\n",
      "887   0.0\n",
      "693   0.0\n",
      "561   0.0\n",
      "156   0.0\n",
      "120   0.0\n",
      "191   0.0\n",
      "670   1.0\n",
      "494   0.0\n",
      "624   0.0\n",
      "753   0.0\n",
      "416   1.0\n",
      "105   0.0\n",
      "163   0.0\n",
      "382   0.0\n",
      "206   1.0\n",
      "       ..\n",
      "437   0.0\n",
      "544   1.0\n",
      "558   1.0\n",
      "248   1.0\n",
      "118   0.0\n",
      "764   0.0\n",
      "204   0.0\n",
      "171   0.0\n",
      "389   0.0\n",
      "122   1.0\n",
      "104   0.0\n",
      "150   0.0\n",
      "569   0.0\n",
      "309   0.0\n",
      "373   0.0\n",
      "515   0.0\n",
      "559   1.0\n",
      "157   0.0\n",
      "405   1.0\n",
      "68    0.0\n",
      "701   0.0\n",
      "403   1.0\n",
      "532   1.0\n",
      "754   1.0\n",
      "52    1.0\n",
      "708   0.0\n",
      "24    0.0\n",
      "801   1.0\n",
      "847   0.0\n",
      "646   0.0\n",
      "Name: SS1, dtype: float64\n",
      "873   0.0\n",
      "630   0.0\n",
      "817   1.0\n",
      "60    0.0\n",
      "111   0.0\n",
      "789   0.0\n",
      "177   0.0\n",
      "572   0.0\n",
      "814   0.0\n",
      "244   0.0\n",
      "503   0.0\n",
      "449   0.0\n",
      "676   0.0\n",
      "92    0.0\n",
      "553   0.0\n",
      "887   0.0\n",
      "693   0.0\n",
      "561   0.0\n",
      "156   0.0\n",
      "120   0.0\n",
      "191   0.0\n",
      "670   1.0\n",
      "494   0.0\n",
      "624   0.0\n",
      "753   0.0\n",
      "416   1.0\n",
      "105   0.0\n",
      "163   0.0\n",
      "382   0.0\n",
      "206   0.0\n",
      "       ..\n",
      "437   0.0\n",
      "544   0.0\n",
      "558   1.0\n",
      "248   1.0\n",
      "118   1.0\n",
      "764   0.0\n",
      "204   0.0\n",
      "171   1.0\n",
      "389   0.0\n",
      "122   0.0\n",
      "104   0.0\n",
      "150   0.0\n",
      "569   0.0\n",
      "309   0.0\n",
      "373   0.0\n",
      "515   0.0\n",
      "559   0.0\n",
      "157   0.0\n",
      "405   0.0\n",
      "68    0.0\n",
      "701   0.0\n",
      "403   0.0\n",
      "532   1.0\n",
      "754   0.0\n",
      "52    0.0\n",
      "708   0.0\n",
      "24    1.0\n",
      "801   1.0\n",
      "847   0.0\n",
      "646   0.0\n",
      "Name: PC1, dtype: float64\n",
      "873   0.0\n",
      "630   0.0\n",
      "817   0.0\n",
      "60    0.0\n",
      "111   0.0\n",
      "789   0.0\n",
      "177   0.0\n",
      "572   0.0\n",
      "814   0.0\n",
      "244   0.0\n",
      "503   0.0\n",
      "449   0.0\n",
      "676   0.0\n",
      "92    0.0\n",
      "553   0.0\n",
      "887   0.0\n",
      "693   0.0\n",
      "561   0.0\n",
      "156   0.0\n",
      "120   0.0\n",
      "191   0.0\n",
      "670   0.0\n",
      "494   0.0\n",
      "624   0.0\n",
      "753   0.0\n",
      "416   0.0\n",
      "105   0.0\n",
      "163   0.0\n",
      "382   0.0\n",
      "206   0.0\n",
      "       ..\n",
      "437   0.0\n",
      "544   0.0\n",
      "558   0.0\n",
      "248   0.0\n",
      "118   0.0\n",
      "764   0.0\n",
      "204   0.0\n",
      "171   0.0\n",
      "389   0.0\n",
      "122   0.0\n",
      "104   0.0\n",
      "150   0.0\n",
      "569   0.0\n",
      "309   0.0\n",
      "373   0.0\n",
      "515   0.0\n",
      "559   0.0\n",
      "157   0.0\n",
      "405   0.0\n",
      "68    1.0\n",
      "701   0.0\n",
      "403   0.0\n",
      "532   0.0\n",
      "754   1.0\n",
      "52    0.0\n",
      "708   0.0\n",
      "24    0.0\n",
      "801   0.0\n",
      "847   0.0\n",
      "646   0.0\n",
      "Name: PC2, dtype: float64\n",
      "YESSS\n",
      "      female  Class1  Fare    C  SS1  PC1  PC2\n",
      "688    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "1       1.0     1.0  -0.7  1.0  1.0 -1.0 -1.0\n",
      "805    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "153    -1.0    -1.0  -0.9 -1.0 -1.0 -1.0  1.0\n",
      "178    -1.0    -1.0  -0.9 -1.0 -1.0 -1.0 -1.0\n",
      "623    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "125    -1.0    -1.0  -1.0  1.0  1.0 -1.0 -1.0\n",
      "681    -1.0     1.0  -0.7  1.0 -1.0 -1.0 -1.0\n",
      "871     1.0     1.0  -0.8 -1.0  1.0  1.0 -1.0\n",
      "187    -1.0     1.0  -0.9 -1.0 -1.0 -1.0 -1.0\n",
      "102    -1.0     1.0  -0.7 -1.0 -1.0  1.0 -1.0\n",
      "505    -1.0     1.0  -0.6  1.0  1.0 -1.0 -1.0\n",
      "302    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "556     1.0     1.0  -0.8  1.0  1.0 -1.0 -1.0\n",
      "480    -1.0    -1.0  -0.8 -1.0 -1.0 -1.0  1.0\n",
      "848    -1.0    -1.0  -0.9 -1.0 -1.0  1.0 -1.0\n",
      "287    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "132     1.0    -1.0  -0.9 -1.0  1.0 -1.0 -1.0\n",
      "767     1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "728    -1.0    -1.0  -0.9 -1.0  1.0 -1.0 -1.0\n",
      "197    -1.0    -1.0  -1.0 -1.0 -1.0  1.0 -1.0\n",
      "649     1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "167     1.0    -1.0  -0.9 -1.0  1.0 -1.0 -1.0\n",
      "207    -1.0    -1.0  -0.9  1.0 -1.0 -1.0 -1.0\n",
      "580     1.0    -1.0  -0.9 -1.0  1.0  1.0 -1.0\n",
      "603    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "717     1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "752    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "380     1.0     1.0  -0.1  1.0 -1.0 -1.0 -1.0\n",
      "372    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "..      ...     ...   ...  ...  ...  ...  ...\n",
      "437     1.0    -1.0  -0.9 -1.0 -1.0 -1.0 -1.0\n",
      "544    -1.0     1.0  -0.6  1.0  1.0 -1.0 -1.0\n",
      "558     1.0     1.0  -0.7 -1.0  1.0  1.0 -1.0\n",
      "248    -1.0     1.0  -0.8 -1.0  1.0  1.0 -1.0\n",
      "118    -1.0     1.0  -0.0  1.0 -1.0  1.0 -1.0\n",
      "764    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "204    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "171    -1.0    -1.0  -0.9 -1.0 -1.0  1.0 -1.0\n",
      "389     1.0    -1.0  -1.0  1.0 -1.0 -1.0 -1.0\n",
      "122    -1.0    -1.0  -0.9  1.0  1.0 -1.0 -1.0\n",
      "104    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "150    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "569    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "309     1.0     1.0  -0.8  1.0 -1.0 -1.0 -1.0\n",
      "373    -1.0     1.0  -0.5  1.0 -1.0 -1.0 -1.0\n",
      "515    -1.0     1.0  -0.9 -1.0 -1.0 -1.0 -1.0\n",
      "559     1.0    -1.0  -0.9 -1.0  1.0 -1.0 -1.0\n",
      "157    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "405    -1.0    -1.0  -0.9 -1.0  1.0 -1.0 -1.0\n",
      "68      1.0    -1.0  -1.0 -1.0 -1.0 -1.0  1.0\n",
      "701    -1.0     1.0  -0.9 -1.0 -1.0 -1.0 -1.0\n",
      "403    -1.0    -1.0  -0.9 -1.0  1.0 -1.0 -1.0\n",
      "532    -1.0    -1.0  -1.0  1.0  1.0  1.0 -1.0\n",
      "754     1.0    -1.0  -0.7 -1.0  1.0 -1.0  1.0\n",
      "52      1.0     1.0  -0.7  1.0  1.0 -1.0 -1.0\n",
      "708     1.0     1.0  -0.4 -1.0 -1.0 -1.0 -1.0\n",
      "24      1.0    -1.0  -0.9 -1.0 -1.0  1.0 -1.0\n",
      "801     1.0    -1.0  -0.9 -1.0  1.0  1.0 -1.0\n",
      "847    -1.0    -1.0  -1.0  1.0 -1.0 -1.0 -1.0\n",
      "646    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "\n",
      "[712 rows x 7 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6ba71573c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_linear_scale(examples_dataframe):#, features1):\n",
    "  \"\"\"Returns a version of the input `DataFrame` that has all its features normalized linearly.\"\"\"\n",
    "  #\n",
    "  # Your code here: normalize the inputs.\n",
    "  #\n",
    "    \n",
    "#   examples_dataframe = examples_dataframe[examples_dataframe[\"TotRmsAbvGrd\"] <= 13]\n",
    "#   examples_dataframe = examples_dataframe[examples_dataframe[\"OverallQual\"] <= 9.5]\n",
    "#   examples_dataframe = examples_dataframe[examples_dataframe[\"GrLivArea\"] <= 3500]\n",
    "\n",
    "#   examples_dataframe.dropna()\n",
    "\n",
    "  new = pd.DataFrame()\n",
    "  for i in examples_dataframe.columns:\n",
    "    print(examples_dataframe[i])\n",
    "    new = pd.concat([new, linear_scale(examples_dataframe[i])], axis=1)\n",
    "  \n",
    "  return new#examples_dataframe.apply(lambda x: linear_scale(x))\n",
    "\n",
    "# def find_na(df):\n",
    "#     l = []\n",
    "#     for label in df.columns:\n",
    "#         df.dropna()\n",
    "# #         if df[label].dropna().shape[0] != df.shape[0]:\n",
    "# #             l.append(label)\n",
    "#     return df\n",
    "\n",
    "correlation_df2 = preprocess_dataframe(titanic_dataframe, 1)#, features1)\n",
    "normalized_dataframe, features_index = preprocess_features(correlation_df2)\n",
    "\n",
    "# normalized_dataframe.dropna()\n",
    "# print(normalized_dataframe.isnull().sum())\n",
    "normalized_training_examples = normalize_linear_scale(normalized_dataframe.head(1000))\n",
    "new_training_targets = preprocess_targets(correlation_df2.head(1000))\n",
    "\n",
    "normalized_validation_examples = normalize_linear_scale(normalized_dataframe.tail(460))\n",
    "new_validation_targets = preprocess_targets(correlation_df2.head(460))\n",
    "\n",
    "print(\"YESSS\\n\", normalized_training_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "LogLoss (on training data):\n",
      "  period 00 : 0.49\n",
      "  period 01 : 0.47\n",
      "  period 02 : 0.46\n",
      "  period 03 : 0.46\n",
      "  period 04 : 0.46\n",
      "  period 05 : 0.46\n",
      "  period 06 : 0.45\n",
      "  period 07 : 0.45\n",
      "  period 08 : 0.45\n",
      "  period 09 : 0.45\n",
      "Model training finished.\n",
      "AUC on the validation set: 0.49\n",
      "Accuracy on the validation set: 0.52\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEbCAYAAACV0PCVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niYhIJCiQREQkEhRIIiISCQokERGJhP8Phcu00jWU/WsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6b72504c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# index1 = list(normalized_training_examples.index.values)\n",
    "# # print(indexs)\n",
    "# index2 = list(normalized_validation_examples.index.values)\n",
    "\n",
    "# print(normalized_training_examples)\n",
    "# print(normalized_validation_examples)\n",
    "\n",
    "linear_classifier, predict_validation_input_fn = train_linear_classifier_model(\n",
    "    learning_rate=0.0005,\n",
    "    steps=5000,#50000,\n",
    "    batch_size=75,\n",
    "    hidden_units=[20, 5],\n",
    "    training_examples=normalized_training_examples,\n",
    "    training_targets=new_training_targets,\n",
    "    validation_examples=normalized_validation_examples,\n",
    "    validation_targets=new_validation_targets)\n",
    "\n",
    "evaluation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\n",
    "\n",
    "print(\"AUC on the validation set: %0.2f\" % evaluation_metrics['auc'])\n",
    "print(\"Accuracy on the validation set: %0.2f\" % evaluation_metrics['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['female', 'Class1', 'Fare', 'C', 'SS1', 'PC1', 'PC2'], dtype='object')\n",
      "0     0.0\n",
      "1     1.0\n",
      "2     0.0\n",
      "3     0.0\n",
      "4     1.0\n",
      "5     0.0\n",
      "6     1.0\n",
      "7     0.0\n",
      "8     1.0\n",
      "9     0.0\n",
      "10    0.0\n",
      "11    0.0\n",
      "12    1.0\n",
      "13    0.0\n",
      "14    1.0\n",
      "15    1.0\n",
      "16    0.0\n",
      "17    0.0\n",
      "18    1.0\n",
      "19    1.0\n",
      "20    0.0\n",
      "21    0.0\n",
      "22    1.0\n",
      "23    0.0\n",
      "24    1.0\n",
      "25    0.0\n",
      "26    1.0\n",
      "27    0.0\n",
      "28    0.0\n",
      "29    0.0\n",
      "       ..\n",
      "388   0.0\n",
      "389   0.0\n",
      "390   0.0\n",
      "391   1.0\n",
      "392   0.0\n",
      "393   0.0\n",
      "394   0.0\n",
      "395   1.0\n",
      "396   0.0\n",
      "397   1.0\n",
      "398   0.0\n",
      "399   0.0\n",
      "400   1.0\n",
      "401   0.0\n",
      "402   1.0\n",
      "403   0.0\n",
      "404   0.0\n",
      "405   0.0\n",
      "406   0.0\n",
      "407   0.0\n",
      "408   1.0\n",
      "409   1.0\n",
      "410   1.0\n",
      "411   1.0\n",
      "412   1.0\n",
      "413   0.0\n",
      "414   1.0\n",
      "415   0.0\n",
      "416   0.0\n",
      "417   0.0\n",
      "Name: female, dtype: float64\n",
      "0     0.0\n",
      "1     0.0\n",
      "2     0.0\n",
      "3     0.0\n",
      "4     0.0\n",
      "5     0.0\n",
      "6     0.0\n",
      "7     0.0\n",
      "8     0.0\n",
      "9     0.0\n",
      "10    0.0\n",
      "11    1.0\n",
      "12    1.0\n",
      "13    0.0\n",
      "14    1.0\n",
      "15    0.0\n",
      "16    0.0\n",
      "17    0.0\n",
      "18    0.0\n",
      "19    0.0\n",
      "20    1.0\n",
      "21    0.0\n",
      "22    1.0\n",
      "23    1.0\n",
      "24    1.0\n",
      "25    0.0\n",
      "26    1.0\n",
      "27    0.0\n",
      "28    1.0\n",
      "29    0.0\n",
      "       ..\n",
      "388   0.0\n",
      "389   0.0\n",
      "390   1.0\n",
      "391   1.0\n",
      "392   0.0\n",
      "393   0.0\n",
      "394   0.0\n",
      "395   1.0\n",
      "396   0.0\n",
      "397   1.0\n",
      "398   0.0\n",
      "399   0.0\n",
      "400   1.0\n",
      "401   0.0\n",
      "402   1.0\n",
      "403   1.0\n",
      "404   1.0\n",
      "405   0.0\n",
      "406   0.0\n",
      "407   1.0\n",
      "408   0.0\n",
      "409   0.0\n",
      "410   0.0\n",
      "411   1.0\n",
      "412   0.0\n",
      "413   0.0\n",
      "414   1.0\n",
      "415   0.0\n",
      "416   0.0\n",
      "417   0.0\n",
      "Name: Class1, dtype: float64\n",
      "0       7.8\n",
      "1       7.0\n",
      "2       9.7\n",
      "3       8.7\n",
      "4      12.3\n",
      "5       9.2\n",
      "6       7.6\n",
      "7      29.0\n",
      "8       7.2\n",
      "9      24.1\n",
      "10      7.9\n",
      "11     26.0\n",
      "12     82.3\n",
      "13     26.0\n",
      "14     61.2\n",
      "15     27.7\n",
      "16     12.3\n",
      "17      7.2\n",
      "18      7.9\n",
      "19      7.2\n",
      "20     59.4\n",
      "21      3.2\n",
      "22     31.7\n",
      "23     61.4\n",
      "24    262.4\n",
      "25     14.5\n",
      "26     62.0\n",
      "27      7.2\n",
      "28     30.5\n",
      "29     21.7\n",
      "       ... \n",
      "388     7.8\n",
      "389    21.1\n",
      "390    93.5\n",
      "391    39.4\n",
      "392    20.2\n",
      "393    10.5\n",
      "394    22.0\n",
      "395    60.0\n",
      "396     7.2\n",
      "397    79.2\n",
      "398     7.8\n",
      "399     7.7\n",
      "400   164.9\n",
      "401    21.0\n",
      "402    59.4\n",
      "403    47.1\n",
      "404    27.7\n",
      "405    13.9\n",
      "406    10.5\n",
      "407   211.5\n",
      "408     7.7\n",
      "409    13.8\n",
      "410     7.8\n",
      "411    90.0\n",
      "412     7.8\n",
      "413     8.1\n",
      "414   108.9\n",
      "415     7.2\n",
      "416     8.1\n",
      "417    22.4\n",
      "Name: Fare, dtype: float64\n",
      "0     0.0\n",
      "1     0.0\n",
      "2     0.0\n",
      "3     0.0\n",
      "4     0.0\n",
      "5     0.0\n",
      "6     0.0\n",
      "7     0.0\n",
      "8     1.0\n",
      "9     0.0\n",
      "10    0.0\n",
      "11    0.0\n",
      "12    0.0\n",
      "13    0.0\n",
      "14    0.0\n",
      "15    1.0\n",
      "16    0.0\n",
      "17    1.0\n",
      "18    0.0\n",
      "19    1.0\n",
      "20    1.0\n",
      "21    0.0\n",
      "22    0.0\n",
      "23    1.0\n",
      "24    1.0\n",
      "25    0.0\n",
      "26    1.0\n",
      "27    1.0\n",
      "28    0.0\n",
      "29    1.0\n",
      "       ..\n",
      "388   0.0\n",
      "389   0.0\n",
      "390   0.0\n",
      "391   0.0\n",
      "392   0.0\n",
      "393   0.0\n",
      "394   0.0\n",
      "395   0.0\n",
      "396   0.0\n",
      "397   1.0\n",
      "398   0.0\n",
      "399   0.0\n",
      "400   0.0\n",
      "401   0.0\n",
      "402   1.0\n",
      "403   0.0\n",
      "404   1.0\n",
      "405   1.0\n",
      "406   0.0\n",
      "407   1.0\n",
      "408   0.0\n",
      "409   0.0\n",
      "410   0.0\n",
      "411   0.0\n",
      "412   0.0\n",
      "413   0.0\n",
      "414   1.0\n",
      "415   0.0\n",
      "416   0.0\n",
      "417   1.0\n",
      "Name: C, dtype: float64\n",
      "0     0.0\n",
      "1     1.0\n",
      "2     0.0\n",
      "3     0.0\n",
      "4     1.0\n",
      "5     0.0\n",
      "6     0.0\n",
      "7     1.0\n",
      "8     0.0\n",
      "9     0.0\n",
      "10    0.0\n",
      "11    0.0\n",
      "12    1.0\n",
      "13    1.0\n",
      "14    1.0\n",
      "15    1.0\n",
      "16    0.0\n",
      "17    0.0\n",
      "18    1.0\n",
      "19    0.0\n",
      "20    1.0\n",
      "21    0.0\n",
      "22    0.0\n",
      "23    0.0\n",
      "24    1.0\n",
      "25    1.0\n",
      "26    0.0\n",
      "27    0.0\n",
      "28    0.0\n",
      "29    0.0\n",
      "       ..\n",
      "388   0.0\n",
      "389   0.0\n",
      "390   0.0\n",
      "391   0.0\n",
      "392   0.0\n",
      "393   0.0\n",
      "394   0.0\n",
      "395   1.0\n",
      "396   0.0\n",
      "397   1.0\n",
      "398   0.0\n",
      "399   0.0\n",
      "400   0.0\n",
      "401   1.0\n",
      "402   0.0\n",
      "403   0.0\n",
      "404   1.0\n",
      "405   0.0\n",
      "406   1.0\n",
      "407   1.0\n",
      "408   0.0\n",
      "409   1.0\n",
      "410   0.0\n",
      "411   1.0\n",
      "412   0.0\n",
      "413   0.0\n",
      "414   0.0\n",
      "415   0.0\n",
      "416   0.0\n",
      "417   1.0\n",
      "Name: SS1, dtype: float64\n",
      "0     0.0\n",
      "1     0.0\n",
      "2     0.0\n",
      "3     0.0\n",
      "4     1.0\n",
      "5     0.0\n",
      "6     0.0\n",
      "7     1.0\n",
      "8     0.0\n",
      "9     0.0\n",
      "10    0.0\n",
      "11    0.0\n",
      "12    0.0\n",
      "13    0.0\n",
      "14    0.0\n",
      "15    0.0\n",
      "16    0.0\n",
      "17    0.0\n",
      "18    0.0\n",
      "19    0.0\n",
      "20    0.0\n",
      "21    1.0\n",
      "22    0.0\n",
      "23    1.0\n",
      "24    0.0\n",
      "25    0.0\n",
      "26    1.0\n",
      "27    0.0\n",
      "28    0.0\n",
      "29    0.0\n",
      "       ..\n",
      "388   0.0\n",
      "389   1.0\n",
      "390   0.0\n",
      "391   1.0\n",
      "392   0.0\n",
      "393   0.0\n",
      "394   1.0\n",
      "395   0.0\n",
      "396   0.0\n",
      "397   1.0\n",
      "398   0.0\n",
      "399   0.0\n",
      "400   0.0\n",
      "401   0.0\n",
      "402   1.0\n",
      "403   0.0\n",
      "404   0.0\n",
      "405   0.0\n",
      "406   0.0\n",
      "407   1.0\n",
      "408   0.0\n",
      "409   1.0\n",
      "410   0.0\n",
      "411   0.0\n",
      "412   0.0\n",
      "413   0.0\n",
      "414   0.0\n",
      "415   0.0\n",
      "416   0.0\n",
      "417   1.0\n",
      "Name: PC1, dtype: float64\n",
      "0     0.0\n",
      "1     0.0\n",
      "2     0.0\n",
      "3     0.0\n",
      "4     0.0\n",
      "5     0.0\n",
      "6     0.0\n",
      "7     0.0\n",
      "8     0.0\n",
      "9     0.0\n",
      "10    0.0\n",
      "11    0.0\n",
      "12    0.0\n",
      "13    0.0\n",
      "14    0.0\n",
      "15    0.0\n",
      "16    0.0\n",
      "17    0.0\n",
      "18    0.0\n",
      "19    0.0\n",
      "20    0.0\n",
      "21    0.0\n",
      "22    0.0\n",
      "23    0.0\n",
      "24    0.0\n",
      "25    0.0\n",
      "26    0.0\n",
      "27    0.0\n",
      "28    0.0\n",
      "29    0.0\n",
      "       ..\n",
      "388   0.0\n",
      "389   0.0\n",
      "390   0.0\n",
      "391   0.0\n",
      "392   1.0\n",
      "393   0.0\n",
      "394   0.0\n",
      "395   0.0\n",
      "396   0.0\n",
      "397   0.0\n",
      "398   0.0\n",
      "399   0.0\n",
      "400   0.0\n",
      "401   0.0\n",
      "402   0.0\n",
      "403   0.0\n",
      "404   0.0\n",
      "405   0.0\n",
      "406   0.0\n",
      "407   0.0\n",
      "408   0.0\n",
      "409   0.0\n",
      "410   0.0\n",
      "411   0.0\n",
      "412   0.0\n",
      "413   0.0\n",
      "414   0.0\n",
      "415   0.0\n",
      "416   0.0\n",
      "417   0.0\n",
      "Name: PC2, dtype: float64\n",
      "     female  Class1  Fare    C  SS1  PC1  PC2\n",
      "0      -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "1       1.0    -1.0  -1.0 -1.0  1.0 -1.0 -1.0\n",
      "2      -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "3      -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "4       1.0    -1.0  -1.0 -1.0  1.0  1.0 -1.0\n",
      "5      -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "6       1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "7      -1.0    -1.0  -0.9 -1.0  1.0  1.0 -1.0\n",
      "8       1.0    -1.0  -1.0  1.0 -1.0 -1.0 -1.0\n",
      "9      -1.0    -1.0  -0.9 -1.0 -1.0 -1.0 -1.0\n",
      "10     -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "11     -1.0     1.0  -0.9 -1.0 -1.0 -1.0 -1.0\n",
      "12      1.0     1.0  -0.7 -1.0  1.0 -1.0 -1.0\n",
      "13     -1.0    -1.0  -0.9 -1.0  1.0 -1.0 -1.0\n",
      "14      1.0     1.0  -0.8 -1.0  1.0 -1.0 -1.0\n",
      "15      1.0    -1.0  -0.9  1.0  1.0 -1.0 -1.0\n",
      "16     -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "17     -1.0    -1.0  -1.0  1.0 -1.0 -1.0 -1.0\n",
      "18      1.0    -1.0  -1.0 -1.0  1.0 -1.0 -1.0\n",
      "19      1.0    -1.0  -1.0  1.0 -1.0 -1.0 -1.0\n",
      "20     -1.0     1.0  -0.8  1.0  1.0 -1.0 -1.0\n",
      "21     -1.0    -1.0  -1.0 -1.0 -1.0  1.0 -1.0\n",
      "22      1.0     1.0  -0.9 -1.0 -1.0 -1.0 -1.0\n",
      "23     -1.0     1.0  -0.8  1.0 -1.0  1.0 -1.0\n",
      "24      1.0     1.0   0.0  1.0  1.0 -1.0 -1.0\n",
      "25     -1.0    -1.0  -0.9 -1.0  1.0 -1.0 -1.0\n",
      "26      1.0     1.0  -0.8  1.0 -1.0  1.0 -1.0\n",
      "27     -1.0    -1.0  -1.0  1.0 -1.0 -1.0 -1.0\n",
      "28     -1.0     1.0  -0.9 -1.0 -1.0 -1.0 -1.0\n",
      "29     -1.0    -1.0  -0.9  1.0 -1.0 -1.0 -1.0\n",
      "..      ...     ...   ...  ...  ...  ...  ...\n",
      "388    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "389    -1.0    -1.0  -0.9 -1.0 -1.0  1.0 -1.0\n",
      "390    -1.0     1.0  -0.6 -1.0 -1.0 -1.0 -1.0\n",
      "391     1.0     1.0  -0.8 -1.0 -1.0  1.0 -1.0\n",
      "392    -1.0    -1.0  -0.9 -1.0 -1.0 -1.0  1.0\n",
      "393    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "394    -1.0    -1.0  -0.9 -1.0 -1.0  1.0 -1.0\n",
      "395     1.0     1.0  -0.8 -1.0  1.0 -1.0 -1.0\n",
      "396    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "397     1.0     1.0  -0.7  1.0  1.0  1.0 -1.0\n",
      "398    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "399    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "400     1.0     1.0  -0.4 -1.0 -1.0 -1.0 -1.0\n",
      "401    -1.0    -1.0  -0.9 -1.0  1.0 -1.0 -1.0\n",
      "402     1.0     1.0  -0.8  1.0 -1.0  1.0 -1.0\n",
      "403    -1.0     1.0  -0.8 -1.0 -1.0 -1.0 -1.0\n",
      "404    -1.0     1.0  -0.9  1.0  1.0 -1.0 -1.0\n",
      "405    -1.0    -1.0  -0.9  1.0 -1.0 -1.0 -1.0\n",
      "406    -1.0    -1.0  -1.0 -1.0  1.0 -1.0 -1.0\n",
      "407    -1.0     1.0  -0.2  1.0  1.0  1.0 -1.0\n",
      "408     1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "409     1.0    -1.0  -0.9 -1.0  1.0  1.0 -1.0\n",
      "410     1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "411     1.0     1.0  -0.6 -1.0  1.0 -1.0 -1.0\n",
      "412     1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "413    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "414     1.0     1.0  -0.6  1.0 -1.0 -1.0 -1.0\n",
      "415    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "416    -1.0    -1.0  -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "417    -1.0    -1.0  -0.9  1.0  1.0  1.0 -1.0\n",
      "\n",
      "[418 rows x 7 columns]      Survived\n",
      "0           0\n",
      "1           1\n",
      "2           0\n",
      "3           0\n",
      "4           1\n",
      "5           0\n",
      "6           1\n",
      "7           0\n",
      "8           1\n",
      "9           0\n",
      "10          0\n",
      "11          0\n",
      "12          1\n",
      "13          0\n",
      "14          1\n",
      "15          1\n",
      "16          0\n",
      "17          0\n",
      "18          1\n",
      "19          1\n",
      "20          0\n",
      "21          0\n",
      "22          1\n",
      "23          0\n",
      "24          1\n",
      "25          0\n",
      "26          1\n",
      "27          0\n",
      "28          0\n",
      "29          0\n",
      "..        ...\n",
      "388         0\n",
      "389         0\n",
      "390         0\n",
      "391         1\n",
      "392         0\n",
      "393         0\n",
      "394         0\n",
      "395         1\n",
      "396         0\n",
      "397         1\n",
      "398         0\n",
      "399         0\n",
      "400         1\n",
      "401         0\n",
      "402         1\n",
      "403         0\n",
      "404         0\n",
      "405         0\n",
      "406         0\n",
      "407         0\n",
      "408         1\n",
      "409         1\n",
      "410         1\n",
      "411         1\n",
      "412         1\n",
      "413         0\n",
      "414         1\n",
      "415         0\n",
      "416         0\n",
      "417         0\n",
      "\n",
      "[418 rows x 1 columns]\n",
      "female    0\n",
      "Class1    0\n",
      "Fare      0\n",
      "C         0\n",
      "SS1       0\n",
      "PC1       0\n",
      "PC2       0\n",
      "dtype: int64\n",
      "Accuracy on test data: 0.95\n",
      "     Survived\n",
      "0           0\n",
      "1           0\n",
      "2           0\n",
      "3           0\n",
      "4           1\n",
      "5           0\n",
      "6           1\n",
      "7           0\n",
      "8           1\n",
      "9           0\n",
      "10          0\n",
      "11          0\n",
      "12          1\n",
      "13          0\n",
      "14          1\n",
      "15          1\n",
      "16          0\n",
      "17          0\n",
      "18          0\n",
      "19          1\n",
      "20          1\n",
      "21          0\n",
      "22          1\n",
      "23          0\n",
      "24          1\n",
      "25          0\n",
      "26          1\n",
      "27          0\n",
      "28          0\n",
      "29          0\n",
      "..        ...\n",
      "388         0\n",
      "389         0\n",
      "390         0\n",
      "391         1\n",
      "392         0\n",
      "393         0\n",
      "394         0\n",
      "395         1\n",
      "396         0\n",
      "397         1\n",
      "398         0\n",
      "399         0\n",
      "400         1\n",
      "401         0\n",
      "402         1\n",
      "403         0\n",
      "404         1\n",
      "405         0\n",
      "406         0\n",
      "407         0\n",
      "408         1\n",
      "409         1\n",
      "410         1\n",
      "411         1\n",
      "412         1\n",
      "413         0\n",
      "414         1\n",
      "415         0\n",
      "416         0\n",
      "417         0\n",
      "\n",
      "[418 rows x 1 columns]\n",
      "             Survived\n",
      "PassengerId          \n",
      "892                 0\n",
      "893                 0\n",
      "894                 0\n",
      "895                 0\n",
      "896                 1\n",
      "897                 0\n",
      "898                 1\n",
      "899                 0\n",
      "900                 1\n",
      "901                 0\n",
      "902                 0\n",
      "903                 0\n",
      "904                 1\n",
      "905                 0\n",
      "906                 1\n",
      "907                 1\n",
      "908                 0\n",
      "909                 0\n",
      "910                 0\n",
      "911                 1\n",
      "912                 1\n",
      "913                 0\n",
      "914                 1\n",
      "915                 0\n",
      "916                 1\n",
      "917                 0\n",
      "918                 1\n",
      "919                 0\n",
      "920                 0\n",
      "921                 0\n",
      "...               ...\n",
      "1280                0\n",
      "1281                0\n",
      "1282                0\n",
      "1283                1\n",
      "1284                0\n",
      "1285                0\n",
      "1286                0\n",
      "1287                1\n",
      "1288                0\n",
      "1289                1\n",
      "1290                0\n",
      "1291                0\n",
      "1292                1\n",
      "1293                0\n",
      "1294                1\n",
      "1295                0\n",
      "1296                1\n",
      "1297                0\n",
      "1298                0\n",
      "1299                0\n",
      "1300                1\n",
      "1301                1\n",
      "1302                1\n",
      "1303                1\n",
      "1304                1\n",
      "1305                0\n",
      "1306                1\n",
      "1307                0\n",
      "1308                0\n",
      "1309                0\n",
      "\n",
      "[418 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "titanic_test_data = pd.read_csv(\"titanic/test.csv\", sep=\",\")\n",
    "titanic_validation_data = pd.read_csv(\"titanic/gender_submission.csv\", sep=\",\")\n",
    "\n",
    "# inds = pd.isnull(titanic_test_data).any().notnull()#.nonzero()[0]\n",
    "# inds = np.where(titanic_test_data.notnull())\n",
    "\n",
    "# print(titanic_test_data.notnull().sum())\n",
    "# print(inds)\n",
    "# titanic_validation_data = titanic_validation_data.iloc[titanic_test_data.isnull().index()]\n",
    "# titanic_test_data = titanic_test_data.dropna()#iloc[inds[0]]\n",
    "\n",
    "# normalize_linear_scale(preprocess_features(titanic_dataframe))\n",
    "\n",
    "\n",
    "# titanic_validation_data = titanic_validation_data.iloc[inds[0]]\n",
    "# for i in titanic_test_data:\n",
    "    \n",
    "    \n",
    "# print(titanic_test_data.isnull().sum())\n",
    "\n",
    "new_titanic_test_data = pd.concat([titanic_test_data, titanic_validation_data], axis=1)\n",
    "# print(titanic_test_data)#.index.values\n",
    "\n",
    "\n",
    "# print(features_index[2:-1])\n",
    "########\n",
    "features_index_n = features_index[2:]\n",
    "print(features_index_n)\n",
    "# features_index = pd.concat([features_index, ['PassengerId']], axis=1)\n",
    "new_examples = normalize_linear_scale(preprocess_test_features(new_titanic_test_data, features_index_n, 'PassengerId'))#.dropna()\n",
    "new_targets = preprocess_targets(titanic_validation_data)#.dropna())\n",
    "\n",
    "# new_examples.set_index('PassengerId')\n",
    "# new_targets.set_index('PassengerId')\n",
    "print(new_examples, new_targets)\n",
    "\n",
    "# new_examples.fillna()\n",
    "# print(\"YESSSSSSSS\",new_examples.isnull().sum())\n",
    "# # ### Double-check\n",
    "# print(\"New examples summary:\")\n",
    "# display.display(new_examples.describe())\n",
    "# print(\"New Targets summary:\")\n",
    "# display.display(new_targets.describe())\n",
    "\n",
    "# new_examples.drop(new_examples.index[[2577-1461,2121-1461]], inplace=True)\n",
    "# new_targets.drop(new_targets.index[[2577-1461,2121-1461]], inplace=True)\n",
    "\n",
    "# new_examples = new_examples.fillna(0)\n",
    "new_examples = new_examples.fillna(new_examples.mean())\n",
    "print(new_examples.isnull().sum())\n",
    "# print(new_examples)\n",
    "# # new1.sort_values(ascending=True)\n",
    "\n",
    "# new_examples['Fare'].fillna(new_examples['Fare'].mean())\n",
    "# print(new_examples.isnull().sum())\n",
    "\n",
    "#############\n",
    "# predict_new_input_fn = lambda: my_input_fn(new_examples, new_targets[\"Survived\"], num_epochs=1, shuffle=False)\n",
    "\n",
    "# # new_predictions = linear_classifier.predict(input_fn=predict_new_input_fn)\n",
    "# # new_predictions = np.array([item['probabilities'][0] for item in new_predictions])\n",
    "\n",
    "\n",
    "# evaluation_metrics_2 = linear_classifier.evaluate(input_fn=predict_new_input_fn)\n",
    "\n",
    "# print(\"AUC on the validation set: %0.2f\" % evaluation_metrics_2['auc'])\n",
    "# print(\"Accuracy on the validation set: %0.2f\" % evaluation_metrics_2['accuracy'])\n",
    "\n",
    "# print(evaluation_metrics_2)\n",
    "\n",
    "##################\n",
    "\n",
    "predict_test_input_fn = lambda: my_input_fn(new_examples, new_targets[\"Survived\"], num_epochs=1, shuffle=False)\n",
    "\n",
    "test_predictions = linear_classifier.predict(input_fn=predict_test_input_fn)\n",
    "test_predictions = np.array([item['class_ids'][0] for item in test_predictions])\n",
    "  \n",
    "accuracy = metrics.accuracy_score(new_targets, test_predictions)\n",
    "print(\"Accuracy on test data: %0.2f\" % accuracy)\n",
    "# print(test_predictions)\n",
    "\n",
    "# #Obtain mean of columns as you need, nanmean is just convenient.\n",
    "# col_mean = np.nanmean(new_predictions, axis=0)\n",
    "# print(\"avg\",col_mean)\n",
    "\n",
    "\n",
    "# #Find indicies that you need to replace\n",
    "# inds = np.where(np.isnan(new_predictions))\n",
    "# print(inds)\n",
    "\n",
    "# #Place column means in the indices. Align the arrays using take\n",
    "# new_predictions[inds] = [col_mean]#np.take(col_mean, inds[1])\n",
    "# # # print(\"YES\")\n",
    "# print(new_predictions)\n",
    "# ##############################\n",
    "\n",
    "\n",
    "# # new_preditions=new_predictions[0:2577-1461] = 100\n",
    "\n",
    "# new_predictions.fillna(new_predictions.mean())\n",
    "\n",
    "# new_predictions = new_predictions[0:2121-1462]\n",
    "# new_targets = new_targets[0:2121-1462]# = 100\n",
    "# # new = pd.DataFrame()\n",
    "# # new['SalePrice'] = 0\n",
    "new = pd.DataFrame(test_predictions, columns=[\"Survived\"])#[1], index=new_predictions[0])\n",
    "print(new)\n",
    "# new.index.name = 'PassengerId'\n",
    "new_targets.index.name = 'PassengerId'\n",
    "new.rename({0:\"Survived\"})#, axis='columns')#[0].rename = 'SalePrice'\n",
    "# pred = new\n",
    "new.index += 892\n",
    "new.index.name = 'PassengerId'\n",
    "# new = pd.concat([new_targets, new], axis=1)\n",
    "# new[\"PassengerId\"] *= 1000\n",
    "new.to_csv('predictions_95.csv')\n",
    "# print(new.isnull().sum())\n",
    "print(new)\n",
    "\n",
    "\n",
    "\n",
    "# new_root_mean_squared_error = math.sqrt(metrics.mean_squared_error(pred, new_targets))\n",
    "# # female  Class1  Fare    C  SS1  PC1  PC2\n",
    "# print(\"Final RMSE (on test data): %0.2f\" % new_root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluation_metrics = linear_classifier.evaluate(input_fn=predict_new_input_fn)\n",
    "\n",
    "print(\"AUC on the validation set: %0.2f\" % evaluation_metrics['auc'])\n",
    "print(\"Accuracy on the validation set: %0.2f\" % evaluation_metrics['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.scatter(training_examples[\"OverallQual\"], training_targets[\"SalePrice\"])\n",
    "# plt.scatter(normalized_training_examples[\"OverallQual\"], training_targets[\"SalePrice\"])\n",
    "plt.scatter(normalized_training_examples[\"GrLivArea\"], training_targets[\"SalePrice\"])\n",
    "# plt.scatter(training_examples[\"TotalBsmtSF\"], training_targets[\"SalePrice\"])\n",
    "# plt.scatter(normalized_training_examples[\"GarageArea\"], training_targets[\"SalePrice\"])\n",
    "# plt.scatter(training_examples[\"GarageYrBlt\"], training_targets[\"SalePrice\"])\n",
    "# training_examples[\"OverallQual\"].hist()\n",
    "# training_examples[\"GrLivArea\"].hist()\n",
    "# training_examples[\"TotalBsmtSF\"].hist()\n",
    "# training_examples[\"GarageArea\"].hist()\n",
    "# training_examples[\"GarageYrBlt\"]#.hist()\n",
    "\n",
    "_ = normalized_training_examples.hist(bins=20, figsize=(18, 12), xlabelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(new_targets)\n",
    "# print(new)\n",
    "# plt.scatter(new_targets.index, new_targets[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.scatter(new.index, new[\"SalePrice\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
