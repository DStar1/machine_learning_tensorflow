{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "1046      0       0       0       0       0       0       0       0       0   \n",
       "2090      4       0       0       0       0       0       0       0       0   \n",
       "984       9       0       0       0       0       0       0       0       0   \n",
       "1379      2       0       0       0       0       0       0       0       0   \n",
       "1650      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "1046       0    ...            0         0         0         0         0   \n",
       "2090       0    ...            0         0         0         0         0   \n",
       "984        0    ...            0         0         0         0         0   \n",
       "1379       0    ...            0         0         0         0         0   \n",
       "1650       0    ...            0         0         0         0         0   \n",
       "\n",
       "      pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "1046         0         0         0         0         0  \n",
       "2090         0         0         0         0         0  \n",
       "984          0         0         0         0         0  \n",
       "1379         0         0         0         0         0  \n",
       "1650         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "%matplotlib inline\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "\n",
    "mnist_dataframe = pd.read_csv(\"data/train.csv\", sep=\",\")\n",
    "\n",
    "# Use just the first 10,000 records for training/validation.\n",
    "mnist_dataframe = mnist_dataframe.head(10000)\n",
    "\n",
    "mnist_dataframe = mnist_dataframe.reindex(np.random.permutation(mnist_dataframe.index))\n",
    "mnist_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel71</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5514</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6576</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel71\n",
       "1046        0\n",
       "2090        0\n",
       "984         0\n",
       "1379        0\n",
       "1650        0\n",
       "...       ...\n",
       "4036        0\n",
       "4976        0\n",
       "5514        0\n",
       "6576        0\n",
       "3378        0\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataframe.iloc[:, 72:73]#.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_labels_and_features(dataset):\n",
    "  \"\"\"Extracts labels and features.\n",
    "  \n",
    "  This is a good place to scale or transform the features if needed.\n",
    "  \n",
    "  Args:\n",
    "    dataset: A Pandas `Dataframe`, containing the label on the first column and\n",
    "      monochrome pixel values on the remaining columns, in row major order.\n",
    "  Returns:\n",
    "    A `tuple` `(labels, features)`:\n",
    "      labels: A Pandas `Series`.\n",
    "      features: A Pandas `DataFrame`.\n",
    "  \"\"\"\n",
    "  labels = dataset['label']\n",
    "\n",
    "  # DataFrame.loc index ranges are inclusive at both ends.\n",
    "  features = dataset.iloc[:,1:785]\n",
    "  # Scale the data to [0, 1] by dividing out the max value, 255.\n",
    "#   features = features // 255\n",
    "  features = features.applymap(lambda x: x / 255)\n",
    "\n",
    "  return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_test_labels_and_features(labels1, dataset):\n",
    "  \"\"\"Extracts labels and features.\n",
    "  \n",
    "  This is a good place to scale or transform the features if needed.\n",
    "  \n",
    "  Args:\n",
    "    dataset: A Pandas `Dataframe`, containing the label on the first column and\n",
    "      monochrome pixel values on the remaining columns, in row major order.\n",
    "  Returns:\n",
    "    A `tuple` `(labels, features)`:\n",
    "      labels: A Pandas `Series`.\n",
    "      features: A Pandas `DataFrame`.\n",
    "  \"\"\"\n",
    "  labels = labels1['Label']\n",
    "\n",
    "  # DataFrame.loc index ranges are inclusive at both ends.\n",
    "  features = dataset.copy()\n",
    "  # Scale the data to [0, 1] by dividing out the max value, 255.\n",
    "#   features = features // 255\n",
    "#   features = features.applymap(lambda x: x / 255)\n",
    "\n",
    "  return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8383</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5586</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7500 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "1046     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2090     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "984      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1379     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1650     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "863      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "8383     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "5586     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3125     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "991      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      pixel9    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "1046     0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "2090     0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "984      0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "1379     0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "1650     0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "...      ...    ...          ...       ...       ...       ...       ...   \n",
       "863      0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "8383     0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "5586     0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "3125     0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "991      0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "      pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "1046       0.0       0.0       0.0       0.0       0.0  \n",
       "2090       0.0       0.0       0.0       0.0       0.0  \n",
       "984        0.0       0.0       0.0       0.0       0.0  \n",
       "1379       0.0       0.0       0.0       0.0       0.0  \n",
       "1650       0.0       0.0       0.0       0.0       0.0  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "863        0.0       0.0       0.0       0.0       0.0  \n",
       "8383       0.0       0.0       0.0       0.0       0.0  \n",
       "5586       0.0       0.0       0.0       0.0       0.0  \n",
       "3125       0.0       0.0       0.0       0.0       0.0  \n",
       "991        0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[7500 rows x 784 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_targets, training_examples = parse_labels_and_features(mnist_dataframe[:7500])\n",
    "training_examples#.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "count  2500.0  2500.0  2500.0  2500.0  2500.0  2500.0  2500.0  2500.0  2500.0   \n",
       "mean      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "std       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "min       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "50%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "75%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "max       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       pixel9    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "count  2500.0    ...       2500.0    2500.0    2500.0    2500.0    2500.0   \n",
       "mean      0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "std       0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "min       0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "25%       0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "50%       0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "75%       0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "max       0.0    ...          0.2       1.0       1.0       0.7       0.1   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "count    2500.0    2500.0    2500.0    2500.0    2500.0  \n",
       "mean        0.0       0.0       0.0       0.0       0.0  \n",
       "std         0.0       0.0       0.0       0.0       0.0  \n",
       "min         0.0       0.0       0.0       0.0       0.0  \n",
       "25%         0.0       0.0       0.0       0.0       0.0  \n",
       "50%         0.0       0.0       0.0       0.0       0.0  \n",
       "75%         0.0       0.0       0.0       0.0       0.0  \n",
       "max         0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[8 rows x 784 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_targets, validation_examples = parse_labels_and_features(mnist_dataframe[7500:10000])\n",
    "validation_examples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAEHCAYAAACOfPs0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEBlJREFUeJzt3XuwXWV9xvHvw60IsRBuiSWYI1OBTi2TscLQiVPiDWjV\nJkJBCjiglNIRkFE7FrGdHBSmajupWMcZJ1yGi4wgNgSKxUBp0oIVEUgNEC6dcqJgchIgQtNCG8iv\nf+wV2Dmc8659zr7m/J7PzJ7srHfttX57Jc9511rvWusoIjCzXHbpdwFm1nsOvllCDr5ZQg6+WUIO\nvllCDr5ZQg5+YpL+WdInev1Z6z8HfxqQ9JSk9/a7jhJJb5N0m6QXJW2U9OV+15SZg29dJ2l34E7g\nLuAgYA5wfV+LSs7Bn8Yk7Vv1shslPVe9P3jMbL8u6T5JL0haJmnfps8fI+leSZslPSTp2CmWchbw\nTERcHhEvR8T/RcTDU/1e1j4Hf3rbBbgKOAR4K/A/wDfGzPMxGsGcDbwK/B1A9QPiH4AvRsRM4M+A\n70naf+xKJB0i6XlJcyao4xhgnaTvS9ok6W5J72j729mUOfjTWEQ8HxHLIuJ/I+K/gb8CfnfMbNdF\nxNqIeAn4S+BkSQJOB26PiB9Uy/on4CfA74+znp9HxH4R8fQEpcwBPgp8DXgL8H1guaTdOvA1bQoc\n/GlM0pskfUvSiKRfAquAfatgb/fzpvfrgN2BA4C5wClVT/68pM3AfBp7BpP1EnBPRKyIiFci4m+A\n/YHfmMr3svY5+NPbZ4G3A0dFxL683ts3B/+Qpvdzga3AszR+IFxb9eT7RcTMiHhzRPz1FOr4KeDb\nQAeIgz997CHpV5peuwJvptHbvihpP2B4nM+dIekISXsBlwDfjca92tcDH5Z0nKRdJO0p6VhJvzaF\n2q4HjpH03mpZnwY2AWun8kWtfQ7+9HE7jZN3L1V/Lgb+FtiLRg/+QxrH1s0CuA64BvgFsAdwIUB1\nvL4QuJhGSNfROMG3S9NngddO7r040cm9iHgCOAP4FvA88GHgDyLilba+sU2Z/CAOs3zc45sl1LPg\nSzpB0mOSnpD0571ab6uqM9//Xl2o8uMBqOdKSaOSfto0baakFZIel/QDSfsMWH2LJT0t6cHqdUIf\n65tTXS/wiKQ1kj5VTR+IbThOfRdU03uyDXuyqy9pF+AJ4H00jiXvB06NiMe6vvIWSfpP4LcjYnO/\nawGQ9G5gC40z60dW074CPBcRX61+eM6MiIsGqL7FwH9FxJJ+1NRM0mxgdkSsljQDeIDGOYuPMwDb\nsFDfR+nBNuxVj3808GRErIuIrcB3aHzJQSIG6NAnIu4Bxv4QWkjjRBzVn4t6WlSTCeqDHYcK+yYi\nNkTE6ur9FhojCHMYkG04QX3bL6fu+jbs1X/0g9nxQpGnef1LDooA7pR0v6Rz+l3MBA6KiFFo/Meh\nccPLoDlf0mpJV/TzUKSZpCFgHvAjYNagbcOm+u6rJnV9Gw5MDzcA5kfEO2lcknpetSs76AZtSOab\nwKERMQ/YAAzCLv8M4GbgwqpnHbvN+roNx6mvJ9uwV8F/hsZNItvNqaYNjIhYX/25CVhG4/Bk0IxK\nmgWvHSNu7HM9O4iITfH6SaOlwFH9rKe6F+BmGvcjLK8mD8w2HK++Xm3DXgX/fhq3f86VtAdwKnBr\nj9ZdS9Je1U9eJO0NHAcMwm2jYsfjvVtp3EkHcCawfOwHemyH+qogbXci/d+GVwGPRsTlTdMGaRu+\nob5ebcOeXcBTDUtcTuOHzZURMTBPYJH0Nhq9fAC7Ad/ud32SbgAW0LiZZZTGlXi3AN+lcX39OuCU\niPjlANX3HhrHqtuAEeDc7cfTfahvPvAvwBoa/65B4yrEHwM30edtWKjvNHqwDX3lnllCPrlnlpCD\nb5aQg2+WkINvllBbwR/0G2/MbHxTPqvf6o03kjxsYNYnETHudf/tPOX0tRtvACRtv/FmnDvuFje9\nX0lj+HdQrcT1tWMlg1vfSga3Nuh8fZdM2NLOrv7OcOONmY3DJ/fMEmpnV38SN96sbHq/Zxur7IWh\nfhdQY6jfBdQY6ncBBUP9LqDGUJufH6le9do5ubcr8DiNk3vraVwD/UcRsXbMfLHjMb6Z9cYlnT+5\nFxGvSjofWMHrN974OelmO4G2fndZRNwBHN6hWsysR3xyzywhB98sIQffLCEH3ywhB98sIQffLCEH\n3ywhB98sIQffLCEH3ywhB98sIQffLCEH3ywhB98sIQffLKG27sc3677di63x0F8U29fNO7DYPqRP\nTrqi6cA9vllCDr5ZQg6+WUIOvllCDr5ZQg6+WUIOvllCHse3gXZ27FNsXzLu74l53WeP2VizhuFJ\n1TNduMc3S8jBN0vIwTdLyME3S8jBN0vIwTdLyME3S6itcXxJI8ALwDZga0Qc3YmiLI93xAeL7Uv3\nL/+Xmvny+vIK9hyeZEU5tHsBzzZgQURs7kQxZtYb7e7qqwPLMLMeaze0Adwp6X5J53SiIDPrvnZ3\n9edHxHpJB9L4AbA2Iu5542wrm94PVS8z66yR6lWvreBHxPrqz02SlgFHA+MEf0E7qzGzlgyxY6e6\nasI5p7yrL2kvSTOq93sDxwEPT3V5ZtY77fT4s4BlkqJazrcjYkVnyjKzbppy8CPiKWBeB2uxaelz\nxda/197Fdj1RXvoLe94x2YIMD8WZpeTgmyXk4Jsl5OCbJeTgmyXk4Jsl5OCbJeTn6lt7FgwXm2Nd\nzYPvTy4367Dragr4j5p2G497fLOEHHyzhBx8s4QcfLOEHHyzhBx8s4QcfLOEPI5vNb5QbH32X8vj\n9E+/Wl76IV99pGb9N9W021S4xzdLyME3S8jBN0vIwTdLyME3S8jBN0vIwTdLyOP42c0ZLjbHtvI4\n/VO/KC/+0POjPMM3yuu37nCPb5aQg2+WkINvlpCDb5aQg2+WkINvlpCDb5ZQ7Ti+pCuBDwGjEXFk\nNW0mcCMwFxgBTomIF7pYp01Z+X76OKo8Tv/ksvLSD/tTj9PvjFrp8a8Gjh8z7SLgrog4HLgb+Hyn\nCzOz7qkNfkTcA2weM3khcE31/hpgUYfrMrMumuox/kERMQoQERuAgzpXkpl1W6eu1a850FvZ9H6o\neplZZ41Ur3pTDf6opFkRMSppNrCxPPuCKa7GzFo3xI6d6qoJ52x1V1/Va7tbgbOq92cCy1stzcz6\nrzb4km4AfggcJulnkj4OfBn4gKTHgfdVfzeznUTtrn5EnDZB0/s7XItNSfmfIT6yR7F9ac04/WWx\ntjyDhsvtNpB85Z5ZQg6+WUIOvllCDr5ZQg6+WUIOvllCDr5ZQn6u/k5ubhxQbL+sfLs9X1hTbv8T\nfWeSFXXab5Wbjzip3P7Y12uW//ykqpku3OObJeTgmyXk4Jsl5OCbJeTgmyXk4Jsl5OCbJeRx/IE3\nXGwdOb5moP7t5WZ9oOZxiTXrr1czDv+T8jh8nFz+fpc9Vl78Jc+Wf93D1gOWlBcwTbnHN0vIwTdL\nyME3S8jBN0vIwTdLyME3S8jBN0vI4/h9N1RsjSfK49jfO6y89D+8pWacftFwub1N+71yTLH96t1q\nrkP4SM3ynyq3b730V8szJOUe3ywhB98sIQffLCEH3ywhB98sIQffLCEH3yyh2nF8SVcCHwJGI+LI\natpi4BxgYzXbxRFxR9eqnNZmlZvfXW4+6YiaxS96cVLVTF75fvvn3jqn2L48jiu2n8WiYvsF+mSx\nna/dV25PqpUe/2rg+HGmL4mId1Yvh95sJ1Ib/Ii4B9g8TlPNJVdmNqjaOcY/X9JqSVdI2qdjFZlZ\n1031Wv1vAl+MiJB0KbAEOHvi2Vc2vR+i7vp0M5uKkepVb0rBj4hNTX9dCtxW/sSCqazGzCZliB07\n1VUTztnqrr5oOqaXNLup7UTg4ZZrM7O+a2U47wYaXfb+kn4GLAbeI2kesI3GvsW5XazRzDqsNvgR\ncdo4k6/uQi0pnRTPFdtvrBk7OfXImvvtHxueXEFj3BL/VmxfeNQlxfbl95fH6XfllWJ73Tj9u2bX\nfP8Nw+X2pHzlnllCDr5ZQg6+WUIOvllCDr5ZQg6+WUIOvllCfq5+15Xvt7/5nI+VPz6v3HzqXW3e\nb3/GcLF5f5UvJHh5S3nxC1esKLY/MN4N300WbNlYnmHGcLndxuUe3ywhB98sIQffLCEH3ywhB98s\nIQffLCEH3ywhj+N33ZvKzU/WfPzYmvbVa4vNX4ryQPuRNeP0D9asfnRGuf2k88rt7zqj5n56j9N3\nhXt8s4QcfLOEHHyzhBx8s4QcfLOEHHyzhBx8s4Q8jt91W4ut61YeWGy/WpuK7Ss4pti+ts3fafyp\n28vtWl8zDv/H19esYXgy5ViHuMc3S8jBN0vIwTdLyME3S8jBN0vIwTdLyME3S6h2HF/SHOBaGg+I\n3wYsjYivS5oJ3AjMBUaAUyLihS7WupN6ptg6pGuK7b8XL5UXr5OKzWeWP82hp9aMw39wuGYJde02\niFrp8V8BPhMRvwn8DnCepCOAi4C7IuJw4G7g890r08w6qTb4EbEhIlZX77cAa4E5wEJge3d1DbCo\nW0WaWWdN6hhf0hCNX+r0I2BWRIxC44cDcFCnizOz7mj5Wn1JM4CbgQsjYouksQeHhYPFlU3vh6qX\nmXXWSPWq11LwJe1GI/TXRcTyavKopFkRMSppNlD47YYLWirGzNoxxI6d6qoJ52x1V/8q4NGIuLxp\n2q3AWdX7M4HlYz9kZoOpleG8+cDpwBpJD9HYpb8Y+Apwk6RPAOuAU7pZqJl1Tm3wI+JeYNcJmt/f\n2XIyuq/Y+o/6XLH96JqlfzpuKM+g4Zol2HTkK/fMEnLwzRJy8M0ScvDNEnLwzRJy8M0ScvDNEvJz\n9Xdyu9e0X/tq+Y78fbi4c8XYTsM9vllCDr5ZQg6+WUIOvllCDr5ZQg6+WUIOvllCiqh5rnq7K5AC\nFnd1HWY2nkuICI3X4h7fLCEH3ywhB98sIQffLCEH3ywhB98sIQffLCEH3ywhB98sIQffLCEH3ywh\nB98sIQffLCEH3ywhB98sodrgS5oj6W5Jj0haI+mCavpiSU9LerB6ndD9cs2sE1r5hRqvAJ+JiNWS\nZgAPSLqzalsSEUu6V56ZdUNt8CNiA7Cher9F0lrg4Kp53Kd7mNlgm9QxvqQhYB5wXzXpfEmrJV0h\naZ8O12ZmXdLyM/eq3fyVwJciYrmkA4FnIyIkXQq8JSLOHudzAcc2TRmqXmbWWSPVa7tVEz5zr6Vf\nmilpN+Bm4LqIWA4QEZuaZlkK3DbxEha0shoza8sQO3aqqyacs9Vd/auARyPi8u0TJM1uaj8ReLjl\n+sysr2p7fEnzgdOBNZIeAgK4GDhN0jxgG439i3O7WKeZdVArZ/XvBXYdp+mOzpdjZr3gK/fMEnLw\nzRJy8M0ScvDNEnLwzRJy8M0ScvDNEnLwzRJy8M0ScvDNEnLwzRLqQ/BHer/KSRnpdwE1RvpdQI2R\nfhdQMNLvAmqM9GxNDv4bjPS7gBoj/S6gxki/CygY6XcBNUZ6tibv6psl5OCbJdTyM/emvAKpuysw\nswlN9My9rgffzAaPd/XNEnLwzRJy8M0ScvDNEnLwzRL6f0/FOE5ic39fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a76f97710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_example = np.random.choice(training_examples.index)\n",
    "_, ax = plt.subplots()\n",
    "# ax.imshow(training_examples.loc[rand_example].values.reshape(28, 28))\n",
    "ax.matshow(training_examples.loc[rand_example].values.reshape(28, 28))\n",
    "ax.set_title(\"Label: %i\" % training_targets.loc[rand_example])\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_feature_columns():\n",
    "  \"\"\"Construct the TensorFlow Feature Columns.\n",
    "\n",
    "  Returns:\n",
    "    A set of feature columns\n",
    "  \"\"\" \n",
    "  \n",
    "  # There are 784 pixels in each image.\n",
    "  return set([tf.feature_column.numeric_column('pixels', shape=784)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_training_input_fn(features, labels, batch_size, num_epochs=None, shuffle=True):\n",
    "  \"\"\"A custom input_fn for sending MNIST data to the estimator for training.\n",
    "\n",
    "  Args:\n",
    "    features: The training features.\n",
    "    labels: The training labels.\n",
    "    batch_size: Batch size to use during training.\n",
    "\n",
    "  Returns:\n",
    "    A function that returns batches of training features and labels during\n",
    "    training.\n",
    "  \"\"\"\n",
    "  def _input_fn(num_epochs=None, shuffle=True):\n",
    "        \n",
    "#     tf.estimator.inputs.numpy_input_fn(               x={\"x\": train_data},\n",
    "#                                                       y=train_labels,\n",
    "#                                                       batch_size=batch_\n",
    "        \n",
    "        \n",
    "    # Input pipelines are reset with each call to .train(). To ensure model\n",
    "    # gets a good sampling of data, even when number of steps is small, we \n",
    "    # shuffle all the data before creating the Dataset object\n",
    "    idx = np.random.permutation(features.index)\n",
    "    raw_features = {\"pixels\":features.reindex(idx)}\n",
    "    raw_targets = np.array(labels[idx])\n",
    "   \n",
    "    ds = Dataset.from_tensor_slices((raw_features,raw_targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "    if shuffle:\n",
    "      ds = ds.shuffle(10000)\n",
    "    \n",
    "    # Return the next batch of data.\n",
    "    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "    return feature_batch, label_batch\n",
    "\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_predict_input_fn(features, labels, batch_size):\n",
    "  \"\"\"A custom input_fn for sending mnist data to the estimator for predictions.\n",
    "\n",
    "  Args:\n",
    "    features: The features to base predictions on.\n",
    "    labels: The labels of the prediction examples.\n",
    "\n",
    "  Returns:\n",
    "    A function that returns features and labels for predictions.\n",
    "  \"\"\"\n",
    "  def _input_fn():\n",
    "    raw_features = {\"pixels\": features.values}\n",
    "    raw_targets = np.array(labels)\n",
    "    \n",
    "    ds = Dataset.from_tensor_slices((raw_features, raw_targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size)\n",
    "    \n",
    "        \n",
    "    # Return the next batch of data.\n",
    "    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "    return feature_batch, label_batch\n",
    "\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "  # MNIST images are 28x28 pixels, and have one color channel\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "  # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Pooling Layer #1\n",
    "  # First max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "  # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2\n",
    "  # Computes 64 features using a 5x5 filter.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "  # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Pooling Layer #2\n",
    "  # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "  # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Flatten tensor into a batch of vectors\n",
    "  # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "  # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "  # Dense Layer\n",
    "  # Densely connected layer with 1024 neurons\n",
    "  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "  # Output Tensor Shape: [batch_size, 1024]\n",
    "  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "  # Add dropout operation; 0.6 probability that element will be kept\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits layer\n",
    "  # Input Tensor Shape: [batch_size, 1024]\n",
    "  # Output Tensor Shape: [batch_size, 10]\n",
    "  logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_cnn_classification_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "  \"\"\"Trains a neural network classification model for the MNIST digits dataset.\n",
    "  \n",
    "  In addition to training, this function also prints training progress information,\n",
    "  a plot of the training and validation loss over time, as well as a confusion\n",
    "  matrix.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: An `int`, the learning rate to use.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n",
    "    training_examples: A `DataFrame` containing the training features.\n",
    "    training_targets: A `DataFrame` containing the training labels.\n",
    "    validation_examples: A `DataFrame` containing the validation features.\n",
    "    validation_targets: A `DataFrame` containing the validation labels.\n",
    "      \n",
    "  Returns:\n",
    "    The trained `CNNClassifier` object.\n",
    "  \"\"\"\n",
    "\n",
    "#   periods = 10\n",
    "#   # Caution: input pipelines are reset with each call to train. \n",
    "#   # If the number of steps is small, your model may never see most of the data.  \n",
    "#   # So with multiple `.train` calls like this you may want to control the length \n",
    "#   # of training with num_epochs passed to the input_fn. Or, you can do a really-big shuffle, \n",
    "#   # or since it's in-memory data, shuffle all the data in the `input_fn`.\n",
    "#   steps_per_period = steps / periods  \n",
    "  \n",
    "#   # Create the input functions.\n",
    "#   predict_training_input_fn = create_predict_input_fn(\n",
    "#     training_examples, training_targets, batch_size)\n",
    "#   predict_validation_input_fn = create_predict_input_fn(\n",
    "#     validation_examples, validation_targets, batch_size)\n",
    "#   training_input_fn = create_training_input_fn(\n",
    "#     training_examples, training_targets, batch_size)\n",
    "\n",
    "  # Create feature columns.\n",
    "#   feature_columns = [tf.feature_column.numeric_column('pixels', shape=784)]\n",
    "\n",
    "#   # Create a CNNClassifier object.\n",
    "#   my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "#   my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "\n",
    "  # Create the Estimator\n",
    "  classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\n",
    "\n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print(\"Training model...\")\n",
    "  print(\"LogLoss error (on validation data):\")\n",
    "# #   training_errors = []\n",
    "# #   validation_errors = []\n",
    "#   for period in range (0, periods):\n",
    "#     # Train the model, starting from the prior state.\n",
    "\n",
    "  # Train the model\n",
    "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": training_examples},\n",
    "      y=training_targets,\n",
    "      batch_size=100,\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "  classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=5#,#000,#20000,\n",
    "    #   hooks=[logging_hook]\n",
    "    )\n",
    "\n",
    "#     # Evaluate the model and print results\n",
    "#   eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#     x={\"x\": validation_examples},\n",
    "#     y=validation_targets,\n",
    "#     num_epochs=1,\n",
    "#     shuffle=False)\n",
    "#   eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "#   print(eval_results)    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     classifier.train(\n",
    "#       input_fn=training_input_fn,\n",
    "#       steps=steps_per_period#,5000\n",
    "#     )\n",
    "#     eval_results = classifier.evaluate(input_fn=predict_training_input_fn)\n",
    "#     print(eval_results)\n",
    "  \n",
    "#     # Take a break and compute probabilities.\n",
    "#     training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
    "#     training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
    "#     training_pred_class_id = np.array([item['classes'][0] for item in training_predictions])\n",
    "#     training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
    "       \n",
    "#     validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
    "#     validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
    "#     validation_pred_class_id = np.array([item['classes'][0] for item in validation_predictions])\n",
    "#     validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n",
    "    \n",
    "#     # Compute training and validation errors.\n",
    "#     training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
    "#     validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
    "#     # Occasionally print the current loss.\n",
    "#     print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
    "#     # Add the loss metrics from this period to our list.\n",
    "#     training_errors.append(training_log_loss)\n",
    "#     validation_errors.append(validation_log_loss)\n",
    "  print(\"Model training finished.\")\n",
    "#   # Remove event files to save disk space.\n",
    "#   _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
    "  \n",
    "#   # Calculate final predictions (not probabilities, as above).\n",
    "#   final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
    "#   final_predictions = np.array([item['classes'][0] for item in final_predictions])\n",
    "  \n",
    "  \n",
    "#   accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
    "#   print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
    "\n",
    "#   # Output a graph of loss metrics over periods.\n",
    "#   plt.ylabel(\"LogLoss\")\n",
    "#   plt.xlabel(\"Periods\")\n",
    "#   plt.title(\"LogLoss vs. Periods\")\n",
    "#   plt.plot(training_errors, label=\"training\")\n",
    "#   plt.plot(validation_errors, label=\"validation\")\n",
    "#   plt.legend()\n",
    "#   plt.show()\n",
    "  \n",
    "#   # Output a plot of the confusion matrix.\n",
    "#   cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
    "#   # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "#   # in each class).\n",
    "#   cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "#   ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
    "#   ax.set_aspect(1)\n",
    "#   plt.title(\"Confusion matrix\")\n",
    "#   plt.ylabel(\"True label\")\n",
    "#   plt.xlabel(\"Predicted label\")\n",
    "#   plt.show()\n",
    "\n",
    "  return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "LogLoss error (on validation data):\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-ae57b82ce0c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtraining_targets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mvalidation_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     validation_targets=validation_targets)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-72-626b0d5c9efb>\u001b[0m in \u001b[0;36mtrain_cnn_classification_model\u001b[1;34m(learning_rate, steps, batch_size, hidden_units, training_examples, training_targets, validation_examples, validation_targets)\u001b[0m\n\u001b[0;32m     74\u001b[0m   classifier.train(\n\u001b[0;32m     75\u001b[0m       \u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;31m#,#000,#20000,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[1;31m#   hooks=[logging_hook]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     )\n",
      "\u001b[1;32m/home/harrisonfsmith95/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/harrisonfsmith95/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m    841\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/harrisonfsmith95/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m    851\u001b[0m       features, labels, input_hooks = (\n\u001b[0;32m    852\u001b[0m           self._get_features_and_labels_from_input_fn(\n\u001b[1;32m--> 853\u001b[1;33m               input_fn, model_fn_lib.ModeKeys.TRAIN))\n\u001b[0m\u001b[0;32m    854\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m       estimator_spec = self._call_model_fn(\n",
      "\u001b[1;32m/home/harrisonfsmith95/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[1;34m(self, input_fn, mode)\u001b[0m\n\u001b[0;32m    689\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_get_features_and_labels_from_input_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;34m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_input_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m     \u001b[1;31m# TODO(anjalisridhar): What about the default DistributionStrategy? Perhaps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;31m# using any input is alright in that case. There is also a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/harrisonfsmith95/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[1;34m(self, input_fn, mode)\u001b[0m\n\u001b[0;32m    796\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/cpu:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 798\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/harrisonfsmith95/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/inputs/numpy_io.py\u001b[0m in \u001b[0;36minput_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mnum_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_threads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[0menqueue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         num_epochs=num_epochs)\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     batch = (\n",
      "\u001b[1;32m/home/harrisonfsmith95/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py\u001b[0m in \u001b[0;36m_enqueue_data\u001b[1;34m(data, capacity, shuffle, min_after_dequeue, num_threads, seed, name, enqueue_size, num_epochs, pad_value)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m       types = [dtypes.int64\n\u001b[1;32m--> 392\u001b[1;33m               ] + [dtypes.as_dtype(col.dtype) for col in data.values()]\n\u001b[0m\u001b[0;32m    393\u001b[0m       \u001b[0mqueue_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m       \u001b[0mget_feed_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_OrderedDictNumpyFeedFn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/harrisonfsmith95/anaconda3/lib/python3.5/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m       types = [dtypes.int64\n\u001b[1;32m--> 392\u001b[1;33m               ] + [dtypes.as_dtype(col.dtype) for col in data.values()]\n\u001b[0m\u001b[0;32m    393\u001b[0m       \u001b[0mqueue_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m       \u001b[0mget_feed_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_OrderedDictNumpyFeedFn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/harrisonfsmith95/anaconda3/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2667\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2669\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2671\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "cnn_classifier = train_cnn_classification_model(\n",
    "    learning_rate=0.05,\n",
    "    steps=5,#1000,\n",
    "    batch_size=30,\n",
    "    hidden_units=[5],#[100, 100],\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "count 28000.0 28000.0 28000.0 28000.0 28000.0 28000.0 28000.0 28000.0 28000.0   \n",
       "mean      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "std       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "min       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "50%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "75%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "max       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       pixel9    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "count 28000.0    ...      28000.0   28000.0   28000.0   28000.0   28000.0   \n",
       "mean      0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "std       0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "min       0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "25%       0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "50%       0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "75%       0.0    ...          0.0       0.0       0.0       0.0       0.0   \n",
       "max       0.0    ...          1.0       1.0       0.8       0.7       0.5   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "count   28000.0   28000.0   28000.0   28000.0   28000.0  \n",
       "mean        0.0       0.0       0.0       0.0       0.0  \n",
       "std         0.0       0.0       0.0       0.0       0.0  \n",
       "min         0.0       0.0       0.0       0.0       0.0  \n",
       "25%         0.0       0.0       0.0       0.0       0.0  \n",
       "50%         0.0       0.0       0.0       0.0       0.0  \n",
       "75%         0.0       0.0       0.0       0.0       0.0  \n",
       "max         0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[8 rows x 784 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test_dataframe = pd.read_csv(\n",
    "  \"data/test.csv\",\n",
    "  sep=\",\")\n",
    "\n",
    "mnist_test_sample_dataframe = pd.read_csv(\n",
    "  \"data/sample_submission.csv\",\n",
    "  sep=\",\")\n",
    "\n",
    "# test_targets, test_examples = parse_test_labels_and_features(mnist_test_sample_dataframe, mnist_test_dataframe)#parse_labels_and_features(mnist_test_dataframe)\n",
    "\n",
    "\n",
    "# test_examples =   features = dataset.iloc[:,1:785]\n",
    "  # Scale the data to [0, 1] by dividing out the max value, 255.\n",
    "#   features = features // 255\n",
    "#   features = features.applymap(lambda x: x / 255)\n",
    "\n",
    "test_examples = mnist_test_dataframe.copy().applymap(lambda x: x / 255)\n",
    "# print(mnist_test_sample_dataframe)\n",
    "test_targets = mnist_test_sample_dataframe['Label'].copy()\n",
    "test_examples.describe()\n",
    "# test_targets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.00\n"
     ]
    }
   ],
   "source": [
    "predict_test_input_fn = create_predict_input_fn(\n",
    "    test_examples, test_targets, batch_size=100)\n",
    "\n",
    "test_predictions = cnn_classifier.predict(input_fn=predict_test_input_fn)\n",
    "test_predictions = np.array([item['class_ids'][0] for item in test_predictions])\n",
    "  \n",
    "accuracy = metrics.accuracy_score(test_targets, test_predictions)\n",
    "print(\"Accuracy on test data: %0.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000\n",
      "       ImageId  Label\n",
      "0            1      8\n",
      "1            2      8\n",
      "2            3      8\n",
      "3            4      8\n",
      "4            5      6\n",
      "...        ...    ...\n",
      "27995    27996      8\n",
      "27996    27997      8\n",
      "27997    27998      8\n",
      "27998    27999      8\n",
      "27999    28000      8\n",
      "\n",
      "[28000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(len(test_predictions))\n",
    "new = pd.DataFrame(test_predictions, columns=[\"Label\"])#[1], index=new_predictions[0])\n",
    "# print(new)\n",
    "# new.index.name = \"ImageId\"\n",
    "new = pd.concat([mnist_test_sample_dataframe['ImageId'], new], axis=1)\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Visualize weights of first hidden layer\n",
    "print(nn_classifier.get_variable_names())\n",
    "\n",
    "weights0 = nn_classifier.get_variable_value(\"dnn/hiddenlayer_0/kernel\")\n",
    "\n",
    "print(\"weights0 shape:\", weights0.shape)\n",
    "\n",
    "num_nodes = weights0.shape[1]\n",
    "num_rows = int(math.ceil(num_nodes / 10.0))\n",
    "fig, axes = plt.subplots(num_rows, 10, figsize=(20, 2 * num_rows))\n",
    "for coef, ax in zip(weights0.T, axes.ravel()):\n",
    "    # Weights in coef is reshaped from 1x784 to 28x28.\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.pink)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "  # MNIST images are 28x28 pixels, and have one color channel\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "  # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Pooling Layer #1\n",
    "  # First max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "  # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2\n",
    "  # Computes 64 features using a 5x5 filter.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "  # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Pooling Layer #2\n",
    "  # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "  # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Flatten tensor into a batch of vectors\n",
    "  # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "  # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "  # Dense Layer\n",
    "  # Densely connected layer with 1024 neurons\n",
    "  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "  # Output Tensor Shape: [batch_size, 1024]\n",
    "  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "  # Add dropout operation; 0.6 probability that element will be kept\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits layer\n",
    "  # Input Tensor Shape: [batch_size, 1024]\n",
    "  # Output Tensor Shape: [batch_size, 10]\n",
    "  logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def main(unused_argv):\n",
    "# Load training and eval data\n",
    "VALID = 10000 # Validation data size\n",
    "\n",
    "LABELS = 10 # Number of different types of labels (1-10)\n",
    "WIDTH = 28 # width / height of the image\n",
    "CHANNELS = 1 # Number of colors in the image (greyscale)\n",
    "data = pd.read_csv('data/train.csv') # Read csv file in pandas dataframe\n",
    "labels = np.array(data.pop('label')) # Remove the labels as a numpy array from the dataframe\n",
    "#   labels = LabelEncoder().fit_transform(labels)[:, None]\n",
    "#   labels = OneHotEncoder().fit_transform(labels).todense()\n",
    "data = StandardScaler().fit_transform(np.float32(data.values)) # Convert the dataframe to a numpy array\n",
    "#   data = data.reshape(-1, WIDTH, WIDTH, CHANNELS) # Reshape the data into 42000 2d images\n",
    "train_data, eval_data = data[:-VALID], data[-VALID:]\n",
    "train_labels, eval_labels = labels[:-VALID], labels[-VALID:]\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3340216, 'global_step': 5, 'accuracy': 0.0896}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#   mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "#   train_data = mnist.train.images  # Returns np.array\n",
    "#   train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "#   eval_data = mnist.test.images  # Returns np.array\n",
    "#   eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "# Create the Estimator\n",
    "mnist_classifier = tf.estimator.Estimator(\n",
    "  model_fn=cnn_model_fn, model_dir=\"~/delete/one\")#/tmp/mnist_convnet_model\")\n",
    "\n",
    "# # Set up logging for predictions\n",
    "# # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "# tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "# logging_hook = tf.train.LoggingTensorHook(\n",
    "#   tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x={\"x\": train_data},\n",
    "  y=train_labels,\n",
    "  batch_size=100,\n",
    "  num_epochs=None,\n",
    "  shuffle=True)\n",
    "mnist_classifier.train(\n",
    "  input_fn=train_input_fn,\n",
    "  steps=5#,#000,#20000,\n",
    "#   hooks=[logging_hook]\n",
    ")\n",
    "\n",
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x={\"x\": eval_data},\n",
    "  y=eval_labels,\n",
    "  num_epochs=1,\n",
    "  shuffle=False)\n",
    "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\"\"\"Convolutional Neural Network Estimator for MNIST, built with tf.layers.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "  # MNIST images are 28x28 pixels, and have one color channel\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "  # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Pooling Layer #1\n",
    "  # First max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "  # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2\n",
    "  # Computes 64 features using a 5x5 filter.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "  # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Pooling Layer #2\n",
    "  # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "  # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Flatten tensor into a batch of vectors\n",
    "  # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "  # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "  # Dense Layer\n",
    "  # Densely connected layer with 1024 neurons\n",
    "  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "  # Output Tensor Shape: [batch_size, 1024]\n",
    "  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "  # Add dropout operation; 0.6 probability that element will be kept\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits layer\n",
    "  # Input Tensor Shape: [batch_size, 1024]\n",
    "  # Output Tensor Shape: [batch_size, 10]\n",
    "  logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "  # Load training and eval data\n",
    "  mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "  train_data = mnist.train.images  # Returns np.array\n",
    "  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "  eval_data = mnist.test.images  # Returns np.array\n",
    "  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "  # Create the Estimator\n",
    "  mnist_classifier = tf.estimator.Estimator(\n",
    "      model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\n",
    "\n",
    "  # Set up logging for predictions\n",
    "  # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "  tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "  logging_hook = tf.train.LoggingTensorHook(\n",
    "      tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "  # Train the model\n",
    "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=100,\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "  mnist_classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=20000,\n",
    "      hooks=[logging_hook])\n",
    "\n",
    "  # Evaluate the model and print results\n",
    "  eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": eval_data},\n",
    "      y=eval_labels,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "  eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "  print(eval_results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
